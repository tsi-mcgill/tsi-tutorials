{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Recent Tutorials","text":""},{"location":"#python","title":"Python:","text":"<ul> <li>Introduction to Python</li> <li>Introduction to Gaia Data (with Python)</li> <li>Benchmarking and Profiling</li> <li>Threading And Multiprocessing</li> <li>Object Orientated Programming and Packaging</li> <li>Scientific Programming in Python</li> <li>Calculating \\(\\pi\\) using Monte Carlo Methods</li> <li>N-Body Simulation</li> <li>Introduction to Machine Learning in Astronomy (Delivered as part of Hackathon)</li> </ul>"},{"location":"#c","title":"C++","text":"<ul> <li>Introduction to C++</li> <li>Introduction to Parallel Programming in C++ with OpenMP </li> </ul>"},{"location":"#rust","title":"Rust","text":"<ul> <li>Programming in Rust</li> </ul>"},{"location":"#version-control","title":"Version Control","text":"<ul> <li>Git (I), Introduction to git and the git command line interface (CLI)</li> <li>Git (II), GitHub as a tool </li> </ul>"},{"location":"#containerization","title":"Containerization","text":"<ul> <li>Introduction to Apptainer</li> <li>Introduction to Docker</li> <li>Using a containerized Jupyter Kernel</li> </ul>"},{"location":"#misc","title":"Misc","text":"<ul> <li>Website building with MkDocs</li> </ul>"},{"location":"Containerization/","title":"Containerization","text":"<ul> <li>Introduction to Apptainer</li> <li>Introduction to Docker</li> <li>Using a containerized Jupyter Kernel</li> </ul>"},{"location":"Containerization/apptainer/","title":"Apptainer","text":""},{"location":"Containerization/apptainer/#what-is-containerization","title":"What is Containerization?","text":"<p>Containerization is a practice in software development in which a single application, or group of applications, is collected into a bundle which includes the application executable and all the libraries and dependencies required to run that application. With this information, the application can be run on any resource that uses the underlying containerization software installed.</p> <p>Containerization is made possible due to \"namespaces.\" In Linux, a namespace defines the boundaries of a process' \"awareness\" of what else is running around it. This essentially means that a process can \"see\" and interact with everything accessible within its own namespace, but not necessarily what is in other namespaces. With this in mind, we can limit what is accessible from a namespace. For example, we might limit the resources available (RAM, CPU) to a namespace or limit the file access of a namespace. When a process runs on a system, we can \"containerize\" the process by restricting what the process has access to. This allows the process to be run in isolation from the rest of the system. As far as the containerized process knows, it is in an operating system by itself with no other processes running (other than child processes!).</p> <p>There are many types of containerization software (e.g. Docker, Podman and Apptainer), but at their base level, they behave similarly. For simplicity, consider a container to be made up of two things:</p> <ol> <li>A file system snapshot. This is a snapshot of the files in a system, including required libraries, executables, input data, etc. Anything that might be required for an application to run.</li> <li>A start command. This could be the command required to start an application (e.g. an analysis pipeline, a website, a Python interpreter). The file system snapshot contains everything needed to execute the start command.</li> </ol> <p>Here we rely on the fact that Linux systems are very similar, so much so that we can use a common Linux kernel to run two different operating systems. Consider OS 1 and OS 2. OS 1 is the \"host\" system, that is the system on which we will be running the container. OS 2 is going to be an image of a different operating system that we are going to run as a container on OS 1. The image for OS 2 contains the specific files needed (file system snapshot) to run the applications of interest. When we want to create a container, we essentially create a new namespace for the container, copy the files from OS 2 into the file system accessible to the container, and then run commands within the container. From within the namespace of the container, it would appear that we are running OS 2 despite using the Linux kernel of OS 1 to actually execute the code.</p> <p>It's worth noting that while containers share the kernel, they don't necessarily emulate an entire operating system. They typically share the kernel and some system resources but maintain separate user spaces.</p> <p>We can start to see some of the benefits of using a container emerge:</p> <ol> <li> <p>Dependencies only need to be solved once! We only need to determine the dependencies once when we create the image. The image can then be used on any host system (e.g., OS 1) which has the appropriate containerization software (Apptainer, Docker, Podman, etc.). Furthermore, I could give this image to anyone else who needs to run the same software.</p> </li> <li> <p>Old Difficult Dependencies: When using older software, it is often difficult to get the dependencies to work on a modern system. For example, if we have old software designed to run on Ubuntu 12.04 and we want to run it on Ubuntu 22.04, we would have to fight through 10 years of development to get the software to work on Ubuntu 22.04. However, we could also just create an image which uses Ubuntu 12.04, install the required software, and then run it as a container on Ubuntu 22.04.</p> </li> <li> <p>Reproducibility: Containers allow us to share the exact system that code was run on. This removes any system or version-dependent behavior. Imagine we submitted a paper that ran some analysis with some dependency. In between submission and receiving comments, the dependency was updated (from version 1.1 to version 1.2). This introduces a 1% discrepancy depending on which version is used. Since our code heavily depends on this dependency, that 1% discrepancy, which might have been acceptable to the developers, propagates to a larger 10% discrepancy. This could be very difficult to debug if we updated a lot of packages, especially if I need to respond to a reviewer's comments within a tight timeframe! Luckily, I created an image of the analysis at the time of submission. Therefore, I can create a container from that older image and address the reviewer's comments using the version of the code the analysis was initially run with.</p> </li> </ol> <p>See also: - What even is a container?</p>"},{"location":"Containerization/apptainer/#containers-vs-virtual-machines","title":"Containers vs Virtual Machines","text":"<p>Containers and Virtual Machines (VMs) are similar concepts, but they differ in how they achieve virtualization, particularly from a software and hardware perspective.</p> <p>A VM is an entire operating system, including the entire file system, system kernel, and anything else required to run the operating system. Because of this, VMs tend to have larger file sizes than container images.</p> <p>On the other hand, a container uses the host system's operating system through a container engine. Therefore, a container only requires the files necessary to run the application or group of applications it is designed for, without using the host system's kernel. This is based on the advanced Linux concept known as \"namespaces.\" Essentially, the container will have its own \"namespace\" with its version of libraries, separate from the host OS.</p> <p>A VM requires dedicated access to real hardware resources (memory, CPU, GPU, etc.). It obtains this using a \"hypervisor,\" a process that allocates parts of the real hardware and creates virtual hardware that the VM will use. For example, if a VM needs to use 20% of the system RAM, the hypervisor would allocate 20% of the available RAM as virtual RAM, which would be used exclusively by the VM. One downside of using VMs is that since only the VM can use the allocated resources, we could never have more than 5 VMs of this configuration active at a time (as it would exceed 100% of the available resources).</p> <p>In contrast, a container is executed like a normal process on the host system. Resources can be allocated as they would for any other process on a system. This allows multiple containers to operate simultaneously, with the system scheduler handling resource allocation between them. For example, if both a VM and a container need 20% of the system RAM during operations, the VM will \"own\" 20% of the RAM for the entire lifetime of the VM. However, the container will only \"own\" as much RAM as it needs at any given time. Therefore, during periods of low resource usage, such as between expensive operations when the RAM usage drops to, say, 10%, the system can allocate the remaining 10% to other processes.</p> <p>Regarding startup time, since containers use the host system, they take seconds to start up (essentially just copying files). In contrast, a VM might require the VM OS to \"boot\" before starting up, which can take minutes (as it needs to boot an entire operating system before executing a command).</p> <p>See also: - Difference between Containers and Virtual Machines</p>"},{"location":"Containerization/apptainer/#apptainersingularity","title":"Apptainer/Singularity","text":"<p>Apptainer, formerly known as Singularity, is a computer program that performs operating-system-level virtualization, commonly known as containerization. It addresses security concerns associated with other containerization software, making it popular in high-performance computing (HPC) environments. Apptainer enables developers to create and develop code in their preferred environment before packaging it into a Singularity Container Image (SCI). These images can be easily shared with others while ensuring reproducibility in various computing environments.</p>"},{"location":"Containerization/apptainer/#pulling-a-pre-exisiting-image","title":"Pulling a Pre-exisiting Image","text":"<p>There are numerous sources of pre-existing images. Commonly used ones are DockerHub, GitHub Container Registry, and Library.</p> <p>Apptainer can convert Docker images directly into Apptainer images. This is extremely useful, allowing us to piggyback on the work of others. For example, let's say we would like to have the latest version of Python to run a test; we could simply use the latest Python Docker image on DockerHub using something like:</p> <pre><code>&gt; apptainer shell docker://python:latest\n</code></pre> <p>This would create an Apptainer image and start an interactive <code>shell</code> in a container generated from the python:latest image hosted on DockerHub. <pre><code>&gt; apptainer shell docker://python:latest\nINFO:    Using cached SIF image\nApptainer&gt; python --version\nPython 3.12.2\n</code></pre> If we wanted to access a different version, for example if we needed Python 2.7, we could use: <pre><code>&gt; apptainer shell docker://python:2.7   \nINFO:    Converting OCI blobs to SIF format\nINFO:    Starting build...\nGetting image source signatures\nCopying blob 6f4489a7e4cf done  \nCopying blob fd4b47407fc3 done  \nCopying blob dc3f0c679f0f done  \nCopying blob 09b6f03ffac4 done  \nCopying blob b32f6bf7d96d done  \nCopying blob 7e2b2a5af8f6 done  \nCopying blob af4b99ad9ef0 done  \nCopying blob 39db0bc48c26 done  \nCopying blob acb4a89489fc done  \nCopying config 8452137826 done  \nWriting manifest to image destination\nStoring signatures\n2024/03/25 11:06:37  info unpack layer: sha256:7e2b2a5af8f65687add6d864d5841067e23bd435eb1a051be6fe1ea2384946b4\n2024/03/25 11:06:38  info unpack layer: sha256:09b6f03ffac4cb4e42f8ab0bfc480bd3a3fa20e1ddee37784db63bc886b0cbb3\n2024/03/25 11:06:38  info unpack layer: sha256:dc3f0c679f0f4c39597721c1df5cdb4f9685b26bd789a44eeb406835a1800d5f\n2024/03/25 11:06:38  info unpack layer: sha256:fd4b47407fc30b8206971ec60f280b107b00df8007da2fb912ebb8656b53695e\n2024/03/25 11:06:40  info unpack layer: sha256:b32f6bf7d96d26a22dc62da6522f384dcdc936c30c88b233d378e06cf127346d\n2024/03/25 11:06:43  info unpack layer: sha256:6f4489a7e4cfcda98c90d9fb220ab8dbf5e40a7a6d756ed414707967aa96bfbd\n2024/03/25 11:06:43  info unpack layer: sha256:af4b99ad9ef03daa029d78458e669f135a3c41764bbc154e9d56a3d9b2ee7bf1\n2024/03/25 11:06:43  info unpack layer: sha256:39db0bc48c262bd32f4b201a4fad3dde162e73d3d1135fdaab433477156ad816\n2024/03/25 11:06:43  info unpack layer: sha256:acb4a89489fc21e4c05c6ef86dacf640cab884b3b3e207cfd5ad24da02f14661\nINFO:    Creating SIF file...\nApptainer&gt; python --version\nPython 2.7.18\n</code></pre></p> <p>We can see that Apptainer will first pull the image in layers from DockerHub before creating a <code>SIF</code> (Singularity Image Format) file. If we leave out the <code>tag</code> when pulling from DockerHub, the <code>latest</code> tag is always chosen. <pre><code>&gt; apptainer shell docker://python    \nINFO:    Using cached SIF image\nApptainer&gt; python --version\nPython 3.12.2\n</code></pre></p>"},{"location":"Containerization/apptainer/#editing-images-using-sandbox","title":"Editing images using <code>sandbox</code>","text":"<p>When taking a base image, it is often the case that we're missing packages or we would like to install other packages into that image. For example, the Python container we've been using doesn't have <code>ipython</code>. <pre><code>&gt; apptainer shell docker://python\nINFO:    Using cached SIF image\nApptainer&gt; ipython\nbash: ipython: command not found\n</code></pre></p> <p>We can create a <code>sandbox</code>, essentially unpacking the contents of the image into a directory, allowing that image to be modified. <pre><code>&gt; apptainer build --sandbox python_project docker://python\nINFO:    Starting build...\nGetting image source signatures\nCopying blob 63941d09e532 skipped: already exists  \nCopying blob d68cd2123173 skipped: already exists  \nCopying blob 567db630df8d skipped: already exists  \nCopying blob 09527fa4de8d skipped: already exists  \nCopying blob 5f899db30843 skipped: already exists  \nCopying blob 3cb8f9c23302 skipped: already exists  \nCopying blob 71215d55680c skipped: already exists  \nCopying blob 097431623722 skipped: already exists  \nCopying config 35a79b0576 done  \nWriting manifest to image destination\nStoring signatures\n2024/03/25 11:18:13  info unpack layer: sha256:71215d55680cf0ab2dcc0e1dd65ed76414e3fb0c294249b5b9319a8fa7c398e4\n2024/03/25 11:18:14  info unpack layer: sha256:3cb8f9c23302e175d87a827f0a1c376bd59b1f6949bd3bc24ab8da0d669cdfa0\n2024/03/25 11:18:14  info unpack layer: sha256:5f899db30843f8330d5a40d1acb26bb00e93a9f21bff253f31c20562fa264767\n2024/03/25 11:18:15  info unpack layer: sha256:567db630df8d441ffe43e050ede26996c87e3b33c99f79d4fba0bf6b7ffa0213\n2024/03/25 11:18:19  info unpack layer: sha256:d68cd2123173935e339e3feb56980a0aefd7364ad43ca2b9750699e60fbf74c6\n2024/03/25 11:18:19  info unpack layer: sha256:63941d09e5322b88281f3a325eff9ced5bf2ee45b691aaf8ec2f829bafbd8021\n2024/03/25 11:18:20  info unpack layer: sha256:097431623722383300c03bb41fd162d32346bf6a02a053263f51969eb9746e3d\n2024/03/25 11:18:20  info unpack layer: sha256:09527fa4de8dd73399164c307942cc43652a01fc2bb370e38ae0f806b42b4b18\nINFO:    Creating sandbox directory...\nINFO:    Build complete: python_project\n(base) \n</code></pre></p> <p>Here we have run <code>apptainer build --sandbox python_project docker://python</code>. We specify the base image as <code>docker://python</code> (the latest version of the Python image on DockerHub), and we <code>build</code> a <code>--sandbox</code> directory with the name <code>python_project</code>.  If we look inside this directory, it will look very similar to what is at <code>/</code> (root directory) of your own machine: <pre><code>&gt; ls python_project \nbin  boot  dev  environment  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  singularity  srv  sys  tmp  usr  var\n</code></pre> We can then create a new container by executing the following command: <pre><code>&gt; apptainer shell --writable python_project\n</code></pre> Within the container, we can proceed to install ipython. <pre><code>pip install ipython\n</code></pre> To exit the image, type <code>exit</code>. To confirm that <code>ipython</code> is now installed, we can recreate the container using the following command: <pre><code>&gt; apptainer shell python_project\nApptainer&gt; ipython --version\n8.22.2\n</code></pre> Note that we aren't calling <code>--writable</code> because we no longer need the directory to be writable. We could have the container writable; however, this is bad practice as modifying a container unintentionally might break compatibility with others using the container.</p>"},{"location":"Containerization/apptainer/#creating-a-basic-image-using-build","title":"Creating a basic Image using <code>build</code>","text":"<p>We have now pulled an image from DockerHub, created a modified version of that image, and saved it to a directory.</p> <p>Looking at the directory, we notice that it takes up a pretty substantial amount of memory: <pre><code>&gt; du -sh ./python_project \n1.1G    ./python_project\n</code></pre></p> <p>If we only have a single image that we work with, 1GB might not be too bad, but as we increase the complexity of the project and use more containers, this will quickly take up a lot of space.</p> <p>We can build a <code>SIF</code> (Singularity Image Format) file from that directory using: <pre><code>&gt; apptainer build python_project.sif python_project\n\nINFO:    Starting build...\nINFO:    Creating SIF file...\nINFO:    Build complete: python_project.sif\n</code></pre> Note that the format is <code>apptainer build &lt;output_name&gt; &lt;input_name&gt;</code>.</p> <p>This command creates a file called <code>python_project.sif</code>, which can be thought of as a compressed version of the <code>python_project</code> directory: <pre><code>&gt; du -sh ./*\n1.1G    ./python_project\n346M    ./python_project.sif\n</code></pre> In this case, the <code>sif</code> file takes up around 1/3 of the storage of the directory.</p>"},{"location":"Containerization/apptainer/#apptainer-run-and-apptainer-exec-commands","title":"<code>apptainer run</code> and <code>apptainer exec</code> commands","text":"<p>Containers can be given a predefined command to run. This command can be executed using <code>apptainer run &lt;image_name&gt;</code>. For example, with the <code>python_project</code> image: <pre><code>&gt; apptainer run python_project.sif \nPython 3.12.2 (main, Mar 12 2024, 11:02:14) [GCC 12.2.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; \n</code></pre></p> <p>We can see that this puts us into a Python interpreter as designed by the maintainers of this image.</p> <p>We can also specify the command to <code>run</code>. Consider the Python script <code>hello.py</code>: <pre><code>print (\"Hello, world!\")\nprint (\"Inside of container!\")\n</code></pre></p> <p>We can run this script using the container: <pre><code>&gt; apptainer exec python_project.sif python hello.py\nHello, world!\nInside of container!\n</code></pre></p> <p><code>apptainer run</code> executes the default command specified by the image author, while <code>apptainer exec</code> allows you to run custom commands within a running container.</p>"},{"location":"Containerization/apptainer/#binding-file-systems","title":"Binding file systems","text":"<p>By default, the Apptainer image will <code>bind</code> directories to the container, allowing them to be accessed from within the container. This behavior may depend on your environment (for example, whether or not a read directory is automatically bound).</p> <p>We can explicitly <code>bind</code> directories when executing/running commands using the following syntax:</p> <pre><code>apptainer exec -B &lt;/path/to/local/directory&gt;:&lt;/path/to/container/mount/point&gt; image.sif &lt;command&gt;\n</code></pre> <p>For example: <pre><code>&gt; apptainer exec -B $HOME/Downloads:/Downloads python_project.sif bash\n</code></pre></p> <p>This command mounts the <code>~/Downloads</code> folder to <code>/Downloads</code> within the container and runs <code>bash</code> to get an interactive terminal. We could then access the downloads from <code>/Downloads</code>.</p> <p>We can bind multiple directories, for example:</p> <pre><code>&gt; apptainer exec -B $HOME/Downloads:/Downloads -B $HOME/Desktop:/Desktop  python_project.sif bash\n</code></pre> <p>By default, Apptainer will also mount the <code>$HOME</code> directory. This can occasionally be problematic; for example, if you have Python libraries stored in <code>$HOME</code>, they may be picked up when mounting the <code>$HOME</code> directory instead of the libraries stored in the image. You can specify the <code>$HOME</code> directory using <code>--home &lt;dir_name&gt;</code>:</p> <pre><code>&gt; apptainer exec --home `pwd` python_project.sif bash                                          \n</code></pre> <p>This command sets the <code>$HOME</code> to be the current directory when launching the container. </p>"},{"location":"Containerization/apptainer/#creating-an-image-from-a-definition-file","title":"Creating an image from a definition file","text":"<p>Similar to Docker, we can create an image from a file with a list of instructions.</p> <p>Apptainer <code>def</code> files follow this format: <pre><code>Bootstrap: docker\nFrom: ubuntu:{{ VERSION }}\nStage: build\n\n%arguments\n    VERSION=22.04\n\n%setup\n\n%files\n\n%environment\n\n\n%post\n\n\n%runscript\n\n%startscript\n\n%test\n\n%labels\n\n%help\n</code></pre></p> <p>We'll walk through these sections one by one.</p>"},{"location":"Containerization/apptainer/#preamble","title":"Preamble","text":"<pre><code>Bootstrap: docker\nFrom: ubuntu\nStage: build\n</code></pre> <ul> <li><code>Bootstrap</code> specifies where we are getting the base image from. In this case, it's <code>docker</code> (DockerHub).</li> <li><code>From:</code> specifies the base image. In this case, it will grab the latest Ubuntu image.</li> <li><code>Stage:</code> specifies the stage of the build. Multiple stages can be used to simplify the build process and reduce the final file size.</li> </ul>"},{"location":"Containerization/apptainer/#arguments","title":"<code>%arguments</code>","text":"<pre><code>Bootstrap: docker\nFrom: ubuntu:{{ VERSION }}\nStage: build\n\n%arguments\n    VERSION=22.04\n</code></pre> <p>Arguments are variables that can be used within the definition file. Using arguments allows us to change variables only in one place rather than multiple instances, preventing bugs.</p> <p>In the above example, we've specified an argument <code>VERSION=22.04</code>. This argument is then accessed in the preamble when selecting the Ubuntu image version:</p> <p><pre><code>From: ubuntu:{{ VERSION }}\n</code></pre> This specifies that we will be using <code>ubuntu:22.04</code>.</p>"},{"location":"Containerization/apptainer/#setup","title":"<code>%setup</code>","text":"<p>Setup commands are first executed outside of the container on the host system before starting to build the image.</p> <p>For example, suppose we want to compress some files that will later be added to the container:</p> <pre><code>%setup\n\n    tar -zcvf files.tar.gz ./*.txt\n</code></pre> <p>This command would compress all the files ending in <code>.txt</code> in the current directory into <code>files.tar.gz</code> (also in the current directory).</p>"},{"location":"Containerization/apptainer/#files","title":"<code>%files</code>","text":"<p>This is where we can specify files to be copied into the container. <pre><code>%files\n    files.tar.gz /opt\n</code></pre> Here, we are copying the <code>files.tar.gz</code> that was created in the <code>%setup</code> into the <code>/opt</code> directory of the image (<code>/opt/files.tar.gz</code>).</p>"},{"location":"Containerization/apptainer/#environment","title":"<code>%environment</code>","text":"<p>Here we specify environmental variables that we want set within the container.</p> <pre><code>%enviroment\n    export PATH=$PATH:/app/bin\n    export DEFAULT_PORT=8001\n</code></pre> <p>In this example, we set two environmental variables. First, we modify the <code>PATH</code> to include <code>/app/bin</code>, where the hypothetical binaries for our application reside. Second, we specify the <code>DEFAULT_PORT</code> to be <code>8001</code>.</p> <p>We can access these variables anytime within the container or the build process.</p>"},{"location":"Containerization/apptainer/#post","title":"<code>%post</code>","text":"<p>In this section, we specify the command we want to run after the base image has downloaded. Environmental variables for the host system are not passed, so this can be considered a clean environment.</p> <p>This will likely be the most detailed section of your definition script. For example:</p> <pre><code>%post\n    apt-get update &amp;&amp; apt-get install -y gcc\n    pip install ipython\n</code></pre> <p>In the above example, we are simply updating the Ubuntu base image and installing <code>gcc</code>. We then install <code>ipython</code> using <code>pip</code>.</p> <p>This is a simple example, but <code>%post</code> would be the section where dependencies would be installed and/or compiled.</p>"},{"location":"Containerization/apptainer/#runscript","title":"<code>%runscript</code>","text":"<p>This is where we define a set of commands that will be executed when running <code>apptainer run image.sif</code> or when running the image itself as a command (e.g., <code>./image.sif</code>).</p> <p>Internally, these commands will form a simple script that will be executed.</p> <pre><code>%runscript\n    ipython\n</code></pre> <p>This example will start an IPython interpreter. We could have something more complicated, such as:</p> <pre><code>%runscript\n    echo \"Recieved the following arguements $*\"\n    ipython $*\n</code></pre> <p>This will output the arguments passed before executing them with IPython. For example:</p> <p><pre><code>&gt; apptainer run ./jupyter.sif --version\nRecieved the following arguements --version\n8.22.2\n</code></pre> Here, we're passing <code>--version</code> as an argument. This gets passed and run as <code>ipython --version</code>, which gives <code>8.22.2</code>.</p> <p>One could use the <code>%runscript</code> section to define a default behavior and how arguments are handled.</p>"},{"location":"Containerization/apptainer/#startscript","title":"<code>%startscript</code>","text":"<p>This is similar to the <code>%runscript</code> section where we create a script to be run when running the container. Specifically, the <code>%startscript</code> runs when the container is launched as an <code>instance</code> rather than a process launched with <code>run</code> or <code>exec</code>. Instances can be considered more of a daemon, which will have a more passive interface. For example, an instance may monitor a port to receive a command that controls its behavior. It might be better to launch a web server as an instance.</p> <p>Likewise, if you have multiple steps in a data pipeline, they could be passed between instances which are persistent compared to the analysis target.</p>"},{"location":"Containerization/apptainer/#test","title":"<code>%test</code>","text":"<p>This defines a test script that is run at the end of the build process and can be used to ensure the validity of the built container.</p> <p>For example, if we are building a data pipeline, we might want to make sure we get the expected answer.</p> <p><pre><code>%test\n    python test_script.py\n    if [ $? -eq 0 ]; then\n        echo \"Script executed successfully\"\n    else\n        echo \"Script failed\"\n        exit 1\n    fi\n</code></pre> Here we are running <code>test_script.py</code>. The output of this code will be accessible using <code>$?</code>, which returns the last return code.</p> <p><pre><code>    if [ $? -eq 0 ]; then\n</code></pre> This line checks if the return code is 0, which is a typical code for a successful execution. In our Python code, we would have a line like:</p> <p><pre><code>if successful_test:\n    exit(0)\nelse:\n    exit(1)\n</code></pre> If the code executes successfully, then the return will be 0; otherwise, it will be 1.</p>"},{"location":"Containerization/apptainer/#labels","title":"<code>%labels</code>","text":"<pre><code>%labels\n    Author myuser@example.com\n    Version v0.0.1\n    MyLabel Hello World\n</code></pre> <p>Here we define a set of labels that are viewable using the <code>apptainer inspect</code> command.</p> <p>Versioning can be super important when developing an application. Maintaining an up-to-date version number can prevent a lot of headaches when trying to debug issues.</p>"},{"location":"Containerization/apptainer/#help","title":"<code>%help</code>","text":"<p>Help specifies a help message that will be outputted:</p> <pre><code>%help\n    This is a container with jupyter lab and notebook install\n</code></pre> <p>This can be accessed using: <pre><code>apptainer run-help my_container.sif\n</code></pre></p>"},{"location":"Containerization/apptainer/#example-definition-script","title":"Example definition script","text":"<p>Here is an example of a <code>.def</code> file which installs <code>Jupyter</code>, <code>IPython</code>, <code>Matplotlib</code>, and <code>NumPy</code>.</p> <pre><code>Bootstrap: docker\nFrom: python:latest\n\n%post\n    pip install jupyter ipykernel jupyterlab notebook\n    pip install matplotlib numpy\n\n%environment\n    export DEFAULT_PORT=8001\n\n%runscript\n    ipython $*\n\n%startscript\n    jupyter lab --port=$DEFAULT_PORT\n</code></pre> <p>This can be built with: <pre><code>&gt; apptainer build jupyter.sif jupyter.def\n</code></pre></p> <p>The <code>runscript</code> will take arguments and pass them to IPython. For example: <pre><code>&gt; ./jupyter.sif hello.py\nHello, world!\nInside of container!\n</code></pre></p> <p>The <code>startscript</code> will start a Jupyter Lab on port 8001. This can be launched using: <pre><code>&gt; apptainer instance start jupyter.sif jupyter-server \n</code></pre></p> <p>When navigating to <code>http://localhost:8001</code>, we'll notice that we need to log in. We can get a login code using:\" <pre><code>&gt; apptainer exec instance://jupyter-server jupyter lab list\nCurrently running servers:\nhttp://localhost:8001/?token=643b97dc15207ca577782ea2e03a3ec1f9337a4445bc1db8 :: /home/obriens/Documents/apptainer\n</code></pre></p> <p>Clicking on that link will log us in. We need to remember to <code>stop</code> the instance once we're finished.</p> <pre><code>&gt; apptainer instance stop jupyter-server            \n</code></pre>"},{"location":"Containerization/apptainer/#example-of-a-multi-stage-build","title":"Example of a multi-stage build","text":"<p>As mentioned earlier, using multi-stage builds can help decrease the final size of the <code>sif</code> file.</p> <p>Consider the following <code>C++</code> code:</p> convert_units.cpp<pre><code>#include &lt;iostream&gt;\n#include &lt;fstream&gt;\n\nusing namespace std;\n\nint main(int argc, char *argv[]){\n\n    // parse the command passed\n    // Input is in meters\n    float input = atof(argv[1]);\n\n    // convert unit to mm\n    float output = input * 1e3;\n\n    // output to a text file\n    ofstream out_file;\n    out_file.open(\"test.txt\");\n    out_file &lt;&lt; output &lt;&lt; endl;\n\n    // Also print\n    cout &lt;&lt; output &lt;&lt; endl;\n    return 0;\n}\n</code></pre> <p>This will convert meters to mm. We can imagine this being part of a larger data analysis pipeline.</p> <p>This can be compiled using: <pre><code>g++ convert_units.cpp -o convert_units\n</code></pre> This will create a binary called <code>convert_units</code>.</p> <p>Let's start to build the definition file:</p> single_stage.def<pre><code>Bootstrap: docker\nFrom: ubuntu\nStage: build\n\n%files\n    convert_units.cpp /build/convert_units.cpp\n\n%post\n    apt-get update &amp;&amp; apt-get upgrade -y &amp;&amp; apt-get install -y g++\n    g++ /build/convert_units.cpp -o /bin/convert_units\n\n%runscript\n    /bin/convert_units $*\n</code></pre> <p>Here we have a single stage called <code>build</code>. In this stage, we copy the source code to the <code>/build</code> directory at the <code>%files</code> stage. In the <code>%post</code> stage, we update the OS and install <code>g++</code>, a C++ compiler. We then compile the code to <code>/bin/convert_units</code>. We then specify this as the entry point of the <code>%runscript</code> stage.</p> <p>We can run this as: <pre><code>&gt; ./single_stage.sif 1.25\n1250\n</code></pre></p> <p>You'll notice that the <code>convert_units.cpp</code> file is no longer needed once <code>convert_units</code> is compiled. Likewise, we only need <code>g++</code> to compile <code>convert_units</code>; we don't use it later in the file. We could turn this into a multi-stage build:</p> multi_stage.def<pre><code>Bootstrap: docker\nFrom: ubuntu\nStage: build\n\n\n%files\n    convert_units.cpp /build/convert_units.cpp\n\n%post\n    apt-get update &amp;&amp; apt-get upgrade -y &amp;&amp; apt-get install -y g++\n    g++ /build/convert_units.cpp -o /build/convert_units\n\nBootstrap: docker\nFrom: ubuntu\nStage: final\n\n%files from build\n  /build/convert_units /bin/convert_units\n\n%post\n    apt-get update &amp;&amp; apt-get upgrade -y\n\n%runscript\n    /bin/convert_units $*\n</code></pre> <p>The definition file is similar to the <code>single_stage.def</code> file; however, we have broken this up into two stages. </p> <p>The first stage, tagged as <code>build</code>, will add the source file <code>convert_units.cpp</code> to the image, update the OS, install <code>g++</code>, and compile <code>/build/convert_units</code>.</p> <p>The second stage, called <code>final</code>, uses the same <code>Bootstrap</code> and base image (<code>ubuntu</code>) as the <code>build</code> stage. However, at the <code>%files</code> stage on line 17, we are only copying the <code>/build/convert_units</code> from the <code>build</code> stage to <code>/bin/convert_units</code> in the <code>final</code> stage. We still want to make sure we have an up-to-date OS (security updates are always important), so we still run <code>apt-get update &amp;&amp; apt-get upgrade -y</code>. Finally, the <code>%runscript</code> stage is only included in the <code>final</code> stage.</p> <p>We can see that we get the same behavior from both images:</p> <pre><code>&gt; ./single_stage.sif 1.25 ;  ./multi_stage.sif 1.25\n1250\n1250\n</code></pre> <p>However, when we look at the size of the files, we see a difference: <pre><code>&gt; ls -lah ./*_stage.sif\n-rwxr-xr-x 1 obriens obriens  63M Mar 25 14:36 ./multi_stage.sif\n-rwxr-xr-x 1 obriens obriens 142M Mar 25 14:36 ./single_stage.sif\n</code></pre></p> <p>You'll notice that the <code>multi_stage.sif</code> build is around half the size of <code>single_stage.sif</code>. This is partly due to the <code>multi_stage.sif</code> not containing the source code (<code>convert_units.cpp</code>), but also due to it not containing <code>g++</code>.</p>"},{"location":"Containerization/containerized_kernels/","title":"Containerizing Jupyter Kernels","text":"<p>In this tutorial we'll look at how to use a containerized Jupyter kernel. The first question one might have is why would we want to containerize a Jupyter kernel? We're often faced with the problem of having complex dependencies that might be difficult to install on ones system. For example, let's say we're using Windows or Mac OS but one of the packages we want to use is only available for Linux. We have a few solutions:</p> <ul> <li> <p>Dual boot Linux: This is when we split our computer's hard drive in two and install Linux on one side of the hard drive, with the original operating system remaining on the other. A downside of this is that we need to partition our hard drive, reducing the storage available to either partition. This is not something we can do on a shared system such as a high performance computing (HPC) cluster.</p> </li> <li> <p>Virtual Machine (VM): In this setup we'd have a Linux virtual machine set up on the Window's system. This requires a full installation of the guest operating system (Linux), which can use a significant amount of storage space on the host's system. VMs also require resources (e.g. CPU, RAM, GPU) to be allocated to the VM. This means that those resources are no longer available to the host system's scheduler. For example if the VM is allocated 20% of the CPU allocation, and is sitting idle, the host system can only access 80% of the CPU allocation, despite the remaining 20% going unused. The availability of VMs on a shared system will be highly depended on the system.</p> </li> <li> <p>100% containerized workflow: With this option, we'd launch Jupyter hub from a container. This isn't a bad solution, but it doesn't allow for swapping between different kernels. Imagine we have a dependency that now only works in Windows and not Linux. We'd have to have two different Jupyter lab instances (one from the host system and one containerized) open to swap between different workflows. This could work in an HPC environment, however an HPC system will likely require such a job by queued in the system as normal job. This may require further layers of ssh-tunneling to go from a login node to an analysis node. HPC clusters which support Jupyter environments (for example Digital Research Alliance of Canada) will typically have users interact with their Jupyter instance through a web portal, which uses a python environment based on python versions available on that cluster. </p> </li> </ul> <p>The most promising and flexible solution is to use a containerized workflow. However, it would be nice if we could use this containerized python environment within the preexisting JupyterHub web portal. In this tutorial we'll do just that.</p>"},{"location":"Containerization/containerized_kernels/#apptainer-or-docker","title":"Apptainer or Docker?","text":"<p>For this tutorial, we'll work in both Apptainer and Docker to show how this can be done using either containerization software. For one's personal machine, it might be preferable to use Docker. However, for HPC environments such as DRAC (Narval, Beluga, Cedar, Niagra) we'll use Apptainer.</p>"},{"location":"Containerization/containerized_kernels/#lets-build-an-image","title":"Let's build an Image","text":"<p>Let's build a simple small image with installs some python packages from a requirements file. For this we'll use the Python 3.12-slim image (see DockerHub) as our base image.</p> <p>We'll use the following <code>requirements.txt</code> file as our requirements: <pre><code>numpy\nmatplotlib\nscipy\nipykernel\nipython\n</code></pre></p>"},{"location":"Containerization/containerized_kernels/#docker-image","title":"Docker image","text":"<p>Dockerfile<pre><code>FROM python:3.12-slim\n\nCOPY ./requirements.txt /build/requirements.txt\n\nRUN pip install -r /build/requirements.txt\n\nCMD [\"python\"]\n</code></pre> Which can be built with the tag <code>kernel</code> as: <pre><code>docker build -t kernel .\n</code></pre></p>"},{"location":"Containerization/containerized_kernels/#apptainer-image","title":"Apptainer image","text":"<p>kernel.def<pre><code>Bootstrap: docker\nFrom: python:3.12-slim\n\n%files\n    ./requirements.txt /build/requirements.txt\n\n%post\n    pip install -r /build/requirements.txt\n\n%runscript\n    python\n</code></pre> Which can be built to the file <code>kernel.sif</code> as: <pre><code>apptainer build kernel.sif kernel.def\n</code></pre></p>"},{"location":"Containerization/containerized_kernels/#installing-a-custom-kernel","title":"Installing a custom kernel","text":"<p>Now that we have our image created, let's create a custom Jupyter kernel. <pre><code>python -m ipykernel install --user --name custom-kernel --display-name=\"custom-kernel\"\n</code></pre> Which installs the kernel in my home directory, e.g.: <pre><code>Installed kernelspec custom-kernel in /home/obriens/.local/share/jupyter/kernels/custom-kernel\n</code></pre> Creating the <code>kernel.json</code> file (<code>/home/obriens/.local/share/jupyter/kernels/custom-kernel/kernel.json</code>): <pre><code>{\n \"argv\": [\n  \"/home/obriens/miniforge3/bin/python\",\n  \"-m\",\n  \"ipykernel_launcher\",\n  \"-f\",\n  \"{connection_file}\"\n ],\n \"display_name\": \"custom-kernel\",\n \"language\": \"python\",\n \"metadata\": {\n  \"debugger\": true\n }\n}\n</code></pre></p> <p>We want to modify this file with our containerized kernel:</p>"},{"location":"Containerization/containerized_kernels/#docker","title":"Docker","text":"<p><pre><code>{\n \"argv\": [\n     \"docker\",\n     \"run\",\n     \"--network=host\",\n     \"--rm\",\n     \"-v\",\n     \"{connection_file}:/connection_file\",\n     \"kernel\",\n     \"python\",\n     \"-m\",\n     \"ipykernel_launcher\",\n     \"-f\",\n     \"/connection_file\"\n ],\n \"display_name\": \"custom-kernel (docker)\",\n \"language\": \"python\",\n \"metadata\": {\n  \"debugger\": true\n }\n}\n</code></pre> Note that we pass the keyword <code>--network=host</code>, and we're mounting the <code>{connection_file}</code> from the host to <code>/connection_file</code> within the container.</p>"},{"location":"Containerization/containerized_kernels/#apptainer","title":"Apptainer","text":"<pre><code>{\n \"argv\": [\n     \"apptainer\",\n     \"exec\",\n     \"--bind\",\n     \"{connection_file}:/tmp/connection_file\",\n     \"/path/to/kernel.sif\",\n     \"python\",\n     \"-m\",\n     \"ipykernel_launcher\",\n     \"-f\",\n     \"/tmp/connection_file\"\n ],\n \"display_name\": \"custom-kernel (apptainer)\",\n \"language\": \"python\",\n \"metadata\": {\n  \"debugger\": true\n }\n}\n</code></pre> <p>Note we're binding <code>{connection_file}</code> from the host to <code>/connection_file</code></p>"},{"location":"Containerization/containerized_kernels/#launching-the-custom-kernel","title":"Launching the custom kernel","text":"<p>With the image created, the kernel installed and <code>kernel.json</code> modified, we can now launch a Jupyter lab instance and connect to the kernel using (feel free to use any custom options such as the instance port): <pre><code>jupyter lab\n</code></pre> Now we can access the launcher by clicked the \"New Launcher\" button or pressing Ctrl + Shift + L:</p> <p> </p>  Launch Button  <p>Once pressed, we can see an option to launch a Notebook or Python console using our custom kernel:</p> <p> </p>  Custom kernels in Jupyter lab  <p>If we're using the Docker kernel, then we can monitor the container using any of the standard methods we'd use for monitoring docker containers, for example: <pre><code>docker ps\n</code></pre> <pre><code>CONTAINER ID   IMAGE                     COMMAND                  CREATED              STATUS              PORTS                                         NAMES\n0c39ccf064d0   obriens/kernel:latest     \"python -m ipykernel\u2026\"   About a minute ago   Up About a minute                                                 determined_chatelet\n</code></pre> Or to read the logs of that container: <pre><code>docker logs 0c39ccf064d0\n</code></pre> <pre><code>NOTE: When using the `ipython kernel` entry point, Ctrl-C will not work.\n\nTo exit, you will have to explicitly quit this process, by either sending\n\"quit\" from a client, or using Ctrl-\\ in UNIX-like environments.\n\nTo read more about this, see https://github.com/ipython/ipython/issues/2049\n\n\nTo connect another client to this kernel, use:\n    --existing /connection_file\n</code></pre> If we shut down the kernel from Jupyter lab (Kernel-&gt;Shut Down All Kernels), you'll notice that the container is deleted upon shut down (<code>docker ps</code>). </p>"},{"location":"Containerization/containerized_kernels/#some-peculiarities","title":"Some Peculiarities","text":""},{"location":"Containerization/containerized_kernels/#number-of-containers","title":"Number of Containers","text":"<p>If we launch multiple notebooks within Jupyter lab, we'll have multiple containers spawned, for example with 4 notebooks: <pre><code>docker ps\n</code></pre> <pre><code>CONTAINER ID   IMAGE                     COMMAND                  CREATED          STATUS          PORTS                                         NAMES\n6830bcd6b669   obriens/kernel:latest     \"python -m ipykernel\u2026\"   32 seconds ago   Up 31 seconds                                                 competent_zhukovsky\nbcec50f54eed   obriens/kernel:latest     \"python -m ipykernel\u2026\"   34 seconds ago   Up 34 seconds                                                 recursing_noether\nc64e9ab35482   obriens/kernel:latest     \"python -m ipykernel\u2026\"   38 seconds ago   Up 37 seconds                                                 blissful_solomon\n20bc592ceb4c   obriens/kernel:latest     \"python -m ipykernel\u2026\"   41 seconds ago   Up 41 seconds                                                 boring_ardinghelli\n</code></pre></p> <p>This might be the desired behavior, however having an individual container for each instance might be very memory and resource consuming. Instead, we could start an instance of some container and then attach any new kernels to that container instead. For this we need to create some \"daemon\" that will keep the container process alive. Let's modify the <code>Dockerfile</code> to create an infinite process: <pre><code>FROM python:3.12-slim\n\nCOPY ./requirements.txt /build/requirements.txt\n\nRUN pip install -r /build/requirements.txt\nRUN echo \"#!/bin/bash\\nwhile true; do sleep 1; done\" &gt;&gt; /build/keep_alive.sh ; chmod a+x /build/keep_alive.sh \n\nCMD [\"/build/keep_alive.sh\"]\n</code></pre></p> <p>Here we're creating a script called <code>/build/keep_alive.sh</code>. This scrip contains a while loop that will loop <code>while true</code>, i.e. until terminated and then simply sleep for 1 second.</p> <p>Aside on <code>docker stop</code></p> <p>Note: When running <code>docker stop</code>, Docker will send a <code>SIGTERM</code> signal to the process and wait, by default, 10 seconds before sending a <code>SIGKILL</code> signal. We can keep this in mind when setting the sleep time. If the sleep time is &gt; 10 seconds then the process will be killed. If &lt; 10 seconds then the process will attempt to gracefully terminate. This can help prevent data coruptions by ensuring files are properly closed or clean up any child processes managed by this instance. This is something to keep in mind when designing your image. </p> <p>We then set the start script to be <code>/build/keep_alive.sh</code>, so that when we start this container, it will default to this script and run until interrupted.</p> <p>Next we need to create a start script that we'll call when trying to start/restart a Jupyter kernel: launch_docker.sh<pre><code>#!/bin/bash\n\n# Get the filename of the connection file\nconnection_file=$(basename $1)\ncontainer_name=\"server\"\nimage_name=\"obriens/kernel:latest\"\n\n# Check if the server is currently runnong\nif [ \"$( docker container inspect -f '{{.State.Running}}' $container_name )\" = \"true\" ]; then\n    echo \"Server is running\"\nelse\n    # if not then start the server\n    echo \"Starting the server\"\n    # Get the path of where the connection files will be stored\n    connection_path=\"$(jupyter --runtime-dir)\"\n    # Stop and remove the container if it already exists\n    docker stop $container_name\n    docker rm $container_name\n    # Create a new container\n    # Add it to the host network and mounting the connection_path\n    docker create --name=$container_name -v  $connection_path:/connections --network=host  $image_name\n    # Start this server instance\n    docker start  $container_name\nfi\n\n# Launch a ipykernel using the connection file that was passed as arg 1\ndocker exec  $container_name python -m ipykernel_launcher -f /connections/$connection_file\n</code></pre></p> <p>We can then make this executable with: <pre><code>chmod +x launch_docker.sh\n</code></pre></p> <p>Finally, we can modify the <code>kernel.json</code> file to call this script: <pre><code>{\n \"argv\": [\n     \"/path/to/launch_docker.sh\",\n     \"{connection_file}\"\n ],\n \"display_name\": \"custom-kernel (docker)\",\n \"language\": \"python\",\n \"metadata\": {\n  \"debugger\": true\n }\n}\n</code></pre></p> <p>With this new <code>kernel.json</code> file and <code>launch_docker.sh</code> in place, we can now relaunch <code>jupyter lab</code> and open multiple notebooks.  If we now check the containers that we have running with <code>docker ps</code>: <pre><code>CONTAINER ID   IMAGE                     COMMAND                  CREATED          STATUS          PORTS                                         NAMES\ne2a7475eb14f   obriens/kernel:latest        \"/build/keep_alive.sh\"   15 minutes ago   Up 15 minutes                                                 server\n</code></pre> We can see that only one kernel is running.</p> <p>The container can be stopped at any time with: <pre><code>docker stop server\n</code></pre> And removed with: <pre><code>docker rm server\n</code></pre></p> <p>One downside is that we'll need to manually stop the container once we're finished.</p>"},{"location":"Containerization/containerized_kernels/#what-about-apptainer","title":"What about Apptainer","text":"<p>Similarly, we can limit the number of Apptainer containers by starting our container as an <code>instance</code> and then connecting to that instance. Apptainer doesn't require a daemon script (like <code>/build/keep_alive.sh</code>), so we don't need to modify the image. Let's use the following script to launch the Apptainer instance: <pre><code>#!/bin/bash\n\n# Get the filename of the connection file\nconnection_file=$(basename $1)\ncontainer_name=\"server\"\nimage_name=\"/path/to/kernel.sif\"\n# Check if the server instance is currently running\nif [ $(apptainer instance list $container_name | wc | awk '{print $1}') -ge 2 ]; then\n    echo \"Server is running\"\nelse\n    # if it isn't start an instance\n    echo \"Starting the server\"\n    # Bind the runtime-dir (where the connection files are) to /connections within the container\n    apptainer instance start --bind `jupyter --runtime-dir`:/connections $image_name $container_name\nfi\n\n# Attach and run ipykernel_laucher\napptainer exec instance://$container_name python -m ipykernel_launcher -f /connections/$connection_file\n</code></pre></p> <p>The instance can be stopped with <pre><code>apptainer instance stop server \n</code></pre></p>"},{"location":"Containerization/containerized_kernels/#networking-and-port-mapping","title":"Networking and Port mapping","text":"<p>You might have noticed that we didn't need to pass any ports to the docker image.  Instead, we simply add the container to the <code>host</code> network.  In this set up, all the communication between the container and Jupyter lab is handled through the <code>host</code> network, with the Jupyter instance itself handling the ports being used.</p> <p>In contrast to launching a Jupyter instance from within a container, we don't actually need to know what ports should be mapped within the container. This greatly simplifies the setup, instead allowing ports to be determined at launch, without the need to reconfigure a run command of a docker-compose file.</p>"},{"location":"Containerization/docker/Part1/","title":"Workshop Part 1: Introduction to Docker and Containerization","text":"<p>Topics that will be covered:</p> <ul> <li> <p>Introduction to Containerization</p> <ul> <li>Definition and benefits of containerization</li> <li>Differences between virtual machines and containers</li> </ul> </li> <li> <p>Docker Basics</p> <ul> <li>Overview of Docker</li> <li>Installing Docker on various platforms</li> <li>Basic Docker commands: <code>docker run</code>, <code>docker ps</code>, <code>docker stop</code>, <code>docker rm</code></li> </ul> </li> <li> <p>Creating Docker Images</p> <ul> <li>Writing a simple Dockerfile</li> <li>Building and running a Docker image</li> <li>Understanding image layers</li> </ul> </li> <li> <p>Managing Docker Containers</p> <ul> <li>Container lifecycle: start, stop, remove</li> <li>Docker networking basics</li> </ul> </li> </ul>"},{"location":"Containerization/docker/Part1/#docker-basics","title":"Docker Basics","text":"<p>Docker has a streamlined install process for most modern operating systems. Please ensure that Docker is installed and tested on your machine before starting the workshop.</p>"},{"location":"Containerization/docker/Part1/#installing-docker-on-various-platforms","title":"Installing Docker on Various Platforms","text":""},{"location":"Containerization/docker/Part1/#docker-engine","title":"Docker Engine","text":"<p>The Docker Engine can be installed on Windows, macOS, and Linux. Instructions can be found here.</p>"},{"location":"Containerization/docker/Part1/#docker-desktop","title":"Docker Desktop","text":"<p>Docker Desktop provides a high-level interface for viewing, accessing, and managing your containers and images. There are many useful extensions you can install, such as:</p> <ul> <li>Disk Usage: A tool that \"displays and categorizes the disk space used by Docker.\"</li> <li>Resource Usage: A tool to monitor the resource usage of your containers.</li> <li>Logs Explorer: A tool for examining and filtering logs from your containers.</li> </ul>"},{"location":"Containerization/docker/Part1/#creating-and-adding-users-to-the-docker-group","title":"Creating and Adding Users to the <code>docker</code> Group","text":"<p>If installing Docker on Linux (including WSL), you might want to add your user to the <code>docker</code> group so that you don't need elevated permissions to run a container. Instructions on how to do this can be found here.</p>"},{"location":"Containerization/docker/Part1/#getting-ready-for-the-workshop","title":"Getting ready for the Workshop","text":"<p>The workshop will require images to be pulled from a remote server, hence requiring internet access.  To avoid network congestion, please <code>pull</code> the following images by running the commands below:</p> <pre><code>docker pull hello-world\ndocker pull python:3.9-slim\ndocker pull ubuntu\n</code></pre>"},{"location":"Containerization/docker/Part1/#overview-of-docker","title":"Overview of Docker","text":""},{"location":"Containerization/docker/Part1/#docker-basics_1","title":"Docker Basics","text":"<p>Docker is a powerful tool that streamlines the process of building, sharing, and running applications.  It uses \"containerization\" technology to create isolated environments, or containers, for applications and their dependencies.  These containers are lightweight, portable, and consistent, ensuring that software runs the same way in development, testing, and production environments.  By encapsulating an application and its environment, Docker simplifies the management of software projects, enhances productivity, and facilitates continuous integration and delivery (CI/CD).  This makes it an essential tool for modern software development and deployment practices.</p> <p>Docker and containerization solve the \"it works on my machine\" problem that one often faces when developing or running an application.  Using Docker, a developer can package the required dependencies of an application, providing a stable environment for the application to run, and then pass this environment onto the user to run as an \"image.\" Using this \"image,\" a user can then recreate the same environment that the developer intended for running the application and execute the application on their own machine as a \"container.\" Critically, this \"container\" acts as a semi-independent operating system within the host operating system, meaning that potential conflicts arising on the host system can be overcome by the containerized image.  For example, if you have a Windows machine and you want to run software developed for Linux, Docker provides a method to run this software as if the host system were Linux. Similarly, if the host system has some dependency (for example, Python 3) and the application has some conflicting dependency (for example, Python 2), the container will run the application using the dependencies in the container rather than the host operating system.</p> <p>You may have previously heard of \"Virtual Machines\" (VMs).  This is a similar idea to containers, with some important differences. VMs work by allocating virtual hardware (e.g., CPU, GPU, RAM, and storage) and installing the entire operating system.  This can make them quite resource-intensive. A container, on the other hand, uses the fact that Linux systems tend to be very similar, to the extent that the underlying system kernel can be used. A container image essentially contains a filesystem snapshot (FSS) and a run command.  The FSS can be considered all the files, folders, programs, and libraries required to run the application that the container has been developed for. When we create a container from the image, we are essentially loading the FSS into a new local \"namespace.\" The host system's resources are then allocated to the container like any other process. Within this new namespace, we have access to the versions of files, programs, and libraries that are defined in the FSS (e.g., we might have Python 3.11.1). By design, the container cannot \"see\" outside of the namespace; as far as the container can see, it is the only thing operating on the system. The run command defines the default behavior of the container.  This could be something like starting a new bash shell, executing a program, or starting a service.</p> <p>In summary, the differences between VMs and containers are:</p> <ol> <li> <p>Operating System Requirements:</p> <ul> <li>A VM requires the full operating system to be installed on the host system.</li> <li>A container shares the host system's kernel and includes only the necessary binaries, libraries, and configuration files needed to run a specific application or suite of applications.</li> </ul> </li> <li> <p>Resource Allocation and Efficiency:</p> <ul> <li>VMs require specific resources to be allocated to them, such as CPU, RAM, and storage. These resources are managed by a \"hypervisor,\" which runs on the host system. Each VM operates in isolation with its own OS, leading to higher overhead due to the need to replicate the OS and allocate dedicated resources.</li> <li>Containers use the host system\u2019s kernel to run processes and do not require a separate OS. They are managed by the container runtime (e.g., Docker Engine) and leverage the host system\u2019s resources dynamically. Containers are subject to the host's system scheduler, just like any other process, allowing them to be more efficient in terms of resource utilization.</li> </ul> </li> <li> <p>Startup Time:</p> <ul> <li>A VM will require the entire guest operating system to boot before running a process.</li> <li>A container does not require a guest operating system, meaning that a process can be started with very little downtime.</li> </ul> </li> <li> <p>Isolation:</p> <ul> <li>VMs provide a different operating system for a process to run on. This provides strong isolation between the host system and any number of guest operating systems.</li> <li>Containers provide process-level isolation. They are isolated from each other using the host operating system's features like namespaces and cgroups, but they share the same OS kernel.</li> </ul> </li> </ol>"},{"location":"Containerization/docker/Part1/#basic-docker-commands-docker-run-docker-ps-docker-stop-docker-rm","title":"Basic Docker commands: <code>docker run</code>, <code>docker ps</code>, <code>docker stop</code>, <code>docker rm</code>","text":"<p>Let's start off by using pre-made Docker images to look at some of the fundamental commands.</p>"},{"location":"Containerization/docker/Part1/#docker-run","title":"<code>docker run</code>","text":"<p>The <code>docker run</code> command can be used to run the default command for a Docker image. For example, let's use the hello-world example: <pre><code>docker run hello-world\n</code></pre></p> <p>If you haven't ran this before, you will see the following: <pre><code>Unable to find image 'hello-world:latest' locally\nlatest: Pulling from library/hello-world\nc1ec31eb5944: Pull complete\nDigest: sha256:94323f3e5e09a8b9515d74337010375a456c909543e1ff1538f5116d38ab3989\nStatus: Downloaded newer image for hello-world:latest\n</code></pre></p> <p>Let's take a moment to break this down: <pre><code>Unable to find image 'hello-world:latest' locally\nlatest: Pulling from library/hello-world\n</code></pre> Here we can see that Docker cannot find the <code>hello-world:latest</code> image locally and instead pulls an image from <code>library/hello-world</code>. Here Docker is searching container repositories like Docker Hub to find the <code>hello-world</code> image.</p> <p>Once the image is finished downloading, a default command is run, which gives the following output: <pre><code>Hello from Docker!\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n 1. The Docker client contacted the Docker daemon.\n 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n    (amd64)\n 3. The Docker daemon created a new container from that image which runs the\n    executable that produces the output you are currently reading.\n 4. The Docker daemon streamed that output to the Docker client, which sent it\n    to your terminal.\n\nTo try something more ambitious, you can run an Ubuntu container with:\n $ docker run -it ubuntu bash\n\nShare images, automate workflows, and more with a free Docker ID:\n https://hub.docker.com/\n\nFor more examples and ideas, visit:\n https://docs.docker.com/get-started/\n</code></pre></p> <p>This provides some information on what has just happened and suggests that we try running a command like: <pre><code>docker run -it ubuntu bash\n</code></pre></p> <p>So let's do that: <pre><code>Unable to find image 'ubuntu:latest' locally\nlatest: Pulling from library/ubuntu\n9c704ecd0c69: Pull complete\nDigest: sha256:2e863c44b718727c860746568e1d54afd13b2fa71b160f5cd9058fc436217b30\nStatus: Downloaded newer image for ubuntu:latest\nroot@b52ba93a0cc5:/#\n</code></pre> Once again, we didn't have the <code>ubuntu:latest</code> image available locally, so Docker downloaded this image.  We notice now that we are left with a prompt: <pre><code>root@a1fe94f95dc6:/#\n</code></pre> Let's breakdown the command we just ran: <pre><code>docker run -it ubuntu bash\n</code></pre> Here we've used <code>run</code> to specify that we want to run an image in a container. <code>-it</code> specifies that we want an interactive shell (<code>-i</code> allows STDIN to remain open for sending commands, and <code>-t</code> allocates a text terminal for running commands within the shell). We specified the image as <code>ubuntu</code>; however, unlike the previous example (<code>hello-world:latest</code>), we didn't specify the \"tag\" or \"version\" to use, so by default, the \"latest\" tag will be used for the <code>ubuntu</code> image. Finally, we specified the command we wanted to run, <code>bash</code>, which starts a new bash shell. </p> <p>In summary, using <code>docker run -it ubuntu bash</code>, we have started a container running <code>ubuntu</code> (specifically <code>ubuntu:latest</code> by default), configured an interactive terminal (<code>-it</code>) for command interaction, and initiated a bash shell (<code>bash</code>) within the container.</p> <p>Jumping back into the container we can run commands like: <pre><code>root@a1fe94f95dc6:/# whoami\nroot\n</code></pre> This tells us that the current user is <code>root</code>.  <pre><code>root@a1fe94f95dc6:/# hostname\na1fe94f95dc6\n</code></pre> This tells us the name of the host as viewed from within the docker container. Notice that this will be different to the name of your system. As far as the container knows, it is running on a seperate system called <code>a1fe94f95dc6</code>. <pre><code>root@a1fe94f95dc6:/# echo \"Hello\"\nHello\n</code></pre> Many commonly used commands are preinstalled in the Ubuntu Docker image. When we want to exit, we simply type: <pre><code>root@a1fe94f95dc6:/# exit\nexit\n</code></pre></p> <p>We can run commands directly by specifying the command: <pre><code>docker run -it ubuntu top\n</code></pre></p> <p>This will run the <code>top</code> command, which is useful for viewing processes running on the machine. You should see output similar to this: <pre><code>top - 14:56:11 up 47 min,  0 user,  load average: 0.02, 0.07, 0.08\nTasks:   1 total,   1 running,   0 sleeping,   0 stopped,   0 zombie\n%Cpu(s):  0.2 us,  0.3 sy,  0.0 ni, 99.3 id,  0.1 wa,  0.0 hi,  0.1 si,  0.0 st\nMiB Mem :   7536.3 total,   5033.9 free,   2113.9 used,    621.6 buff/cache\nMiB Swap:   2048.0 total,   2048.0 free,      0.0 used.   5422.5 avail Mem\n\n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND\n    1 root      20   0    8864   5012   2908 R   0.0   0.1   0:00.02 top\n</code></pre></p> <p>Notice that there is only one process running. This is because the container is isolated and cannot see processes outside of its own environment. As far as the container is concerned, it operates in isolation with no other processes running. To quit <code>top</code>, simply press <code>q</code>.</p>"},{"location":"Containerization/docker/Part1/#docker-ps","title":"<code>docker ps</code>","text":"<p>Let's run a Docker container with a command, but this time let's run it in the background without using <code>-it</code>. Instead, we'll use <code>-d</code> to \"detach\" the container: <pre><code>docker run -d ubuntu sh -c \"while true; do echo 'Hello, Docker!'; sleep 60; done\"\n</code></pre> The command we ran will start with an <code>ubuntu</code> image and execute <code>sh -c \"while true; do echo 'Hello, Docker!'; sleep 60; done\"</code>. This will start a new <code>sh</code> shell, which loops indefinitely and prints \"Hello, Docker!\" every 60 seconds.</p> <p>The output we should see is something like this:</p> <p><pre><code>5af71a22c48073e0feccea5d6b6100ee1d428449fbef74b95324a29b6cfc6d18\n</code></pre> This is the container's ID, which we can later use to access the container. Notably, we don't immediately see the \"Hello, Docker!\" output. However, when we run:</p> <p><pre><code>docker ps\n</code></pre> We will see: <pre><code>CONTAINER ID   IMAGE     COMMAND                  CREATED         STATUS         PORTS     NAMES\n5af71a22c480   ubuntu    \"sh -c 'while true; \u2026\"   2 minutes ago   Up 2 minutes             nostalgic_hertz\n</code></pre> Here we can see the containers currently running. The output displays the container ID, the image being used, the command that was run, the time it was created, the current status, any exposed ports (more on this later), and a container name.</p> <p>We can view the output of the container using the <code>docker logs</code> command. Here, we can either pass the container ID or the container name:</p> <p><pre><code>docker logs 5af71a22c480\nHello, Docker!\nHello, Docker!\nHello, Docker!\nHello, Docker!\nHello, Docker!\n</code></pre> or  <pre><code>docker logs nostalgic_hertz\nHello, Docker!\nHello, Docker!\nHello, Docker!\nHello, Docker!\nHello, Docker!\nHello, Docker!\n</code></pre></p> <p>Giving a name to a container makes it easier to identify and manage. We can assign a name to a container using the <code>--name</code> flag when launching it: <pre><code>docker run -d --name greeter ubuntu sh -c \"while true; do echo 'Hello, Docker!'; sleep 60; done\"\n</code></pre></p> <p>In this example, <code>--name greeter</code> names the container as \"greeter\". We can verify this by using <code>docker ps</code>: <pre><code>docker ps\nCONTAINER ID   IMAGE     COMMAND                  CREATED          STATUS          PORTS     NAMES\n34e6f000f761   ubuntu    \"sh -c 'while true; \u2026\"   44 seconds ago   Up 42 seconds             greeter\n5af71a22c480   ubuntu    \"sh -c 'while true; \u2026\"   7 minutes ago    Up 7 minutes              nostalgic_hertz\n</code></pre></p> <p>By default, <code>docker ps</code> shows only active containers. To view all containers, including those that have exited, we use <code>docker ps -a</code>, which might display something like this: <pre><code>docker ps -a\nCONTAINER ID   IMAGE         COMMAND                  CREATED             STATUS                         PORTS     NAMES\n34e6f000f761   ubuntu        \"sh -c 'while true; \u2026\"   13 minutes ago      Up 12 minutes                            greeter\n5af71a22c480   ubuntu        \"sh -c 'while true; \u2026\"   19 minutes ago      Up 19 minutes                            nostalgic_hertz\nb1709a781e28   ubuntu        \"pwd\"                    About an hour ago   Exited (0) About an hour ago             laughing_lumiere\nf02b7d8ca480   ubuntu        \"ls\"                     About an hour ago   Exited (0) About an hour ago             lucid_curran\n120a97deeb54   ubuntu        \"ls /home/obriens/\"      About an hour ago   Exited (2) About an hour ago             silly_swartz\naee56c00887d   ubuntu        \"ls\"                     About an hour ago   Exited (0) About an hour ago             great_payne\n7f344b33f26f   ubuntu        \"whoami\"                 About an hour ago   Exited (0) About an hour ago             flamboyant_mahavira\n05317adbd6b3   ubuntu        \"hostname\"               About an hour ago   Exited (0) About an hour ago             laughing_tesla\n</code></pre></p> <p>This output shows two running containers (<code>greeter</code> and <code>nostalgic_hertz</code>) and several containers that have exited but are still present.</p>"},{"location":"Containerization/docker/Part1/#docker-stop","title":"<code>docker stop</code>","text":"<p>Let's examine our active containers: <pre><code>docker ps\nCONTAINER ID   IMAGE     COMMAND                  CREATED          STATUS          PORTS     NAMES\n34e6f000f761   ubuntu    \"sh -c 'while true; \u2026\"   15 minutes ago   Up 15 minutes             greeter\n5af71a22c480   ubuntu    \"sh -c 'while true; \u2026\"   22 minutes ago   Up 22 minutes             nostalgic_hertz\n</code></pre></p> <p>To stop a container, use <code>docker stop</code> followed by either the container name or ID: <pre><code>docker stop 5af71a22c480\n</code></pre></p> <p>This command may take a few seconds to complete. When <code>docker stop</code> is invoked, it sends a <code>SIGTERM</code> signal to the process running inside the container. <code>SIGTERM</code> is a soft request for the process to finish. If the process is designed to handle this signal, it may initiate a graceful shutdown.</p> <p>After sending <code>SIGTERM</code>, Docker waits for 10 seconds (by default) to allow for a graceful shutdown. If the process does not terminate gracefully within this time frame, Docker then sends a <code>SIGKILL</code> signal, which forcefully terminates the process.</p> <p>In summary, <code>docker stop</code> attempts to gracefully terminate the program inside the container. If the program does not respond to <code>SIGTERM</code>, Docker resorts to forcefully terminating it with <code>SIGKILL</code>.</p>"},{"location":"Containerization/docker/Part1/#docker-rm","title":"<code>docker rm</code>","text":"<p>Similar to the <code>rm</code> command in Unix-like systems, <code>docker rm</code> removes a container. Let's first review our running containers:</p> <pre><code>CONTAINER ID   IMAGE     COMMAND                  CREATED          STATUS          PORTS     NAMES\n34e6f000f761   ubuntu    \"sh -c 'while true; \u2026\"   29 minutes ago   Up 29 minutes             greeter\n</code></pre> <p>Now, let's try to remove the <code>greeter</code> container: <pre><code> docker rm greeter\nError response from daemon: You cannot remove a running container 34e6f000f76147c340648064e1e0356d483142f8ad6cb02fcb228a536c0ac39a. Stop the container before attempting removal or force remove\n</code></pre> We need to stop a container before removing it: <pre><code>docker stop greeter &amp;&amp; docker rm greeter\n</code></pre></p> <p>Checking with <code>docker ps</code> confirms that the <code>greeter</code> container has been stopped. Running <code>docker ps -a</code> shows that <code>greeter</code> is no longer listed, but <code>nostalgic_hertz</code> is still there. We can remove it using: <pre><code>docker rm nostalgic_hertz\n</code></pre></p> <p>Stale containers can consume memory over time. To remove all exited containers, you can use: <pre><code>docker container prune\n</code></pre></p> <p>This command prompts for confirmation before deleting the containers permanently.</p>"},{"location":"Containerization/docker/Part1/#creating-docker-images","title":"Creating Docker Images","text":"<p>Up until now, we've been using pre-made Docker images. However, there may come a time when we need to create our own custom image.</p>"},{"location":"Containerization/docker/Part1/#writing-a-simple-dockerfile","title":"Writing a Simple Dockerfile","text":"<p>The first step in creating a Docker image is to write a <code>Dockerfile</code>. This file contains instructions on how to build the image. Let's start by writing a short Python program that will serve as the entry point for our image. We'll call this script <code>app.py</code>:</p> <pre><code>import numpy as np\n\ndef generate_random_numbers(num_points):\n    return np.random.rand(num_points)\n\ndef calculate_statistics(numbers):\n    mean = np.mean(numbers)\n    std_dev = np.std(numbers)\n    return mean, std_dev\n\nif __name__ == \"__main__\":\n    num_points = 1000  # Size of the random number list\n    numbers = generate_random_numbers(num_points)\n    mean, std_dev = calculate_statistics(numbers)\n    print(f\"Generated {num_points} random numbers\")\n    print(f\"Mean: {mean}\")\n    print(f\"Standard Deviation: {std_dev}\")\n</code></pre> <p>This program generates 1000 random numbers using <code>NumPy</code> and prints their mean and standard deviation. Note that <code>NumPy</code> is a requirement for this program, so we should create a <code>requirements.txt</code>file: <pre><code>numpy\n</code></pre></p> <p>Now, let's put together the <code>Dockerfile</code>: <pre><code># Use the official Python image from the Docker Hub\nFROM python:3.9-slim\n\n# Set the working directory inside the container\nWORKDIR /app\n\n# Copy the requirements.txt file into the container\nCOPY requirements.txt .\n\n# Install the dependencies specified in requirements.txt\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy the rest of the application code into the container\nCOPY app.py .\n\n# Specify the command to run the application\nCMD [\"python\", \"app.py\"]\n</code></pre></p> <p>Let's walk through this <code>Dockerfile</code>:</p> <ul> <li> <p><code>FROM python:3.9-slim</code>: This line specifies that we're using the official Python image from Docker Hub, specifically Python 3.9 in its slim variant.</p> </li> <li> <p><code>WORKDIR /app</code>: Sets the working directory inside the container to <code>/app</code>. If the directory doesn't exist, it will be created.</p> </li> <li> <p><code>COPY requirements.txt .</code>: Copies the <code>requirements.txt</code> file from the host into the <code>/app</code> directory in the container.</p> </li> <li> <p><code>RUN pip install --no-cache-dir -r requirements.txt</code>: Installs the Python dependencies listed in <code>requirements.txt</code> using <code>pip</code>. The <code>--no-cache-dir</code> flag ensures that downloaded files aren't cached, which can reduce the size of the final image.</p> </li> <li> <p><code>COPY app.py .</code>: Copies app.py from the host into the /app directory in the container.</p> </li> <li> <p><code>CMD [\"python\", \"app.py\"]</code>: Specifies the command to run when the container starts. In this case, it executes python app.py within the /app directory.</p> </li> </ul> <p>It's common practice to use all caps when specifying <code>Dockerfile</code> keywords (<code>FROM</code>, <code>COPY</code>, <code>WORKDIR</code>, <code>RUN</code>, <code>CMD</code>). This helps differentiate them from commands within the scripts being run inside the container.</p> <p>Creating custom Docker images allows you to package your applications with all their dependencies, ensuring consistency and reproducibility across different environments.</p>"},{"location":"Containerization/docker/Part1/#building-and-running-a-docker-image","title":"Building and Running a Docker Image","text":"<p>Now that we have everything we need, let's build the image. Replace <code>obriens</code> with your Docker Hub username if you intend to push this image to Docker Hub:</p> <p><pre><code>docker build . -t obriens/part1:latest \n</code></pre> This command searches for a file named <code>Dockerfile</code> in the current directory (specified with <code>.</code>), builds the Docker image, and tags it (<code>-t</code>) as <code>obriens/part1:latest</code>. The username (<code>obriens</code> in this case) is specified to indicate where the image will be pushed if you decide to upload it to Docker Hub. The output will resemble something like this: <pre><code>docker build . -t obriens/part1:latest                                                                       130 \u21b5\n[+] Building 17.2s (11/11)\n...\n =&gt; =&gt; writing image sha256:016e858e57ceb90b7b12b2aa8ec0b79642ad20d0ef356c8453e7bc6f2fc78d03                       0.0s\n =&gt; =&gt; naming to docker.io/obriens/part1:latest                                             \n</code></pre> The build process involves multiple stages (<code>[1/5]</code> to <code>[5/5]</code>), where each stage corresponds to a command in your Dockerfile. The final stage (<code>CMD [\"python\", \"app.py\"]</code>) specifies the command that will be executed when the container starts. Each command creates a separate \"layer\" of the image, with subsequent layers building upon previous ones.</p> <p>To run our newly built image as a container, use the <code>docker run</code> command: <pre><code>docker run --rm -it obriens/part1:latest\n</code></pre> Here, the <code>--rm</code> flag automatically removes the container once the process inside it finishes. The output should show something like: <pre><code>Generated 1000 random numbers\nMean: 0.4942313233338699\nStandard Deviation: 0.28658542837100653 \n</code></pre> When building the image, we didn't specify the filename (<code>Dockerfile</code>) explicitly because Docker defaults to looking for a file named <code>Dockerfile</code>. However, you can specify a different filename if needed.</p> <p>Let's create a development version of our image by modifying <code>app.py</code> to include a message indicating it's running from the development container:</p> <pre><code>...\n# Copy the rest of the application code into the container\nCOPY app.py .\n\nRUN echo \"print ('Ran from dev container')\" &gt;&gt; app.py\n\n# Specify the command to run the application\nCMD [\"python\", \"app.py\"]\n</code></pre> <p>This modification uses <code>echo</code> to append a line to <code>app.py</code> that prints \"Ran from dev container\". Now, build the development version using a different <code>Dockerfile</code> (<code>Dockerfile.dev</code>) and tag it as <code>obriens/part1:dev</code>: <pre><code>docker build . -f Dockerfile.dev -t obriens/part1:dev\n</code></pre> Notice that we specified the filename with <code>-f Dockerfile.dev</code> and changed the tag to <code>obriens/part1:dev</code>. If you try to run <code>obriens/part1</code> without specifying a tag, Docker defaults to the latest tag. For example: <pre><code> docker run --rm -it obriens/part1\nGenerated 1000 random numbers\nMean: 0.5130255869429714\nStandard Deviation: 0.29060466306009264\n</code></pre> This will run the <code>latest</code> tagged image. However, when running the development version: <pre><code>docker run --rm -it obriens/part1:dev\nGenerated 1000 random numbers\nMean: 0.5011963504428272\nStandard Deviation: 0.28711061209965844\nRan from dev container\n</code></pre> The output will include the additional message from <code>app.py</code> indicating it's running from the development container.</p> <p>Using multiple tags (<code>latest</code>, <code>dev</code>, etc.) is useful for specifying different versions or configurations of your Docker image. It helps manage different stages of development or deployment scenarios effectively.</p>"},{"location":"Containerization/docker/Part1/#why-use-different-tags-for-images","title":"Why Use Different Tags for Images?","text":"<p>In the example provided, different tags serve several key purposes:</p> <ol> <li> <p>Version Control and Stability: Tags like <code>obriens/part1:latest</code> and <code>obriens/part1:dev</code> help distinguish different versions of the same application or service. This ensures that users can choose between stable releases (<code>latest</code>) and potentially less stable development versions (<code>dev</code>).</p> </li> <li> <p>Environment Specificity: Tags can denote images optimized for specific environments or purposes. For instance, <code>python:3.9-slim</code> indicates a Python 3.9 base image that is minimal in size (<code>slim</code>), which is preferable for lightweight deployments compared to a full version (<code>python:3.9</code>).</p> </li> <li> <p>Dependency Management: Tags also facilitate managing dependencies. The <code>dev</code> tag might include additional libraries or tools needed for testing and development, while the <code>latest</code> tag could be streamlined for production use.</p> </li> </ol> <p>By using specific tags like <code>python:3.9-slim</code>, developers communicate to users the exact environment and optimizations applied to the image. This clarity helps in maintaining consistency across deployments and ensures compatibility with specific requirements.</p>"},{"location":"Containerization/docker/Part1/#understanding-image-layers","title":"Understanding Image Layers","text":"<p>Previously, we introduced the concept of layers and how images are built layer by layer. Let's explore why this matters with a practical example.</p> <p>First, let's modify our <code>requirements.txt</code> file to include additional dependencies:</p> <pre><code>numpy\nmatplotlib\n</code></pre> <p>Now, let's rebuild the image: <pre><code>docker build . -t obriens/part1\n[+] Building 17.2s (10/10) FINISHED                                                                      docker:default\n =&gt; [internal] load .dockerignore                                                                                  0.0s\n =&gt; =&gt; transferring context: 2B                                                                                    0.0s\n =&gt; [internal] load build definition from Dockerfile                                                               0.0s\n =&gt; =&gt; transferring dockerfile: 506B                                                                               0.0s\n =&gt; [internal] load metadata for docker.io/library/python:3.9-slim                                                 0.3s\n =&gt; [1/5] FROM docker.io/library/python:3.9-slim@sha256:e9074b2ea84e00d4a73a7d0c01c52820e7b68d8901c5fa282be4f1b28  0.0s\n =&gt; [internal] load build context                                                                                  0.0s\n =&gt; =&gt; transferring context: 89B                                                                                   0.0s\n =&gt; CACHED [2/5] WORKDIR /app                                                                                      0.0s\n =&gt; [3/5] COPY requirements.txt .                                                                                  0.0s\n =&gt; [4/5] RUN pip install --no-cache-dir -r requirements.txt                                                      15.8s\n =&gt; [5/5] COPY app.py .                                                                                            0.0s\n =&gt; exporting to image                                                                                             0.8s\n =&gt; =&gt; exporting layers                                                                                            0.8s\n =&gt; =&gt; writing image sha256:449c5e51b012be670f98fb5f33a2b7cd1ddc50dddf4f42f2f040fb69f0e4c2c7                       0.0s\n =&gt; =&gt; naming to docker.io/obriens/part1                                                                           0.0s\n</code></pre> Notice the output shows: <pre><code>...\n =&gt; CACHED [2/5] WORKDIR /app                                                                                      0.0s\n...\n</code></pre></p> <p>What's happening here is that Docker has \"cached\" the previous image layers. This means that we don't need to rerun those stages. Instead, Docker starts from the last cached layer and continues building from there.</p> <p>Now, let's modify <code>app.py</code> to print out the median of the sample as well: <pre><code>import numpy as np\n\ndef generate_random_numbers(num_points):\n    return np.random.rand(num_points)\n\ndef calculate_statistics(numbers):\n    mean = np.mean(numbers)\n    median = np.median(numbers)\n    std_dev = np.std(numbers)\n    return mean, median, std_dev\n\nif __name__ == \"__main__\":\n    num_points = 1000  # Size of the random number list\n    numbers = generate_random_numbers(num_points)\n    mean, median, std_dev = calculate_statistics(numbers)\n    print(f\"Generated {num_points} random numbers\")\n    print(f\"Mean: {mean}\")\n    print(f\"Median: {median}\")\n    print(f\"Standard Deviation: {std_dev}\")\n</code></pre></p> <p>and build this with: <pre><code>docker build . -t obriens/part1\n[+] Building 0.5s (10/10) FINISHED                                                                       docker:default\n =&gt; [internal] load .dockerignore                                                                                  0.0s\n =&gt; =&gt; transferring context: 2B                                                                                    0.0s\n =&gt; [internal] load build definition from Dockerfile                                                               0.0s\n =&gt; =&gt; transferring dockerfile: 506B                                                                               0.0s\n =&gt; [internal] load metadata for docker.io/library/python:3.9-slim                                                 0.3s\n =&gt; [1/5] FROM docker.io/library/python:3.9-slim@sha256:e9074b2ea84e00d4a73a7d0c01c52820e7b68d8901c5fa282be4f1b28  0.0s\n =&gt; [internal] load build context                                                                                  0.0s\n =&gt; =&gt; transferring context: 674B                                                                                  0.0s\n =&gt; CACHED [2/5] WORKDIR /app                                                                                      0.0s\n =&gt; CACHED [3/5] COPY requirements.txt .                                                                           0.0s\n =&gt; CACHED [4/5] RUN pip install --no-cache-dir -r requirements.txt                                                0.0s\n =&gt; [5/5] COPY app.py .                                                                                            0.0s\n =&gt; exporting to image                                                                                             0.0s\n =&gt; =&gt; exporting layers                                                                                            0.0s\n =&gt; =&gt; writing image sha256:455ae0e0323b56ada03933dc2523c7e7fcbe312b25f04536aa8f6cca14943349                       0.0s\n =&gt; =&gt; naming to docker.io/obriens/part1                                                                           0.0s\n\nWhat's Next?\n  View a summary of image vulnerabilities and recommendations \u2192 docker scout quickview\n</code></pre> Again, notice that Docker uses cached versions of each layer, avoiding recreating previous steps. This concept is crucial when creating images\u2014frequent development and rebuilding can be sped up by caching unchanged steps early in the Dockerfile. However, any change detected in a later step requires rerunning all subsequent steps, regardless of whether they'll result in changes.</p>"},{"location":"Containerization/docker/Part1/#managing-docker-containers","title":"Managing Docker Containers","text":""},{"location":"Containerization/docker/Part1/#docker-networking-basics","title":"Docker networking basics","text":"<p>We can enable networking between our Docker image and the host system. Let's create an image that requires networking. Suppose we've encountered difficulty installing a Python package on our local machine but know it installs correctly on another. We'll build an image containing this Python package, based on the <code>python:3.9-slim</code> base image. Given that the base image already includes most dependencies, our <code>Dockerfile</code> will be brief: <pre><code>FROM python:3.9-slim\n\n# Use pip to install the requirements\nRUN pip install numpy matplotlib jupyterlab notebook ipykernel ipython ipywidgets\n\n\n# Create a new user and change to that user\nRUN useradd europa\nUSER europa\n# Move the europa's home directory\nWORKDIR /home/europa\n\nCMD [\"jupyter\", \"lab\", \"--ip=0.0.0.0\", \"--port=8000\"]\n</code></pre></p> <p>Here, the <code>CMD [\"jupyter\", \"lab\"]</code> command will start a JupyterLab server. The <code>--ip=0.0.0.0</code> flag binds the server to all available IP addresses, making it accessible to the outside world. The <code>--port=8000</code> option specifies that we're starting the Jupyter server on port 8000.</p> <p>We've also created a new user within the container called <code>europa</code>. By default, Docker runs everything as <code>root</code>. This poses several potential issues. Firstly, anyone who can run this container gains root access to everything accessible by the container. For instance, if a file system is accessible to the container (as we'll see shortly), the user would have root privileges within that folder, enabling them to delete or modify any files. This can be particularly problematic on shared systems like computer clusters and poses a significant security concern. If an unauthorized party gains access to the container, they could execute commands as root, potentially installing and running malicious code within the container.</p> <p>In the example, we created a new user using <code>RUN useradd europa</code>, switched to that user using <code>USER europa</code>, and set their home directory to <code>/home/europa</code> using <code>WORKDIR /home/europa</code>. Once we invoked <code>USER europa</code>, all subsequent commands ran as that user, meaning any files created would be owned by that user. Since we haven't granted this user sudo access, they cannot execute commands requiring sudo permissions.</p> <p>Saving the Dockerfile in a subdirectory (<code>jupyter_example/Dockerfile</code>), we can build this as: <pre><code>docker build ./jupyter_example -t obriens/part1-jupyter\n</code></pre></p> <p>Note how we've specified the location of the <code>Dockerfile</code> by passing the path to <code>./jupyter_example</code> instead of <code>.</code>. Once built, this image can be run as: <pre><code>docker run --rm -it obriens/part1-jupyter\n</code></pre></p> <p>Initially, the Jupyter Lab server starts, but we cannot access it by navigating to <code>localhost/8000</code>. This is because we need to map ports from the container to our host system. If we stop the container and rerun it with port mapping: <pre><code>docker run --rm -it -p 8000:8000 obriens/part1-jupyter\n</code></pre></p> <p>Now, we can navigate to localhost/8000 and see the Jupyter Lab server running!</p> <p>If we shut down this container and launch a new one, we'll notice that the files created previously are no longer there. This happens because the <code>/home/europa</code> filesystem within the container is deleted when the container is deleted. To maintain persistence, we can mount a local directory at runtime using: <pre><code>docker run --rm -it -p 8000:8000 -v $(pwd):/home/europa obriens/part1-jupyter\n</code></pre> Here, the <code>-v</code> or <code>--volume</code> flag mounts <code>$(pwd)</code> (the current directory) on the host to <code>/home/europa</code> within the container, ensuring that files created or modified in <code>/home/europa</code> persist beyond the container's lifecycle.</p>"},{"location":"Containerization/docker/Part1/#permission-errors-build-arguments-and-entry-points","title":"Permission errors, build arguments and entry points","text":"<p>It is possible that you still cannot save files to this directory. This might be because the <code>europa</code> user that we've created has a different user ID than the user who is running the Docker container.</p> <p>To resolve this issue, we can use an \"ARG\" or argument within our Docker image. When creating the <code>europa</code> user, we can specify the user ID at build time. We can add the following lines to our <code>Dockerfile</code>: <pre><code>...\n# Create a new user and change to that user\nARG UID=1000\nRUN useradd -m europa -u $UID\nUSER europa\n...\n</code></pre> This defines a new argument called <code>UID</code> with a default value of <code>1000</code>. The value of <code>UID</code> is then used when setting the ID of the europa user. When building the image, we can set this value to the ID of the current user with: <pre><code>docker build  --build-arg UID=$(id -u) ./jupyter_example -t obriens/part1-jupyter\n</code></pre> Here, <code>--build-arg UID=$(id -u)</code> sets the <code>UID</code> argument to the current user's ID (<code>$(id -u)</code>). This ensures that the user within the container has the same ID and permissions as the user who built the image.</p> <pre><code>docker run --rm -it -p 8000:8000 -v $(pwd):/home/europa:rw obriens/part1-jupyter\n</code></pre> <p>Note the <code>:rw</code> after the volume mounting. Similarly, we can restrict permissions when mounting using <code>:ro</code> to specify read-only. This can be useful when handling files or directories that we don't want the user to modify.</p> <p>Using <code>--build-arg</code> allows us to specify arguments at build time, which may not always be practical if we don't know the parameters the user will set at runtime. For example, building an image that assumes a specific user ID (e.g., <code>1000</code>) might not suit another user with a different ID (e.g., <code>1001</code>).</p> <p>To address this, we can refactor our image to use variables passed at runtime using an <code>ENTRYPOINT</code>. <code>ENTRYPOINT</code> defines a command or script that is executed upon starting the container, before the default command specified with <code>CMD</code> or using <code>docker run</code>. Consider the following script (<code>entrypoint.sh</code>): <pre><code>#!/bin/bash\n\nif [ -z \"$UID\" ] || [ $UID -eq 0 ]; then\n    USER_ID=1000\nelse\n    USER_ID=$UID\nfi\n\n# Create a new user with the specified UID\nuseradd -u $USER_ID -s /bin/bash europa\n\n# Change ownership of the home directory\nchown -R $USER_ID:$USER_ID /home/europa\n\n# Switch to the new user and execute the command\nexec gosu europa \"$@\"\n</code></pre></p> <p>This script sets the <code>USER_ID</code> parameter based on the <code>UID</code> passed at runtime (defaulting to <code>1000</code> if not specified or if <code>UID</code> is <code>0</code>). It then creates a new user <code>europa</code>, changes ownership of <code>/home/europa</code>, and switches to that user to execute further commands.</p> <p>We can incorporate this script into our <code>Dockerfile</code>: <pre><code>FROM python:3.9-slim\n\nRUN apt-get update &amp;&amp; apt-get install -y gosu &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Use pip to install the requirements\nRUN pip install numpy matplotlib jupyterlab notebook ipykernel ipython ipywidgets\n\n# Create a new user and change to that user\nADD entrypoint.sh /home/europa/entrypoint.sh\nRUN chmod +x /home/europa/entrypoint.sh\n\n# # Move the europa's home directory\nWORKDIR /home/europa\n\n# Set the entrypoint\nENTRYPOINT [ \"/home/europa/entrypoint.sh\" ]\n\n\nCMD [\"jupyter\", \"lab\", \"--ip=0.0.0.0\", \"--port=8000\"]\n</code></pre></p> <p>This <code>Dockerfile</code> installs gosu, a utility for running commands as another user, and adds <code>entrypoint.sh</code>, making it executable. The <code>ENTRYPOINT</code> directive specifies the entry point script for the image.</p> <p><pre><code>RUN apt-get update &amp;&amp; apt-get install -y gosu &amp;&amp; rm -rf /var/lib/apt/lists/*\n</code></pre> This command updates the OS and installs <code>gosu</code>, before cleaning up the install meta data. <pre><code>ADD entrypoint.sh /home/europa/entrypoint.sh\nRUN chmod +x /home/europa/entrypoint.sh\n</code></pre> This command adds the <code>entrypoint.sh</code> to the home directory of some user <code>europa</code>.</p> <p><pre><code># Set the entrypoint\nENTRYPOINT [ \"/home/europa/entrypoint.sh\" ]\n</code></pre> This line specifies the entrypoint script for the image. </p> <p>Let's build and test the image: <pre><code>docker build ./jupyter_example -t obriens/part1-jupyter \n&gt; docker run --rm -it   obriens/part1-jupyter id\n\nuid=1000(europa) gid=1000(europa) groups=1000(europa)\n</code></pre></p> <p>By default, the <code>europa</code> user's ID is set to <code>1000</code>. We can modify this at runtime using <code>-e</code> to specify a variable: <pre><code>&gt; docker run --rm -it  -e UID=1001  obriens/part1-jupyter id\n\nuid=1001(europa) gid=1001(europa) groups=1001(europa)\n</code></pre></p> <p>It's best practice to avoid using the <code>root</code> user by default and to avoid hard-coding values like default user IDs. Using <code>--build-arg</code> and <code>ENTRYPOINT</code> provides flexibility at build and runtime, allowing Docker images to be configured dynamically based on user requirements.</p>"},{"location":"Containerization/docker/Part1/#container-lifecycle-start-stop-remove","title":"Container lifecycle: start, stop, remove","text":"<p>The above Jupyter server is an excellent example of a service that can run in the background. Let's launch the container and detach from it using: <pre><code>docker run -d -it -p 8000:8000 -v $(pwd):/home/europa:rw --name jupyter obriens/part1-jupyter\n</code></pre> Here's what each flag does: - Removed the <code>--rm</code> flag to prevent the container from automatically deleting after exiting. - Added <code>-d</code> to detach the running container, allowing it to run in the background. - Specified a name using <code>--name jupyter</code> for easier identification and management.</p> <p>If we navigate to <code>localhost:8000</code>, we'll notice that we need to log in to the server using a token. This token would have been printed as output if we hadn't used the <code>-d</code> flag. To retrieve the token, we can attach a new terminal to the running container: <pre><code>docker exec -it jupyter bash\n</code></pre> This command starts a new interactive bash shell within the <code>jupyter</code> container (<code>-it</code> ensures an interactive session). From here, we can get the login token for the Jupyter server using: <pre><code>jupyter server list\nCurrently running servers:\nhttp://8de969dd1385:8000/?token=84f69c1c8c61944fd6e322526db1c236457dc9b62fe15ffb :: /home/europa\n</code></pre> Copy and paste the token (<code>84f69c1c8c61944fd6e322526db1c236457dc9b62fe15ffb</code> in this example) into your web browser to log in to the Jupyter server. Once we're finished with the Jupyter server, we can stop it using: <pre><code>docker stop jupyter\n</code></pre> To restart it later, use: <pre><code>docker start jupyter\n</code></pre></p> <p>Keeping regularly used containers set up and ready to run can be very useful. Some use case examples include:</p> <ul> <li>Debugging tools: Such as Valgrind  which can be challenging to install on Mac or Windows directly.</li> <li>Analysis Pipelines: Tools like Heasoft that allow running pipelines on locally stored data.</li> <li>Specific Development Environments: Such as legacy Python versions (e.g., Python 2) that may be difficult to compile on modern systems.</li> </ul> <p>By managing containers effectively, developers can streamline their workflows and maintain consistent environments across different platforms and projects.</p>"},{"location":"Containerization/docker/Part1/#summary","title":"Summary","text":"<p>In this workshop, we've covered the basics of Docker:</p> <ul> <li>Running pre-built images.</li> <li>Building custom images and understanding image layers to optimize development.</li> <li>Configuring containers that require networking and mapping ports between the host system and the container.</li> <li>Mounting volumes within containers and managing permissions to control file access.</li> </ul> <p>In the next workshop, we will explore more advanced Docker features and dive into using Docker Compose to simplify managing and orchestrating Docker containers.</p>"},{"location":"Containerization/docker/Part1/app/","title":"App","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n</pre> import numpy as np In\u00a0[\u00a0]: Copied! <pre>def generate_random_numbers(num_points):\n    return np.random.rand(num_points)\n</pre> def generate_random_numbers(num_points):     return np.random.rand(num_points) In\u00a0[\u00a0]: Copied! <pre>def calculate_statistics(numbers):\n    mean = np.mean(numbers)\n    median = np.median(numbers)\n    std_dev = np.std(numbers)\n    return mean, median, std_dev\n</pre> def calculate_statistics(numbers):     mean = np.mean(numbers)     median = np.median(numbers)     std_dev = np.std(numbers)     return mean, median, std_dev In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    num_points = 1000  # Size of the random number list\n    numbers = generate_random_numbers(num_points)\n    mean, median, std_dev = calculate_statistics(numbers)\n    print(f\"Generated {num_points} random numbers\")\n    print(f\"Mean: {mean}\")\n    print(f\"Median: {median}\")\n    print(f\"Standard Deviation: {std_dev}\")\n</pre> if __name__ == \"__main__\":     num_points = 1000  # Size of the random number list     numbers = generate_random_numbers(num_points)     mean, median, std_dev = calculate_statistics(numbers)     print(f\"Generated {num_points} random numbers\")     print(f\"Mean: {mean}\")     print(f\"Median: {median}\")     print(f\"Standard Deviation: {std_dev}\")"},{"location":"Containerization/docker/Part1/test/","title":"Test","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n</pre> import numpy as np import matplotlib.pyplot as plt In\u00a0[3]: Copied! <pre>x = np.linspace(-3,3)\ny = np.sin(x)\nplt.plot(x,y)\nplt.plot(x,np.cos(x))\nplt.grid()\nplt.savefig(\"myplot.png\")\n</pre> x = np.linspace(-3,3) y = np.sin(x) plt.plot(x,y) plt.plot(x,np.cos(x)) plt.grid() plt.savefig(\"myplot.png\") In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Cpp/","title":"C++ Tutorials","text":"<ul> <li>Introduction to C++</li> <li>Introduction to Parallel Programming in C++ with OpenMP</li> </ul>"},{"location":"Cpp/cpp-tutorial/","title":"Introduction to C++","text":""},{"location":"Cpp/cpp-tutorial/#what-is-the-aim-of-this-tutorial","title":"What is the aim of this tutorial?","text":"<p>The aim of this tutorial is to provide you with an introduction to the C++ programming language. I assume that you have some background in programming, so I will skip topics like boolean logic. I'll also assume that you are not coming from a computer science background. We'll skip topics such as references and pointers, which may be the subject of a future workshop. Instead, we will focus on the core components involved in writing a C++ program.</p> <p>By the end of this tutorial, you will be able to do the following:</p> <ul> <li>Understand data types: You will understand some of the different data types in C++ and when you might want to use one over the other.</li> <li>Understand the structure of C++ code: You'll understand how to structure your code and learn some of the best practices.</li> <li>Understand loops and statements: You'll understand how <code>for</code> and <code>while</code> loops work. We'll also touch on <code>do while</code> loops. We'll also learn how to control flow using <code>if</code> - <code>else if</code> - <code>else</code> statements.</li> <li>Use arrays and vectors: We'll look at how to create arrays and vectors. We'll learn about some best practices for memory safety.</li> <li>Functions: You'll understand how to write reusable code using functions. We'll learn the difference between a value and a reference when dealing with functions.</li> <li>Parsing files and strings: You'll understand how to print to the command line and to files. You'll learn how to read text files, parse them, and extract useful information.</li> </ul>"},{"location":"Cpp/cpp-tutorial/#how-to-run-this-tutorial","title":"How to run this tutorial","text":"<p>The code used in this tutorial is available on GitHub. I strongly recommend checking out the exercise branch and trying to work along with the notes here. Try to answer the problems before reading the solutions. If you get stuck, you can always check out the solutions branch.</p> <p>To run this tutorial, you'll need <code>g++</code> or <code>clang++</code> installed. These can be installed through the <code>gcc</code> or <code>clang</code> packages. Instructions on how to install these can be found here. Alternatively, you can compile all the examples using the GCC Docker image. You can pull the latest Docker image using:</p> <pre><code>docker pull gcc\n</code></pre>"},{"location":"Cpp/cpp-tutorial/#simplifying-docker-setup","title":"Simplifying Docker Setup","text":"<p>To save time and streamline your Docker setup, consider pre-downloading the necessary image. Once you have the image, you can easily create a container to compile and execute your code. Here's how to do it:</p> <ol> <li> <p>Pull the Docker Image:</p> <p>Use the following command to pull the <code>gcc</code> image:</p> <p><pre><code>docker pull gcc\n</code></pre> This command downloads the image and stores it locally on your system.</p> </li> <li> <p>Create a Docker Container:</p> <p>Now, you can create a Docker container for compiling and running your code. Use the following command: <pre><code>docker run -it --rm -v $(pwd):/data -w /data gcc bash\n</code></pre></p> <p>Let's break down this command:</p> <ul> <li>`docker run`` initiates a new container.</li> <li><code>-it</code> specifies that you want an interactive shell.</li> <li><code>--rm</code> automatically removes the container when you exit.</li> <li><code>-v $(pwd):/data</code> mounts the current directory into the container at the location /data.</li> <li><code>-w /data</code> sets the working directory to /data within the container.</li> </ul> </li> </ol>"},{"location":"Cpp/cpp-tutorial/#what-is-c","title":"What is C++","text":"<p>C++ is one of the most influential programming languages today, known for its exceptional performance. It empowers developers to create memory-efficient code that often outperforms native Python in terms of speed. Whether you are delving into the world of IoT or tackling high-performance computing, C++ remains a top choice for projects that prioritize efficient memory utilization and speedy execution.</p>"},{"location":"Cpp/cpp-tutorial/#key-differences-from-python","title":"Key Differences from Python","text":"<p>Coming from a language like Python, you will immediately notice several differences when working with C++:</p> <ol> <li> <p>Compiled Language:    Unlike Python, which can be executed through an interpreter, C++ is a \"compiled\" language. This means that before running C++ code, it must be compiled. Compilation takes code that is relatively readable to humans and transforms it into low-level machine code. One of the benefits of a compiled language is that the compiled program can be highly optimized at compile time, resulting in high-performing code.</p> </li> <li> <p>Statically-Typed Language:    Unlike Python, where variable types are determined at runtime, C++ requires knowing the type of a variable at compile time. Python is a \"dynamically-typed\" language, allowing you to work with variables without specifying their type in advance. In C++, specifying the type during compilation may seem limiting, but it enables the compiler to optimize the executable for better performance.</p> </li> <li> <p>Manual Memory Management:    In Python, the interpreter regularly pauses execution to check which variables are no longer within scope and to free up memory, thanks to the \"garbage collector.\" While this automated memory management is convenient, it incurs a significant performance overhead. In C++, when a variable goes out of scope, its \"destructor\" is typically called. The destructor is a function that handles memory cleanup. However, in C++, developers need to be mindful because they can allocate memory dynamically using <code>new</code> or <code>alloc</code>. Such allocated memory must be explicitly released using <code>delete</code> or <code>dealloc</code> to avoid memory leaks.</p> </li> <li> <p>Limited Memory Safety:    In Python, attempting to access an element beyond the boundaries of an array, such as the 11th element in a 10-element array, results in an error. In C++, however, such boundary violations allow access to memory that should not be touched. This leads to completely undefined behavior.</p> </li> <li> <p>Thread-Friendly:    Python imposes the \"Global Interpreter Lock\" (GIL), restricting the execution of CPU-bound tasks to one at a time, regardless of the available cores. This is a memory safety feature aimed at avoiding data races. In C++, there is no such constraint. Developers can create a nearly infinite number of logical threads, and the code can utilize the available CPU cores for temporally concurrent threads. Memory safety and data race avoidance are managed using atomic types or barriers (for more details, see Introduction to Parallel Programming in C++ with OpenMP).</p> </li> </ol> <p>These differences reflect the unique strengths and characteristics of C++, making it a powerful and versatile programming language.</p>"},{"location":"Cpp/cpp-tutorial/#why-use-c","title":"Why use C++?","text":"<p>C++ allows us to write fast and memory efficient code that outpaces Python on most metrics.  Being a compiled language, we can pass pre-compiled binaries to end users, providing highly optimized executables ready for use.  The low foot print makes C++ ideal for devices such as microcontrollers (e.g. Arduinos), FPGAs and any memory limited devices. </p> <p>C++ is very common in scientific programming.  Complex memory intensive code (such as scientific simulations), often require huge resources. C++ naturally fufills these requirements, with code that easily scales.</p>"},{"location":"Cpp/cpp-tutorial/#the-anatomy-of-a-c-program","title":"The Anatomy of a C++ program","text":""},{"location":"Cpp/cpp-tutorial/#hello-world","title":"Hello World","text":"<p>Like all good tutorials, we start with the basic \"Hello World\" example:</p> hello_world.cpp<pre><code>#include &lt;iostream&gt;\n\nint main(){\n    std::cout &lt;&lt; \"Hello World\" &lt;&lt; std::endl;\n\n    return 0;\n}\n</code></pre> <p>Here we have an example code that prints \"Hello World\" to the terminal. Let's talk through this line-by-line.</p> <ol> <li> <p><code>#include &lt;iostream&gt;</code> Here we are \"including\" an external library to our code. We're using the library \"iostream\", a library that allows us to use functions relating to input and output. <code>iostream</code> is part of the C++ standard library, a vast set of code that we can build upon.</p> </li> <li> <p><code>int main() {...}</code> Here we are defining our <code>main</code> function. Executables should have a <code>main</code> function. This tells the complier that this is the entry point to the program. </p> </li> <li> <p><code>std::cout &lt;&lt; \"Hello World\" &lt;&lt; std::endl;</code> Here we are using <code>std::cout</code> to print a message to the screen. We are specifying the standard name space <code>std</code>. We pass the string \"Hello World\" to <code>std::cout</code> using <code>&lt;&lt;</code>. We then pass an additional arguement <code>std::endl</code>. <code>endl</code> passes the \"end line\" command to the <code>cout</code> essentially terminating the line. Finally we end the line with <code>;</code>. </p> </li> <li> <p><code>return 0</code> Here we are ending our <code>main</code> function by return 0. A program will return an exit code when it terminates. Exit codes tell the user about how the code finishes. An exit code of 0 means the code terminated successfully. We could return any number we want, but 0 typically means a successfull termination.</p> </li> </ol> <p>We can compile this code using either <code>g++</code> or <code>clang++</code>: <pre><code>g++ hello_world.cpp -o hello_world\n</code></pre></p> <p>or <pre><code>clang++ hello_world.cpp -o hello_world\n</code></pre></p> <p>Here the first argument <code>hello_world.cpp</code> is the source code we want to compile. We specify the target binary with the <code>-o</code> flag. We name the output executable <code>hello_world</code>. We can run our code using:</p> <pre><code>./hello_world\n</code></pre>"},{"location":"Cpp/cpp-tutorial/#data-types","title":"Data Types","text":"<p>In C++, understanding different data types is crucial due to its static typing nature. While the actual size of these types may vary depending on the compiler and system, here are common data types:</p> <ul> <li><code>int</code>: Used for integer numbers (e.g., -1, 0, 23). Typically, it occupies 4 bytes or 32 bits.</li> <li><code>bool</code>: Represents Boolean values, either <code>true</code> or <code>false</code>, allowing for Boolean arithmetic.</li> <li><code>float</code>: Used for non-integer numbers (e.g., -0.2, 43.4, 12.0) with 32-bit precision.</li> <li><code>double</code>: Similar to <code>float</code> but with double the precision (64 bits).</li> <li><code>char</code>: Represents a single character or small integer value and typically occupies 8 bits (1 byte) of memory.</li> </ul> <p>Additional data types include:</p> <ul> <li><code>unsigned int</code>: Integer numbers that are unsigned, expanding the maximum possible value of the integer.</li> <li><code>short int</code>: Signed integers with half the size (2 bytes or 16 bits).</li> <li><code>long</code>: Allows storage of larger numbers than standard <code>int</code>, typically occupying 8 bytes or 64 bits.</li> </ul> <p>Beyond these basic types, you will also encounter during this tutorial:</p> <ul> <li><code>string</code>: A sequence of characters stored as an array of <code>char</code> elements, used to represent text or character data.</li> <li><code>fstream</code>: Used for file input and output operations, enabling reading from and writing to files.</li> </ul> data_types.cpp<pre><code>#include &lt;iostream&gt;\n// Include the &lt;limits&gt; header for accessing information about data type limits.\n#include &lt;limits&gt;\n\nint main() {\n\n    // int - 4-byte integer (minimum/maximum values):\n    int a = 42;\n    std::cout &lt;&lt; \"integer a = \" &lt;&lt; a &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"Minimum value for int: \" &lt;&lt; std::numeric_limits&lt;int&gt;::min() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"Maximum value for int: \" &lt;&lt; std::numeric_limits&lt;int&gt;::max() &lt;&lt; std::endl;\n\n\n    // Precision of float and double:\n    std::cout.precision(20);\n    float b = 3.14159265358979323846;\n    std::cout &lt;&lt; \"float b = \\t\" &lt;&lt; b &lt;&lt; std::endl;\n\n    double c = 3.14159265358979323846;\n    std::cout &lt;&lt; \"double c = \\t\" &lt;&lt; c &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"Full      \\t\" &lt;&lt; \"3.14159265358979323846\" &lt;&lt; std::endl;\n\n    // Display minimum and maximum values for float and double:\n    std::cout &lt;&lt; \"Minimum value for float: \" &lt;&lt; std::numeric_limits&lt;float&gt;::min() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"Maximum value for float: \" &lt;&lt; std::numeric_limits&lt;float&gt;::max() &lt;&lt; std::endl;\n\n    std::cout &lt;&lt; \"Minimum value for double: \" &lt;&lt; std::numeric_limits&lt;double&gt;::min() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"Maximum value for double: \" &lt;&lt; std::numeric_limits&lt;double&gt;::max() &lt;&lt; std::endl;\n\n\n    return 0;\n}\n</code></pre> <p>On line 3 we are including the <code>limits</code> library. We are using the <code>std::numeric_limits&lt;T&gt;::min()</code> and <code>std::numeric_limits&lt;T&gt;::max()</code> method to get the minimum and maximum value of a data type <code>T</code>. Don't worry too much about the sytax here, they indicate that we're using a template, allowing us to appy these functions to different types of data types.</p> <p>When we compile this and look at the output we see: <pre><code>integer a = 42\nMinimum value for int: -2147483648\nMaximum value for int: 2147483647\nfloat b =       3.1415927410125732422\ndouble c =      3.141592653589793116\nFull            3.14159265358979323846\nMinimum value for float: 1.175494350822287508e-38\nMaximum value for float: 3.4028234663852885981e+38\nMinimum value for double: 2.2250738585072013831e-308\nMaximum value for double: 1.7976931348623157081e+308\n</code></pre></p> <p>We can determine the minimum and maximum values for <code>int</code>, <code>float</code>, and <code>double</code>. It's evident that when using a <code>float</code> instead of a <code>double</code>, precision is reduced. In the example above, the value of \\(\\pi\\) is accurate to the 6th decimal place for <code>float</code>, but extends to the 15th decimal place for <code>double</code>. This difference becomes significant when a high degree of precision is required.</p>"},{"location":"Cpp/cpp-tutorial/#namespaces","title":"Namespaces","text":"<p>In C++, namespaces serve as a way to organize and group related code elements, including variables, functions, and classes, into distinct logical scopes. This organization helps prevent naming conflicts and enhances the modularity of your code.</p> <p>In our examples, we will primarily rely on the \"standard\" C++ library. It's beneficial to specify which namespace we are using to save us from having to write it out every time.</p> <p>Consider the <code>hello_world.cpp</code> example, where we can streamline the code by explicitly indicating the namespace we're using:</p> hello_world.cpp<pre><code>#include &lt;iostream&gt;\n\nusing namespace std;\n\nint main(){\n    cout &lt;&lt; \"Hello World\" &lt;&lt; endl;\n\n    return 0;\n}\n</code></pre> <p>By employing <code>using namespace std;</code>, we eliminate the need to specify that we are using <code>cout</code> from the <code>std</code> namespace. The compiler will automatically assume the <code>std</code> namespace.</p> <p>While the examples we use here may not involve multiple namespaces, it is generally considered good practice to be explicit and specify the namespace of the elements you are using. This helps avoid issues when working with libraries that may contain classes or functions sharing the same names.</p>"},{"location":"Cpp/cpp-tutorial/#scopes","title":"Scopes","text":"<p>In C++ we can make use of multiple \"Scopes\". You can think of scopes as blocks of code. Within these blocks we can define variables, perform operations, enter nested scopes, etc. In general, when we exit a scope any objects created within that scope will have their <code>destructor</code> called once we exit the scope (with the execption of objects created with <code>new</code>or  <code>alloc</code>). Objects native to a scope cannot be accessed from outside that scope, but objects within a scope can be accessed from within a nested scope. We can define a scope in C++ by wrapping a body of code between curley brackets.</p> scopes.cpp<pre><code>#include &lt;iostream&gt;\n\nusing namespace std;\n\nint main() {\n    int a = 123;\n    int b = 456;\n\n    cout &lt;&lt; \"a is \" &lt;&lt; a &lt;&lt; endl;\n    cout &lt;&lt; \"b is \" &lt;&lt; b &lt;&lt; endl;\n\n    // Creating a new \"nested\" scope within our main function\n    {\n        cout &lt;&lt; \"Within the nested scope\" &lt;&lt; endl;\n\n        // Here, we are \"shadowing\" the variable named 'a'.\n        // Within the nested scope, we have a new 'a', which temporarily hides the outer 'a'.\n        // This is allowed, but it can lead to confusion and is generally considered poor practice.\n        int a = 43;  // Shadowing the outer 'a'\n\n        // The variable 'c' is created locally within this scope.\n        int c = 789;\n\n        cout &lt;&lt; \"a is \" &lt;&lt; a &lt;&lt; endl;    // Refers to the 'a' in the nested scope\n        cout &lt;&lt; \"b is \" &lt;&lt; b &lt;&lt; endl;    // Accesses the outer 'b'\n        cout &lt;&lt; \"c is \" &lt;&lt; c &lt;&lt; endl;    // Refers to the 'c' in the nested scope\n        cout &lt;&lt; \"Leaving the nested scope\" &lt;&lt; endl;\n    }\n\n    cout &lt;&lt; \"a is \" &lt;&lt; a &lt;&lt; endl;    // Refers to the outer 'a'\n    cout &lt;&lt; \"b is \" &lt;&lt; b &lt;&lt; endl;    // Accesses the outer 'b'\n\n    // This will cause an error!\n    // Introduction/scopes.cpp:27:24: error: 'c' was not declared in this scope\n    // 36 |     cout &lt;&lt; \"c is \" &lt;&lt; c &lt;&lt; endl;\n    // cout &lt;&lt; \"c is \" &lt;&lt; c &lt;&lt; endl;\n    // This is because 'c' was only local to the nested scope; the outer scope cannot access it.\n\n    return 0;\n}\n</code></pre> <p>On line 3 we define a new scope associated with the <code>main</code> function, this scope ends on line 40. We define integers <code>a</code> and <code>b</code> in the <code>main</code> scope. We created a new scope on line 13 which spans until line 28. This scope is \"nested\" within the main scope. Within this scope we define the integer <code>a</code> on line 19. What we are doing here is known as \"shadowing\" we are borrowing the use of the name within this scope. When we exit the scope the varable <code>a</code> reverts back to what it was outside of the scope, this is evident on line 30. On line 22 we define an integer <code>c</code> within the scope. This variable is local to the nested scope, but inaccessible to the parent scope (the <code>main</code> scope). When we exit the nested scope the <code>destructor</code> is called and the variable is dropped. If we uncommented line 36 we would get an error.</p> <p>Scopes are general blocks of code, we can modify the behaviour of the scope using loops and control access to the scope using statements. We'll learn about these in the next section.</p>"},{"location":"Cpp/cpp-tutorial/#flow-control","title":"Flow Control","text":"<p>In this sections we'll look at how to control the flow of our code using <code>for</code>, <code>while</code> and <code>do while</code> loops and control access to scopes using <code>if</code>-<code>else if</code> and <code>else</code> statements. We won't touch <code>goto</code> and neither should you. We may briefly discuss <code>switch</code>.</p>"},{"location":"Cpp/cpp-tutorial/#for-loops","title":"For loops","text":"<p><code>for</code> loops are used when we have a loop with a known number of iterations or a regular pattern we want to iterate over. In C++ they are defined as follows:</p> <pre><code>for ( initialization ; termination ; iteration){\n    // block of code to be ran\n}\n</code></pre> <p>The <code>for</code> loop has 3 fields each separated by a <code>;</code>. You can think of them as the following: - initialization : What we want to initialize at the start of the loop. This could be variables that we want to modify within the loop. For example <code>int i = 0;</code>. We can also initilize multiple variables, for example <code>int i = 0,  j = 5;</code>. In this case we are defining <code>i</code> within the scope of the <code>for</code> loop, but <code>j</code> would be local to the parent loop. - termination : The condition on which we will terminate the loop. This needs to be a boolean. For example if we want to exit the loop when i passes a certain value: <code>i &lt; 10</code>. While <code>i &lt; 10</code> this will be true, however when <code>i == 10</code>, this becomes false so we exit. - iteration : This field allows us to change something at the end each iteration of the loop. So for example if we want to increase the value of <code>i</code> by 1 we could use <code>i++</code></p> <p>Putting these together we can write a for loop, to loop from 0-9 as follows: <pre><code>// Print out 0-9\nfor ( int i = 0; i &lt; 10 ; i++ ){\n    cout &lt;&lt; i &lt;&lt; endl;\n}\n</code></pre> Starting at <code>i = 0</code> we pass through the loop. When we reach the end of the loop on line 4, <code>i++</code> will run and we will restart at the beginning of the loop with <code>i = 1</code>. The loop will check if <code>1 &lt; 10</code>, find <code>true</code> and continue running the loop. This will contiune until <code>i = 9</code>, when we finish this iteration of the loop we will apply <code>i++</code>, so now <code>i = 10</code>. When we try to reenter the loop the <code>10 &lt; 10</code> returns <code>false</code> and we exit the loop.</p> <p>You can create an infinite loop by not passing a termination condition. <pre><code>// This code will run forever!\nfor (int i = 0; ; i++){\n    // block of code    \n}\n</code></pre></p> <p>Creating a for loop using a range</p> <p>Since C++ 11 (the 2011 update of C++), one can loop over ranges and arrays using the following format: <pre><code>int values[] = {0,1,2,3,4,5};\nfor (int i: values){\n    cout &lt;&lt; i &lt;&lt; endl;\n}\n</code></pre></p>"},{"location":"Cpp/cpp-tutorial/#while-loops","title":"While loops","text":"<p>In C++, <code>while</code> loops iterate over a block of code as long as a specified condition is true. The basic format of a <code>while</code> loop is as follows: <pre><code>while (condition){\n    // block of code\n}\n</code></pre> In boolean logic, any condition that is not <code>false</code> or <code>0</code> is considered <code>true</code>. For instance, <code>while (1)</code> and <code>while (1 &lt; 10)</code> are equivalent to <code>while (true)</code>. Even conditions like <code>while (\"apple\")</code> are treated as true.</p> <p>Here's an example of a <code>while</code> loop that counts from 0 to 9: <pre><code>int i = 0;\nwhile (i &lt; 10){\n    cout &lt;&lt; i &lt;&lt; endl;\n    i++;\n}\n</code></pre></p> <p>You can also create infinite loops by providing a condition that is always <code>true</code>: <pre><code>int i = 0;\nwhile (true){\n    cout &lt;&lt; i &lt;&lt; endl;\n    i++;\n}\n</code></pre></p> <p>In the latter example, the loop will continue indefinitely until manually terminated, often by sending a kill command to the program (e.g., <code>Ctrl + C</code>).</p> <p>To include a condition within the <code>while</code> loop and control when it should exit, you can use the <code>break</code> statement. Here's an example:</p> <p>while_with_break.cpp<pre><code>int i = 0;\nwhile (true){\n    cout &lt;&lt; i &lt;&lt; endl;\n    i++;\n    if (i &gt; 9 ) {\n        break;\n    }\n}\n</code></pre> In this example, the loop starts with <code>i = 0</code>, prints the value of <code>i</code>, increments <code>i</code> by one (<code>i++</code>), and checks if <code>i</code> is greater than 9. After 10 iterations (0 through 9), the <code>i &gt; 9</code> condition becomes <code>true</code> (<code>10 &gt; 9</code>), triggering the <code>break</code> statement, which exits the loop.</p> <p>There is another type of loop known as a <code>do while</code> loop. This is very similar to a <code>while</code> loop, except the condition is check at the end of the scope, rather than at the start of the scope. </p>"},{"location":"Cpp/cpp-tutorial/#controlling-flow-with-if-else-if-else","title":"Controlling flow with <code>if</code>-<code>else if</code>-<code>else</code>","text":"<p>You can use <code>if</code>, <code>else if</code>, and <code>else</code> statements to control the flow of your program, directing it to different branches or code scopes based on conditions. Here's the format:</p> <pre><code>if (condition1) {\n    // Code block for condition1\n} else if (condition2) {\n    // Code block for condition2\n} else if (condition3) {\n    // Code block for condition3\n} else {\n    // Default code block\n}\n</code></pre> <p>In this example, the program checks conditions one by one. If <code>condition1</code> is true, it enters the scope of the first <code>if</code>. If it's <code>false</code>, it checks <code>condition2</code>, and so on. If none of the conditions are true, it enters the <code>else</code> block, which serves as the default code. Keep in mind that you always need to start with an <code>if</code> statement, but you can omit <code>else if</code> or <code>else</code> as needed. In the <code>while_with_break.cpp</code> example, only an <code>if</code> statement was used.</p> <p>The order of the <code>else if</code> statements are also important. If <code>condition2</code> and <code>condition3</code> are both <code>true</code> we will only enter the first block corresponding to <code>condition2</code>.</p> <p>Here's a more detailed example:</p> <pre><code>// Loop from 0-infinity \n// Loop from 0 to infinity\nint i = 0;\nwhile (true) {\n\n    // Check if i is even using the % operator\n    if (i % 2 == 0) {\n        cout &lt;&lt; i &lt;&lt; \" is even!\" &lt;&lt; endl;\n    } else {\n        cout &lt;&lt; i &lt;&lt; \" is odd!\" &lt;&lt; endl;\n    }\n\n    i++;\n\n    if (i &gt; 10) {\n        break;  // Exit the loop if i is greater than 10\n    } else if (i == 3) {\n        continue;  // Skip to the start of the next iteration if i is 3\n    } else {\n        cout &lt;&lt; \"Back to the start!\" &lt;&lt; endl;  // Print a message if neither condition is true\n    }\n}\n</code></pre> <p>In this code, the <code>%</code> operator is used for the modulo operation, which returns the remainder of a division operation. <code>i % 2</code> will return <code>0</code> if the number is even or <code>1</code> if the number is odd. We increment the value of <code>i</code> by one (line 13) and use a second set of <code>if</code> statements (lines 15-21). The first <code>if</code> provides a break if <code>i &gt; 10</code>, while the <code>else if</code> allows us to skip to the start of the next iteration using a <code>continue</code> command if i is 3. The <code>else</code> block prints a message if neither of these conditions is <code>true</code>.</p> <p>This code will give us the following output: <pre><code>0 is even!\nBack to the start!\n1 is odd!\nBack to the start!\n2 is even!\n3 is odd!\nBack to the start!\n4 is even!\nBack to the start!\n5 is odd!\nBack to the start!\n6 is even!\nBack to the start!\n7 is odd!\nBack to the start!\n8 is even!\nBack to the start!\n9 is odd!\nBack to the start!\n10 is even!\n</code></pre></p> <p>When using <code>if</code>-<code>else if</code>-<code>else</code> we can have an infinite number of <code>else if</code> conditions, but only one <code>if</code> and at most one <code>else</code>. </p>"},{"location":"Cpp/cpp-tutorial/#controling-flow-with-a-switch","title":"Controling Flow with a Switch","text":"<p>We can also use <code>switch</code> statements to control flow.</p> <p>Switch statements are a useful method for controlling the flow of code. They are particularly effective for handling errors and parsing parameters when the potential outcomes are well-known. </p> <p>A <code>switch</code> statement can be implemented with the following syntax: <pre><code>switch (value){\n    case value_1:\n        // block of code\n        break;\n    case value_2:\n        // block of code\n        break;\n    ...\n    case value_n:\n        // block of code\n        break;\n    default:\n        // block of code\n        break;\n}\n</code></pre></p> <p>Here, the <code>value</code> is a parameter we want to test within the <code>switch</code>. When using a <code>switch</code>, we test specific cases (<code>case</code>). For example, in the <code>case value_1</code> branch, we are testing if <code>value == value_1</code>. If this is true, then the block of code within that branch will be executed. Notice here that a <code>break</code> statement is used at the end of the block of code. This prevents the other branches from being executed by <code>break</code>ing from the <code>switch</code> construct.</p> <p>We can have any number of <code>case</code> statements within the <code>switch</code>.  You'll notice that there is a special condition called <code>default</code>.  This block will execute if reached regardless of the value. It specifies the \"default\" behavior of the code.</p> <p>For example, we can write a similar block to our <code>if</code> statement example: <pre><code>int i = 0;\nwhile (true) {\n\n    // Check if i is even using the % operator\n    switch (i % 2) {\n        case 0:\n            cout &lt;&lt; i &lt;&lt; \" is even!\" &lt;&lt; endl;\n            break;\n        case 1:\n            cout &lt;&lt; i &lt;&lt; \" is odd!\" &lt;&lt; endl;\n            break;\n    }\n\n    i++;\n\n    switch (i){\n        case 0 ... 3:\n            cout &lt;&lt; i &lt;&lt; \" is between 0 and 3\" &lt;&lt; endl;\n            break;\n        case 4:\n            cout &lt;&lt; i &lt;&lt; \" is exactly 4\" &lt;&lt; endl;\n            break;\n        case 5 ... 7:\n            cout &lt;&lt; i &lt;&lt; \" is between 5 and 7\" &lt;&lt; endl;\n            break;\n        default:\n            cout &lt;&lt; i &lt;&lt; \" greter than 7\" &lt;&lt; endl;\n            return 0;\n\n    }\n}\n</code></pre></p> <p>Here we are using two <code>switch</code> statements.  The first, on line 5, takes <code>i % 2</code>, which will be <code>0</code> if <code>i</code> is even and <code>1</code> if <code>i</code> is odd. The second, starting on line 16, takes the argument <code>i</code>.  Here we're checking the actual value of <code>i</code>.  On lines 17 and 23, we are checking if <code>i</code> lies within a range. In C++, we can do this with the <code>lower ... higher</code> syntax. Note this is explicitly <code>0 ... 3</code>, not <code>0...3</code>. On line 28, we are using a <code>return</code> to exit not just the switch but also the <code>while</code> loop. This is because running a <code>break</code> from within the <code>switch</code> would only break from the <code>switch</code> and not the outer <code>while</code> loop. This would output the following:</p> <pre><code>0 is even!\n1 is between 0 and 3\n1 is odd!\n2 is between 0 and 3\n2 is even!\n3 is between 0 and 3\n3 is odd!\n4 is exactly 4\n4 is even!\n5 is between 5 and 7\n5 is odd!\n6 is between 5 and 7\n6 is even!\n7 is between 5 and 7\n7 is odd!\n8 greter than 7\n</code></pre> <p>Let's consider a more useful example. Let's say we have a device that we are controlling with C++.  The device can have one of three states: on, off, or standby.  We can define an <code>enum</code> to handle these three options.</p> <pre><code>enum Status {On, Off, Standby};\n</code></pre> <p>An <code>enum</code> is very useful when considering a fixed number of possible options.  The option to use a human-readable <code>enum</code> can also help with debugging by providing a human-readable status (e.g., <code>Status::Standby</code>) instead of an error code that might not make sense within the correct context (e.g., <code>2</code>).</p> <p>Let's now define a function to get a random status.  This will emulate a device that we want to interface with:</p> <p><pre><code>Status get_status(){\n    srand (time(NULL));\n    int stat = rand() % 3 + 1;\n    switch(stat){\n        case 1:\n            return Status::On;\n        case 2:\n            return Status::Off;\n        default:\n            return Status::Standby;\n    }\n}\n</code></pre> On line 1, we are defining that this is a function (more on functions later) that returns a <code>Status</code> type (which we defined before as an <code>enum</code>). On lines 2 and 3, we initialize a random number generator and get a random number between 1 and 3.  We then use this random number to return a status. When the random number is 1, we return <code>Status::On</code>. When the random number is 2, we return <code>Status::Off</code>.  The <code>default</code> case here is <code>Status::Standby</code>.</p> <p>We can imagine that the <code>get_status</code> function is actually part of the API for some device. Let's say it is a readout device; when the status is <code>On</code>, it is powered on.  When the status is <code>Off</code>, either the device is off or it cannot be reached. When the status is <code>Standby</code>, it is awaiting instructions. Let's assume we send a power-on command with some function <code>activate_device</code>. For now, let's take:</p> <pre><code>void activate_device(){\n    sleep(1);\n}\n</code></pre> <p>In our workflow, it would be important to wait until the device has completely powered on and is in <code>Standby</code> mode before we send instructions.  We could write something like: <pre><code>int main(){\n\n    activate_device();\n\n    Status stat;\n    while (stat != Status::Standby)\n    {        \n        stat = get_status();\n\n        switch (stat){\n            case Status::On:\n                cout &lt;&lt; \"Device is On\" &lt;&lt; endl;\n                sleep(1);\n                break;\n            case Status::Off:\n                cout &lt;&lt; \"Device is Off\" &lt;&lt; endl;\n                activate_device();\n                sleep(1);\n                break;\n            case Status::Standby:\n                cout &lt;&lt; \"Device is in Standby\" &lt;&lt; endl;\n                break;\n        }\n\n    }\n}\n</code></pre></p> <p>On line 3, we send the power-on command to our device. On line 5, we define a <code>Status</code> object called <code>stat</code>. On line 6, we use a <code>while</code> loop to repeatedly check the status of the device. On line 8, we retrieve the status of the device. On line 10, we use a <code>switch</code> statement to match the <code>stat</code> to the possible values of that <code>enum</code>. If the device is either <code>Status::On</code> or <code>Status::Off</code>, we sleep for 1 second and then restart the loop. If the status is <code>Status::Off</code>, we also resend the power-on command with the <code>activate_device</code> function.  Finally, if the status is <code>Status::Standby</code>, we can continue with our program.</p> <p>Here is an example output from such a program:</p> <pre><code>Device is On\nDevice is Off\nDevice is Off\nDevice is On\nDevice is On\nDevice is Off\nDevice is On\nDevice is in Standby\n</code></pre>"},{"location":"Cpp/cpp-tutorial/#storing-data","title":"Storing Data","text":""},{"location":"Cpp/cpp-tutorial/#arrays","title":"Arrays","text":"<p>We can define arrays of a data type in C++ using the following format: <pre><code>    // Define an array of 6 integers\n    int my_array[6] = {1, 2, 3, 4, 5, 6};\n\n    // This will print the 3rd entry -&gt; 3\n    cout &lt;&lt; \"The 3rd entry in the array is: \" &lt;&lt; my_array[2] &lt;&lt; endl;\n</code></pre></p> <p>If we do not know the size of an array at compile time, we may <code>dynamically</code> allocate memory for the array: <pre><code>// Where n is some integer unknown at compile time.\nint *my_array = new int[n];\n\n/* do stuff with code */\n\ndelete [] my_array;\n</code></pre> On line we create a new array called <code>my_array</code> using the <code>new</code> keyword. When we do this we are dynamically allocating memory on the heap. The size of the region is going to be enough to hold <code>n</code> integers. The <code>int *my_array</code> syntax is important in C++. We have defined a \"pointer\" to an array of integers.  When we define a new array of values it is important to <code>delete</code> the data once we're finished with it. This is done on line 6. If we fail to do so, we will end up with a memory leak, causing our memory usage to rise over time.</p> <p>Aside on pointers</p> <p>We may discuss pointers later, but essentially, we can think of them not as the actual array itself, but rather as signposts to where the array is stored. <pre><code>int a = 42;\nint b = a;\nint *a_pointer = &amp;a;\n\n// Access the value of the pointer\ncout &lt;&lt; \"The Value of a is: \" &lt;&lt; a &lt;&lt; \" or by pointer \" &lt;&lt; *a_pointer &lt;&lt; endl;\ncout &lt;&lt; \"The address of a is: \" &lt;&lt; &amp;a &lt;&lt; \" or by pointer \" &lt;&lt; a_pointer &lt;&lt; endl;\ncout &lt;&lt; \"The Value of b is: \" &lt;&lt; b &lt;&lt; \" and its address is \" &lt;&lt; &amp;b &lt;&lt; endl;\n\nreturn 0;\n</code></pre> This will output: <pre><code>The Value of a is: 42 or by pointer 42\nThe address of a is: 0x7ffee5f38334 or by pointer 0x7ffee5f38334\nThe Value of b is: 42 and its address is 0x7ffee5f38330\n</code></pre></p> <p>On line 1, we define the integer <code>a</code>. On line 2, we set <code>b</code> to be equal to <code>a</code>. On line 3, we create a pointer <code>a_pointer</code> and assign it to <code>&amp;a</code>. Here, <code>&amp;`` represents that we're passing a reference to</code>a<code>rather than the value of</code>a<code>. The reference can be thought of as the actual location in memory of</code>a`.</p> <p>From the output, we see that <code>a</code>, <code>b</code>, and <code>a_pointer</code> all return <code>42</code>. However, the address of <code>a</code> and <code>b</code> is different. While the values are equal, they are not the same. However, the address returned by <code>&amp;a</code> and <code>a_pointer</code> is identical. This is because <code>a_pointer</code> is pointing to the spot in memory that <code>a</code> occupies.</p> <p>When working with arrays we need to be careful when accessing elements of the array as C++ doesn't offer protection about going out of range:</p> <p><pre><code>    // Define an array of 6 integers\n    int my_array[6] = {1, 2, 3, 4, 5, 6};\n\n    // This will print the 3rd entry -&gt; 3\n    cout &lt;&lt; \"The 3rd entry in the array is: \" &lt;&lt; my_array[2] &lt;&lt; endl;\n    cout &lt;&lt; \"The 7th entry in the array is: \" &lt;&lt; my_array[6] &lt;&lt; endl;\n</code></pre> The above example will execute, but we will be accessing whatever is in the memory at that location. This will be undefined behaviour!</p>"},{"location":"Cpp/cpp-tutorial/#vectors","title":"Vectors","text":"<p>Vectors offer a safer way to store data, and they can be defined as follows: <pre><code>// Create an empty vector of integers\nvector&lt;int&gt; my_vector;\n// Create a vector of length n\nvector&lt;int&gt; my_vector_long(n);\n// Create a vector of length n with a default value of 42\nvector&lt;float&gt; my_vector_long(n, 42.0);\n// Create a vector with known values\nvector&lt;double&gt; my_known_vector{1.1, 2.3};\n// Create 2D vectors (vectors of vectors)\nvector&lt;vector&lt;float&gt;&gt; my_2d_vector;\n</code></pre></p> <p>Vectors provide safety and have a known length, which can be obtained using: <pre><code>unsigned int vec_size = my_vector.size();\n\n// Get the last element of the vector\ncout &lt;&lt; \"Last element: \" &lt;&lt; my_vector[vec_size - 1] &lt;&lt; endl;\n</code></pre></p> <p>You can dynamically add values to the vector using <code>push_back</code>: <pre><code>// Define an empty vector\nvector&lt;int&gt; my_vector;\n\n// Add numbers 0-9 to our vector\nfor (int i = 0; i &lt; 10; i++){\n    my_vector.push_back(i);\n}\n// This will output 10\ncout &lt;&lt; \"Length of vector \" &lt;&lt; my_vector.size() &lt;&lt; endl;\n</code></pre></p> <p>You can also change the size of the vector with: <pre><code>vector&lt;int&gt; my_vector;\n\n// Resize to have 10 elements\nmy_vector.resize(10);\n// Resize to have 5 elements\nmy_vector.resize(5);\n\n// Resize to have 20 elements and set the values to 0\nmy_vector.resize(20, 0);\n</code></pre></p> <p>Vectors provide a memory-efficient way to store arrays of data. When a <code>vector</code> is removed, either by deletion or leaving scope, the destructor of the vector's elements is called, freeing the memory so that you don't need to manage it.</p>"},{"location":"Cpp/cpp-tutorial/#functions","title":"Functions","text":"<p>So far, we have used a single function, the main function. In C++, main is a special keyword that designates the entry point for our code. Let's take a closer look at the main function to understand the basics of how functions work in C++: <pre><code>int main(){\n    return 0;\n}\n</code></pre> On line 1, we define a function called <code>main</code>. It's important to note that this function has a return value, which is an <code>int</code>. The code for the function is enclosed within curly brackets <code>{}</code>, defining the scope of the function. In the example above, the <code>main</code> function simply returns <code>0</code>.</p> <p>We can also define functions that take arguments by specifying the argument types and providing them with local names within the function's scope:</p> <pre><code>// Define a function that adds two vectors of integers together and returns the result\nvector&lt;int&gt; add_two_vectors(vector&lt;int&gt; a, vector&lt;int&gt; b) {\n    // Define a vector to hold the result, ensuring it has the same size as the input vectors\n    vector&lt;int&gt; result(a.size());\n\n    for (int i = 0; i &lt; a.size(); i++) {\n        result[i] = a[i] + b[i];\n    }\n    // Return the result\n    return result;\n}\n\n...\n\nint main() {\n    vector&lt;int&gt; x, y;\n    for (int i = 0; i &lt; 10; i++) {\n        x.push_back(i);\n        y.push_back(2 * i);\n    }\n\n    vector&lt;int&gt; z = add_two_vectors(x, y);\n\n    return 0;\n}\n</code></pre> <p>In the example above, we define a function named <code>add_two_vectors</code> that takes two vectors of integers as arguments, adds the corresponding elements together, and returns the result as a new <code>vector</code>. This demonstrates how you can define and use functions in C++ to modularize your code and perform specific tasks.</p> <p>In C++, unlike in Python, we can \"overload\" functions by defining multiple functions with the same name but accepting different argument types. For example, consider the following case: <pre><code>// Define a function that adds two vectors of integers together and returns the result\nvector&lt;int&gt; add_two_vectors(vector&lt;int&gt; a, vector&lt;int&gt; b) {\n    // Define a vector to hold the result, ensuring it has the same size as the input vectors\n    vector&lt;int&gt; result(a.size());\n\n    for (int i = 0; i &lt; a.size(); i++) {\n        result[i] = a[i] + b[i];\n    }\n    // Return the result\n    return result;\n}\n\n// Define a function that adds two vectors of booleans together and returns the result\nvector&lt;bool&gt; add_two_vectors(vector&lt;bool&gt; a, vector&lt;bool&gt; b) {\n    // Define a vector to hold the result, ensuring it has the same size as the input vectors\n    vector&lt;bool&gt; result(a.size());\n\n    for (int i = 0; i &lt; a.size(); i++) {\n        result[i] = a[i] || b[i];\n    }\n    // Return the result\n    return result;\n}\n</code></pre></p> <p>In the above example, we have \"overloaded\" the <code>add_two_vectors</code> function to accept vectors of integers and vectors of booleans. The structure of the two functions is very similar, with the main difference occurring on line 23. In the second function, we replaced the addition operator <code>+</code> with the logical \"OR\" operator <code>||</code>. This demonstrates how you can create multiple functions with the same name but different behaviors based on the types of arguments they receive.</p> <p>Knowing that logical '+' represents \"OR\" and \"*\" stands for \"AND\", we can rewrite the previous example using multiplication:</p> <p><pre><code>// Define a function that multiplies two vectors of integers and returns the result\nvector&lt;int&gt; multiply_two_vectors(vector&lt;int&gt; a, vector&lt;int&gt; b) {\n    // Define a vector to hold the result, ensuring it has the same size as the input vectors\n    vector&lt;int&gt; result(a.size());\n\n    for (int i = 0; i &lt; a.size(); i++) {\n        result[i] = a[i] * b[i];\n    }\n    // Return the result\n    return result;\n}\n\n// Define a function that multiplies two vectors of booleans and returns the result\nvector&lt;bool&gt; multiply_two_vectors(vector&lt;bool&gt; a, vector&lt;bool&gt; b) {\n    // Define a vector to hold the result, ensuring it has the same size as the input vectors\n    vector&lt;bool&gt; result(a.size());\n\n    for (int i = 0; i &lt; a.size(); i++) {\n        result[i] = a[i] &amp;&amp; b[i];\n    }\n    // Return the result\n    return result;\n}\n</code></pre> Here the <code>&amp;&amp;</code> operator represents the logical \"and\" operator. Overloading functions is super useful when you want to use common names for functions that perform similar operations on different data types.</p> <p>We can also set default values in functions using the following: <pre><code>void print_message(bool hello = true){\n    if (hello){\n        cout &lt;&lt; \"Hello World\" &lt;&lt; endl;\n    }\n    else {\n        cout &lt;&lt; \"Goodbye World\" &lt;&lt; endl;\n    }\n}\n\n...\n\nint main(){\n    // Will print \"Hello World\"\n    print_message();\n    // Will print \"Goodbye World\"\n    print_message(false);\n\n    return 0;\n}\n</code></pre></p> <p>On line 1, we define a function with a return type specified as <code>void</code>, indicating that the function doesn't return any value. When defining the function's parameters, we allow the bool parameter to have a default value of <code>true</code>. On line 14, when calling the function without passing any arguments, the default values are used. On line 16, we explicitly pass the value <code>false</code>, which overrides the default argument.</p>"},{"location":"Cpp/parallel-cpp/","title":"Introduction to Parallel Programming in C++ with OpenMP","text":""},{"location":"Cpp/parallel-cpp/#introduction-to-openmp-in-c","title":"Introduction to OpenMP in C++","text":"<p>In this tutorial, I aim to introduce you to OpenMP, a library facilitating multiprocessing in C++. I assume little-to-no background in computer science or low-level programming, and only a basic understanding of C++. I will steer clear of technical jargon wherever possible. Many online resources presume you are a seasoned programmer or computer scientist. Instead, I assume that you utilize programming as a tool and prioritize the \"how\" over the \"why\". However, for those interested in the rationale behind concepts, I will include some links at the bottom for further reading.</p> <p>If you are an experienced programmer, you might notice some terminology that may seem imprecise. For instance, when using <code>shared</code> and <code>private</code>, I refer to \"copy\" instead of delving into references and mutability intricacies. As mentioned, this tutorial caters to learners seeking to grasp concepts and initiate coding rather than the theoretical computer science underpinning these concepts.</p> <p>Worried about setting up your environment correctly? Want to just run the examples here? These examples can be run from a docker image, see here for the latest image.  <pre><code>docker run -it --rm --gpus all obriens/parallel:latest \n</code></pre></p>"},{"location":"Cpp/parallel-cpp/#how-to-calculate-pi","title":"How to Calculate \\(\\pi\\)","text":""},{"location":"Cpp/parallel-cpp/#calculating-using-a-riemann-sum-approximation","title":"Calculating \u03c0 using a Riemann Sum Approximation","text":"<p>To determine the value of \u03c0, we can solve the following integral:</p> \\[ \\pi = \\int_0^1 \\frac{4}{1+x^2} \\,dx. \\] <p>An approximation of this integral can be made using a Riemann Sum, also known as the \"rectangle rule,\" defined as:</p> \\[ S = \\sum_{i=1}^{n} f(x^*_i) \\, \\Delta x, \\] <p>where:</p> <ul> <li>\\(n\\) represents the number of \"rectangles.\"</li> <li>\\(f(x^*_i)\\) denotes a point within the range \\((x_{i-1}, x_{i})\\).</li> <li>\\(\\Delta x = x_{i} - x_{i-1}\\) represents the width of each rectangle.</li> <li>\\(f(x^*_i)\\) is the function to approximate at \\(x^*_i\\).</li> </ul> <p>Within this summation, the calculation involves finding the area of a rectangle with width \\(\\Delta x\\) and height \\(f(x^*_i)\\). Determining the value of \\(n\\) allows full coverage of the entire integral domain by summing rectangles of width \\(\\Delta x\\).</p> <p>For instance, in the case of approximating \u03c0 using 100 steps ($ n = 100 $), the width is calculated as \\(\\Delta x = \\frac{(b - a)}{n} = \\frac{(1 - 0)}{100} = 0.01\\).</p> <p> </p>  Riemann sum example. (https://en.wikipedia.org/wiki/Riemann_sum)"},{"location":"Cpp/parallel-cpp/#estimating-using-riemann-sum","title":"Estimating \u03c0 using Riemann Sum","text":"<p>To estimate the value of \u03c0, we can use the following formula:</p> \\[ \\pi \\approx \\sum_{i=1}^{n} \\frac{4}{1+((i+0.5)*\\Delta x)^2} \\, \\Delta x. \\] <p>Here, we define \\(x^*_i\\) as the midpoint between \\(x_{i-1}\\) and \\(x_{i}\\). As \\(n \\rightarrow \\infty\\), the estimation becomes independent of how we define \\(x^*_i\\).</p> <p>In C++, the approximation formula would look something like this:</p> calc_pi_serial<pre><code>double calc_pi_serial (int num_steps){ \n    int i; \n    double x, pi, sum = 0.0;\n    double step = 1.0/(double) num_steps;\n    for ( i=0; i&lt; num_steps; i++){\n        x = (i+0.5)*step;\n        sum = sum + 4.0/(1.0+x*x);\n    }\n    pi = step * sum;\n    return pi;\n}\n</code></pre> <p>On line 4, we utilize the requested number of steps (<code>num_steps</code>) to define <code>step</code>, which represents \\(\\Delta x\\). Note that we \"cast\" the <code>int</code> <code>num_steps</code> to a <code>double</code> type in the division. </p> <p>Moving to line 6, we determine \\(x^*_i\\) as the midpoint between \\(x_{i-1}\\) and \\(x_{i}\\) before adding the current iteration to the total.</p> <p>Finally, on line 9, we multiply the <code>sum</code> by <code>step</code>, thereby calculating the area of the rectangle. As \\(\\Delta x\\) remains constant, we can optimize the code by taking this operation out of the loop.</p> <p>Before evaluating the code's performance, let's include some useful libraries.</p> <p>Example:</p> <pre><code>#include &lt;iostream&gt;\n#include &lt;chrono&gt;\n</code></pre> <p>In this implementation, we'll utilize <code>iostream</code> along with <code>cout</code> for printing the results. Additionally, we will employ <code>chrono</code> to measure the time taken for the code execution. </p> <p>Combining these libraries together, we get the following:</p> calculate_pi.cpp<pre><code>#include &lt;iostream&gt;\n#include &lt;chrono&gt;\n\n// Function to calculate pi\ndouble calculate_pi (int num_steps){\n    int i;\n    double x, pi, sum = 0;\n    double step = 1./ (double) num_steps;\n\n    for (i = 0; i &lt; num_steps; i++){\n        x = (i+0.5)*step;\n        sum = sum + 4.0/(1.0+x*x);\n    }\n    pi = sum * step;\n    return pi;\n}\n\nint main(){\n\n    // 1 million steps\n    int num_steps = 1000000;\n    int n_repeat = 50;\n\n    // mean pi value\n    double serial_pi = 0;\n\n    // Run timer\n    auto start_time = std::chrono::high_resolution_clock::now();\n    for (int i = 0; i &lt; n_repeat ; i ++) serial_pi += calculate_pi(num_steps);\n    auto end_time = std::chrono::high_resolution_clock::now();\n\n    std::chrono::duration&lt;double&gt; serial_duration = end_time - start_time;\n\n    // Print average results\n    std::cout &lt;&lt; \"Serial Calculation of Pi: \" &lt;&lt; serial_pi / n_repeat \n              &lt;&lt; std::endl &lt;&lt; \"Duration: \" &lt;&lt; serial_duration.count() / n_repeat &lt;&lt; \" seconds\" &lt;&lt; std::endl;\n\n}\n</code></pre> <p>Lines 21 and 22 involve defining <code>num_steps</code> as 1 million and <code>n_repeat</code> as 50. This setup allows us to repeat the calculation of \\(\\pi\\) 50 times, considering the mean run time as our performance metric.</p> <p>Moving to lines 28-30, we initiate a timer, execute the function 50 times, and subsequently stop the timer, calculating the duration on line 32. Finally, we present the mean results.</p> <p>Upon compiling this example and running it, the observed output is:</p> <pre><code>Serial Calculation of Pi: 3.14159 \nDuration: 0.00263172 seconds \n</code></pre> <p>An execution time of 0.003 seconds doesn't sound too bad, providing us with an accurate estimation of \\(\\pi\\). However, it would be beneficial to explore how we can further enhance the speed of this code.</p> <p>To leverage parallel execution, we must reconsider our algorithm. This involves identifying which sections can be executed in parallel and which sections must remain in serial execution. Let's establish this as our baseline. The speedup factor is defined as:</p> \\[  \\mathrm{speed~up} = \\frac{\\mathrm{baseline~runtime}}{\\mathrm{test~runtime}}  \\]"},{"location":"Cpp/parallel-cpp/#poorly-parallelized-code","title":"Poorly Parallelized Code","text":"<p>Examining the <code>calc_pi_serial</code> function, we observe a substantial loop that appears to be a promising candidate for parallelization. Our initial step will involve optimizing this for loop to execute across multiple threads.</p> <p>Defining a parallel scope</p> <ul> <li>To define a parallel scope or a block of code to run in parallel, we are going to use openMP. First we will include the openMP library: <pre><code>#include&lt;opm.h&gt;\n</code></pre></li> <li>Once we identify the area of the code to run in parallel, we define a new ``scope'' for that section <pre><code>// Define a parallel block of code\n#pragma omp parallel\n{\n    // Code we want to run in parallel\n    ...\n}\n</code></pre></li> <li>In this example we will be using a fixed number of threads that we know at compile time. On Linux we can identify how many CPU cores we have access to using <code>lscpu</code>:  <pre><code>lscpu\n...\nCPU(s):                  16\n  On-line CPU(s) list:   0-15\n...\n</code></pre> Here I have access to 16 cores. I will therefore define a constant in my code equal to the number of cores:  <pre><code>#define NUM_THREADS 16\n</code></pre></li> </ul> <p>Here is my initial (incorrect) attempt at a parallel version for calculating \\(\\pi\\):</p> calculate_pi_wrong<pre><code>double calculate_pi_wrong (int num_steps){\n    double x, pi, sum = 0;\n    double step = 1./ (double) num_steps;\n    int chunks = num_steps / NUM_THREADS;\n\n    omp_set_num_threads(NUM_THREADS);\n    #pragma omp parallel\n    {\n        int i;\n        int id = omp_get_thread_num();\n        for (i = id*chunks; i &lt; (id+1)*chunks; i++){\n            x = (i+0.5)*step;\n            sum = sum + 4.0/(1.0+x*x);\n        }\n    }\n\n    pi = sum * step;\n    return pi;\n}\n</code></pre> <p>At line 4, we define <code>chunks</code>, representing the number of iterations that will occur on each thread. Subsequently, on line 6, <code>omp_set_num_threads(NUM_THREADS)</code> is utilized to specify the number of threads to be invoked.</p> <p>Moving to line 7, we establish a parallel block spanning from line 8 to line 15. Within this block, on line 10, <code>omp_get_thread_num()</code> retrieves the thread ID for the currently executing thread. Following that, line 11 introduces a loop specifically iterating over the steps assigned to each thread.</p> <p>To compile, it's necessary to use the <code>-fopenmp</code> flag. For instance:</p> <p><pre><code>g++ mycode.cpp -o mycode -fopenmp\n</code></pre> Running the code, produces the get the following result: <pre><code>Parallel (Wrong) Calculation of Pi: 0.297578\nDuration: 0.00838327 seconds (0.313425 speed up) \n</code></pre> If we rerun the code, we also see: <pre><code>Parallel (Wrong) Calculation of Pi: 0.283952\nDuration: 0.00774014 seconds (0.338872 speed up) \n</code></pre></p> <p>Our parallel implementation takes longer to execute than the serial example, and it also computes an incorrect value for \\(\\pi\\). Additionally, the calculated value of \\(\\pi\\) is not reproducable. Let's delve deeper into understanding the code's behavior...</p> <p>What went wrong</p> <p>In the initial setup, we define several variables within the \"parent scope\" to be utilized within the parallel section of the code. However, these variables are accessed and modified in parallel, leading to read and write operations occurring simultaneously at lines 12-13. calculate_pi_wrong<pre><code>double calculate_pi_wrong (int num_steps){\n    double x, pi, sum = 0;\n    double step = 1./ (double) num_steps;\n    int chunks = num_steps / NUM_THREADS;\n\n    omp_set_num_threads(NUM_THREADS);\n    #pragma omp parallel\n    {\n        int i;\n        int id = omp_get_thread_num();\n        for (i = id*chunks; i &lt; (id+1)*chunks; i++){\n            x = (i+0.5)*step;\n            sum = sum + 4.0/(1.0+x*x);\n        }\n    }\n\n    pi = sum * step;\n    return pi;\n}\n</code></pre> Running in parallel, all threads attempt to read from/write to the <code>sum</code> variable without control or predictability regarding which thread modifies the <code>sum</code> value. Threads accessing and modifying <code>sum</code> concurrently can lead to unexpected behavior. For instance, consider that on iteration 1, thread 1 takes <code>sum</code> as 0 and modifies it. Concurrently, thread 2 attempts the same, leading to different values being written to the same variable. Consequently, the calculation of <code>sum</code> becomes unreliable due to this simultaneous access.</p> <p>This issue, known as a race condition, arises as the code outcome depends on the order in which threads access data.</p> <p>Let's look at how we may solve this issue...</p>"},{"location":"Cpp/parallel-cpp/#solving-the-race-condition-using-an-array","title":"Solving the Race Condition using an array","text":"<p>To resolve the race condition, ensuring that only one thread accesses a variable at any given time is essential. For this purpose, we require a variable accessible to all threads yet controllable by a main thread for summation. An effective solution involves employing an array of size <code>NUM_THREADS</code>.</p> <p>calculate_pi_correct<pre><code>double calculate_pi_correct (int num_steps){\n\n    double pi = 0.0;\n    double sum [NUM_THREADS];\n    double step = 1.0/(double) num_steps;\n    int chunks = num_steps / NUM_THREADS;\n\n    // Set up the parallel loop\n    omp_set_num_threads(NUM_THREADS);\n    #pragma omp parallel\n    {\n        // get the thread id\n        int i, id;\n        double x;\n        id = omp_get_thread_num();\n        // Each thread sums over its own chunk\n        for ( i=id*chunks, sum[id]=0; i &lt; (id+1)*chunks; i++){\n            x = (i+0.5)*step;\n            // Sum over each's own sum counter\n            sum[id] += 4.0/(1.0+x*x);\n        }\n    }\n\n    // Combine in serial\n    for (int i = 0; i &lt; NUM_THREADS; i++) pi += step * sum[i];\n    return pi;\n\n}    \n</code></pre> At line 4, a <code>double</code> array of size <code>NUM_THREADS</code> is initialized. On line 14, the variable <code>x</code> is localized for each thread. Moving to line 17, a loop iterates through each section for every thread, setting <code>sum[id]</code> to 0 for each thread. Then, at line 20, modification occurs solely within the thread's respective entry in the array. Finally, on line 25, the aggregated list is combined.</p> <p>Upon compilation and execution, the observed outcome is:</p> <pre><code>Parallel (Correct) Calculation of Pi: 3.14159 \nDuration: 0.00562214 seconds (0.466534 speed up)  \n</code></pre> <p>Although we accurately estimate \\(\\pi\\) with reproducible results, there's no observed enhancement in runtime. To comprehend this, scrutiny of lines 4, 17, and 20 is necessary.</p> <p>We've defined <code>sum</code> as a <code>double</code> array of size <code>NUM_THREADS</code>. In memory, <code>sum</code> refers to a contiguous block of memory. Loading <code>sum</code> into our cache requires the passing of <code>sum</code> between threads. Consider threads A and B: when <code>sum</code> enters the cache, it occupies a \"cache line,\" awaiting CPU utilization. Both threads attempt to access <code>sum</code>. While thread A successfully accesses <code>sum</code>, thread B encounters a \"cache miss,\" leading to a delay. This issue can be addressed by incorporating knowledge of CPU and cache behavior into our data type design.</p>"},{"location":"Cpp/parallel-cpp/#pading-our-array","title":"Pading our Array","text":"<p>Understanding our cache design allows us to optimize how data is organized, resulting in improved performance. One method for this optimization is known as \"padding\". By employing padding, we introduce additional, redundant information to ensure that our data is stored within the same cache line.</p> <p>In the context of the aforementioned example, we can implement padding for the <code>sum</code> array by transforming it into a 2D array with dimensions <code>NUM_THREADS</code>\\(\\times64\\). But why 64? To understand this, let's examine our CPU:</p> <p><pre><code>&gt; cat /proc/cpuinfo| grep cache \ncache size : 512 KB \ncache_alignment : 64 \n</code></pre> Imagine the cache as a traffic stop with 64 lanes. When the traffic light turns green, 64 cars pass before it turns red again. Subsequently, the next row gets the green light, and so forth.</p> <p>How does this analogy apply to us? When we read from the cache, our aim is to minimize how often we access an array. If we solely require one row of an array, one cache line suffices. To achieve this, we pad our row with 63 other non-essential pieces of information. Subsequently, the second read will access the second row, which is on the subsequent \"row of traffic\".</p> <p>We design our array to leverage the cache's ability to read 64 elements of a row at once. The following example demonstrates the implementation of this strategy:</p> <p>calculate_pi_padded<pre><code>double calculate_pi_padded (int num_steps){\n\n    double pi = 0.0;\n    double sum [NUM_THREADS][PAD];\n    double step = 1.0/(double) num_steps;\n    int chunks = num_steps / NUM_THREADS;\n\n    // Set up the parallel loop\n    omp_set_num_threads(NUM_THREADS);\n    #pragma omp parallel\n    {\n        // get the thread id\n        int i, id;\n        double x;\n        id = omp_get_thread_num();\n        // Each thread sums over its own chunk\n        for ( i=id*chunks, sum[id][0]=0; i &lt; (id+1)*chunks; i++){\n            x = (i+0.5)*step;\n            // Sum over each's own sum counter\n            sum[id][0] += 4.0/(1.0+x*x);\n        }\n    }\n\n    // Combine in serial\n    for (int i = 0; i &lt; NUM_THREADS; i++) pi += step * sum[i][0];\n    return pi;\n\n}\n</code></pre> This example is similar to the previous version; however, we can see on line 4 we define <code>sum</code> as a 2D <code>double</code> array. On lines 17, 20 and 25, we are only accessing the 0\\(^{th}\\) element of each row.  </p> <pre><code>Parallel (Padded) Calculation of Pi: 3.14159 \nDuration: 0.00167924 seconds (1.56197 speed up)  \n</code></pre> <p>Success! We observe a 1.56x improvement in performance. While this approach isn't memory-efficient, it demonstrates how organizing our data can significantly enhance performance.</p>"},{"location":"Cpp/parallel-cpp/#using-a-locking-construct","title":"Using a Locking Construct","text":"<p>So far, we have tried to solve the race condition issue by increasing the memory requirement. We can also use a construct to control the order in which threads can access a section of the code.  </p> <p>Using constructs</p> <p>OpenMP has several constructs that we can use in our code to modify a thread's flow. See here for a useful cheat sheet. </p> <ul> <li> <p>Thus far, we've encountered the <code>parallel</code> construct, defining a code section to run in parallel: <pre><code>#pragma omp parallel\n{\n    // Code to be ran in parallel\n    ...\n}\n</code></pre></p> </li> <li> <p>The <code>master</code> construct designates a code section to execute on a single thread, identified as the \"master\" or main thread. <pre><code>#pragma omp parallel\n{\n    // Code running on all threads\n    a += 1;\n\n\n    #pragma omp master\n    {\n        // Code only running on one thread\n        b += 1;\n    }\n}\n</code></pre></p> </li> <li> <p>A <code>barrier</code> construct synchronizes threads, ensuring that all threads reach the same point before proceeding. <pre><code>#pragma omp parallel\n{\n    // Code running on all threads\n    a += 1;\n\n    // Waiting until all threads reach this point\n    #pragma omp barrier\n    b += 1;\n}\n</code></pre></p> </li> <li> <p>The <code>sections</code> construct specifies distinct code sections to execute in parallel across various threads. <pre><code>// Start of sections block\nomp_get_thread_num(2);\n#pragma omp sections\n{\n    // Section 1\n    #pragma omp section\n    {\n        a+=1;\n    }\n    // Section 2\n    #pragma omp section\n    {\n        b+=1;\n    }\n}\n</code></pre></p> </li> <li>Furthermore, the behavior of a construct can be modified using \"directives\". <pre><code>#pragma omp parallel private (a) shared (b)\n{\n    a += b;\n}\n</code></pre> In the provided code, we modified the behavior of the <code>parallel</code> construct. Each thread now carries a \"private\" copy of <code>a</code> and shares a copy of <code>b</code>. Changes to <code>a</code> are confined to the thread locally, while modifications to <code>b</code> affect all threads uniformly.</li> </ul> <p>Let us modify the <code>calculate_pi_correct</code> example to use constructs to improve the performance.</p> calculate_pi_critical<pre><code>double calculate_pi_critical (int num_steps){\n\n    double pi = 0.0;\n    double step = 1.0/(double) num_steps;\n    int chunks = num_steps / NUM_THREADS;\n    double sum, x;\n\n    // Set up the parallel loop\n    omp_set_num_threads(NUM_THREADS);\n    #pragma omp parallel private(sum, x) shared (pi)\n    {\n        // get the thread id\n        int i, id;\n        id = omp_get_thread_num();\n        // Each thread sums over its own chunk\n        for ( i=id*chunks, sum=0; i &lt; (id+1)*chunks; i++){\n            x = (i+0.5)*step;\n            // Sum over each's own sum counter\n            sum += 4.0/(1.0+x*x);\n        }\n\n        // Use a critical barrier\n        // Only one thread at a time\n        #pragma omp critical\n        pi += step * sum;\n\n    }\n    return pi;\n\n}    \n</code></pre> <p>In the above code, variables <code>pi</code>, <code>sum</code>, and <code>x</code> are defined outside the loop, making them accessible to all sub-scopes. We use the <code>private</code> and <code>shared</code> directives. This gives each thread a private copy of <code>sum</code> and <code>x</code>, and a shared copy of <code>pi</code>. This is shown on line 10.</p> <p>With this adjustment, concerns about moving around <code>sum</code> and <code>x</code> are eliminated since each thread possesses its own copy. However, caution is necessary regarding how threads modify <code>pi</code>. To address this, we employ a <code>critical</code> construct, designating a code region accessible to only one thread at a time.</p> <p>The utilizating the <code>critical</code> construct on line 24 acts as a \"lock\", effectively halting other threads from progressing beyond this code section. Once a thread completes the addition operation and moves to line 27, the scope ends, releasing the \"lock\" and enabling the subsequent thread to enter this part of the code.</p> <p><code>critical</code> constructs serve a valuable purpose in managing thread access when modifying shared data accessible to all threads.</p> <pre><code>Parallel (Critical) Calculation of Pi: 3.14159 \nDuration: 0.00025696 seconds (10.2075 speed up)  \n</code></pre> <p>We are now observing a tenfold (\\(\\times10\\)) enhancement over the baseline test. Through refining the management of variable passing and controlling access using constructs, we've significantly enhanced our code.</p>"},{"location":"Cpp/parallel-cpp/#using-an-atomic-construct","title":"Using an Atomic Construct","text":"<p>You might be familiar with <code>atomic</code> data types\u2014these are data types that automatically trigger a lock whenever a read/write operation is executed on them. In OpenMP, we utilize an <code>atomic</code> construct to segment a section of the code where we perform such <code>atomic</code> operations.</p> calculate_pi_atomic<pre><code>double calculate_pi_atomic (int num_steps){\n\n    double pi = 0.0;\n    double step = 1.0/(double) num_steps;\n    int chunks = num_steps / NUM_THREADS;\n    double sum, x;\n\n    // Set up the parallel loop\n    omp_set_num_threads(NUM_THREADS);\n    #pragma omp parallel private(sum, x) shared(pi)\n    {\n        // get the thread id\n        int i, id;\n        id = omp_get_thread_num();\n        // Each thread sums over its own chunk\n        for ( i=id*chunks, sum=0; i &lt; (id+1)*chunks; i++){\n            x = (i+0.5)*step;\n            // Sum over each's own sum counter\n            sum += 4.0/(1.0+x*x);\n        }\n\n        // Use an atomic barrier\n        // Only one thread at a time can modify\n        #pragma omp atomic\n        pi += step * sum;\n\n    }\n    return pi;\n\n}    \n</code></pre> <p>The main distinction between <code>calculate_pi_critical</code> and <code>calculate_pi_atomic</code> lies on line 24, where an <code>atomic</code> construct replaces the <code>critical</code> construct. When utilizing an <code>atomic</code> construct, the subsequent operation is treated as <code>atomic</code>, triggering a lock during read/write operations.</p> <p><pre><code>Parallel (Atomic) Calculation of Pi: 3.14159\nDuration: 0.000249191 seconds (10.5257 speedup) \n</code></pre> We notice a slight improvement; however, this improvement might just be noise. It's crucial to recognize that atomic operations (<code>++</code>, <code>--</code>, <code>+</code>, <code>*</code>, <code>-</code>, <code>/</code>, <code>&amp;</code>, <code>^</code>, <code>&lt;&lt;</code>, <code>&gt;&gt;</code>, <code>|</code>) are limited and can only be applied to primitive data types (e.g., <code>int</code>, <code>float</code>, <code>double</code>). While atomic constructs are useful, they are less versatile compared to critical constructs.</p>"},{"location":"Cpp/parallel-cpp/#using-a-reduction","title":"Using a Reduction","text":"<p>Operations such as summations and products are frequently encountered. In fact, many of these operations can be executed using constructs and directives available in OpenMP. The <code>reduction</code> directive serves the purpose of indicating to the code that a variable is the result of a well-known operation. The compiler will then handle the necessary tasks, including ensuring memory safety, optimization, and more.</p> calculate_pi_reduction<pre><code>double calculate_pi_reduction (int num_steps){\n\n    double pi = 0.0;\n    double step = 1.0/(double) num_steps;\n    int i;\n    double sum = 0;\n    double x;\n\n    // Set up the parallel loop\n    omp_set_num_threads(NUM_THREADS);\n    #pragma omp parallel for reduction (+:sum) private (x)\n    for ( i=0; i &lt; num_steps; i++){\n        x = (i+0.5)*step;\n        // Sum over each's own sum counter\n        sum += 4.0/(1.0+x*x);\n    }\n\n    pi = step * sum;\n\n    return pi;\n\n}    \n</code></pre> <p>All the previous steps have been consolidated into one line, specifically line 11. Here, we instruct the compiler that within a <code>for</code> loop, we intend to execute a well-known <code>reduction</code> operation, specifically adding multiple values to the variable <code>sum</code> (<code>(+:sum)</code>). Additionally, we specify that each thread requires a private copy of <code>x</code>. This approach eliminates the necessity for <code>critical</code> or <code>atomic</code> constructs, as the compiler handles these complexities on our behalf.</p> <pre><code>Parallel (Reduction) Calculation of Pi: 3.14159\nDuration: 0.000212785 seconds (12.3266 speed up) \n</code></pre> <p>We observe a  \\(\\times 12\\) enhancement in execution speed.</p>"},{"location":"Cpp/parallel-cpp/#summary","title":"Summary","text":"<pre><code>Serial Calculation of Pi: 3.14159\nDuration: 0.00262292 seconds\nParallel (Wrong) Calculation of Pi: 0.283952\nDuration: 0.00774014 seconds (0.338872 speed up) \nParallel (Correct) Calculation of Pi: 3.14159\nDuration: 0.00562214 seconds (0.466534 speed up) \nParallel (Padded) Calculation of Pi: 3.14159\nDuration: 0.00167924 seconds (1.56197 speed up) \nParallel (Critical) Calculation of Pi: 3.14159\nDuration: 0.00025696 seconds (10.2075 speed up) \nParallel (Atomic) Calculation of Pi: 3.14159\nDuration: 0.000249191 seconds (10.5257 speed up) \nParallel (Reduction) Calculation of Pi: 3.14159\nDuration: 0.000212785 seconds (12.3266 speed up) \n</code></pre> <p>When comparing the baseline to optimal code, we only need an additional 4 lines of code: <pre><code>#include&lt;opm.h&gt;\n...\n#define NUM_THREADS 16\n...\n    omp_set_num_threads(NUM_THREADS);\n    #pragma omp parallel for reduction (+:sum) private (x)\n...\n</code></pre></p> <p>OpenMP equips us with pre-existing \"constructs\" and \"directives\" that facilitate the development of high-performing code. This allows us to emphasize the code's functionality rather than dwelling extensively on optimization techniques.</p> <p>Even for more generic code that cannot utilize <code>atomic</code> or <code>reduction</code>, significant performance enhancements can still be achieved by modestly rethinking how our algorithms operate.</p> <p>In this example, our focus has been exclusively on CPUs. The code utilized in this demonstration is available on GitHub.</p>"},{"location":"Cpp/parallel-gpu/","title":"GPU Programming in C++","text":"<p>Coming soon... Not sure how much desire there is for a C++ GPU tutorial. Maybe focus on Python:</p> <ul> <li>Drop in replacement -&gt; <code>cupy</code></li> <li>Compiled with numba -&gt; <code>numba.cuda</code></li> <li>Coding cuda kernels?</li> <li>Working with tensors? -&gt; <code>torch</code></li> </ul>"},{"location":"Cpp/parallel-gpu/#introduction","title":"Introduction","text":"<p>Expanding on the OpenMP tutorial, one might be interested in how to write parallel code in C++ that is designed to make use of the GPU rather than the CPU.</p> <p>In this tutorial, we will first discuss the \"calculate \\(\\pi\\)\" example. We shall implement a GPU version of the code and discuss the performance and the downfalls of this code. We will then discuss in more detail when GPU programming is more suitable than CPU programming, and use some examples to illustrate the case uses for both.</p>"},{"location":"Cpp/parallel-gpu/#calculating-pi","title":"Calculating \\(\\pi\\)","text":""},{"location":"Misc/","title":"Misc","text":"<ul> <li>Website building with MkDocs</li> </ul>"},{"location":"Misc/websites/","title":"Websites","text":""},{"location":"Misc/websites/#introduction","title":"Introduction","text":"<p>Welcome to this workshop where we'll explore the use of MkDocs to build your own personal website. MkDocs, a Python package, streamlines the process by allowing you to focus on creating content in Markdown \u2013 a highly human-readable language. This eliminates the need to delve into HTML, JavaScript, or other web development languages. Originally designed for documentation, MkDocs offers a swift and intuitive solution for crafting a personalized website.</p>"},{"location":"Misc/websites/#requirements","title":"Requirements","text":"<p>In this workshop, we'll assume that you have a Python package manager. If you don't already have one set up, I recommend installing mamba. Mamba is designed to be a \"drop-in replacement for conda,\" meaning you can substitute any conda commands for mamba. For example:</p> <p><pre><code>conda install numpy\n</code></pre> Becomes <pre><code>mamba install numpy\n</code></pre></p> <p>And so on. </p> <p>We'll be using the following packages. Installing them prior to the workshop will help you follow along: <pre><code>mkdocs\nmkdocs-material\nmkdocstrings\nmkdocstrings-python\nmkdocs-jupyter\n</code></pre></p> <p>To create a new working environment and install these packages run: <pre><code>mamba create -n website-workshop python=3.11 mkdocs mkdocs-material mkdocstrings mkdocstrings-python mkdocs-jupyter\n</code></pre></p> <p>Substituting mamba for conda if you're running with conda.</p> <p>You can activate this environment with: <pre><code>mamba activate website-workshop\n</code></pre></p>"},{"location":"Misc/websites/#about-mkdocs","title":"About MkDocs","text":"<p>MkDocs provides a convenient method for creating quick static websites. Originally designed to generate documentation for code packages, learning how to utilize MkDocs offers the added benefit of generating documentation pages for your own coding projects.</p> <p>For instance, this tutorial demonstrates how to use MkDocs to generate documentation from Python docstrings. This process combines the steps of properly commenting on one's code with providing future users accessible documentation, all in a single, simple step that can be run as a pre-commit step.</p>"},{"location":"Misc/websites/#creating-a-new-project","title":"Creating a new project","text":"<p>With MkDocs we can create a new project using the following command: <pre><code>mkdocs new project-name\n</code></pre> Alternatively, we can initialize MkDocs for an existing project using: <pre><code>mkdocs init .\n</code></pre></p> <p>This will create the following files: <pre><code>\u2570\u2500\u27a4  tree\n.\n\u251c\u2500\u2500 docs\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 index.md\n\u2514\u2500\u2500 mkdocs.yml\n</code></pre></p> <p>The <code>mkdocs.yml</code> file is the configuration file for MkDocs. At the moment it will just contain the <code>site_name</code>.</p> <p><pre><code>site_name: My Docs\n</code></pre> The website will appear with the \"My Docs\" title.</p> <p>The <code>docs</code> folder is where we'll write all our pages in Markdown. Inside this folder, we have the file <code>index.md</code>, which serves as a placeholder page.</p>"},{"location":"Misc/websites/#launching-a-test-server","title":"Launching a test server","text":"<p>We've created a new project. To preview our webpage, we can launch a test server with the following command: <pre><code>mkdocs serve\n</code></pre> We can now view our webpage by navigating to <code>127.0.0.1:8000</code> or <code>localhost:8000</code> in our browser. If the <code>8000</code> port is in use, you can specify a different port using:</p> <pre><code>mkdocs serve -a localhost:8001\n</code></pre>"},{"location":"Misc/websites/#website-layout","title":"Website Layout","text":"<p>Looking at <code>docs/index.md</code>, we see that this is our \"home page.\" We can add new pages as new <code>.md</code> files. Any <code>.md</code> files within the <code>docs</code> folder or subfolders will be rendered to HTML. For example, if we create a file <code>about/about.md</code> to contain a small about section, it will be rendered to <code>website.com/about/about/</code>. Alternatively, using <code>about/index.md</code> will render the page to <code>website.com/about</code>.</p>"},{"location":"Misc/websites/#creating-a-navigation-bar","title":"Creating a navigation bar","text":"<p>A navigation bar can be managed from the <code>mkdocs.yml</code> file. This is handled in the <code>nav</code> section of the environment file:</p> <pre><code># For describing the navigation panel\nnav:\n  - Home: index.md\n  - About Me: about/index.md\n  - Research: reasearch/index.md\n  - Projects: projects/index.md\n</code></pre> <p>This links the <code>index.md</code> page to \"Home,\" <code>about/index.md</code> to \"About Me,\" <code>research/index.md</code> to \"Research,\" and <code>projects/index.md</code> to \"Projects.\" This will appear as:</p> <p></p>"},{"location":"Misc/websites/#example-file-layout","title":"Example File Layout","text":"<p>Here is an example file layout. Note that each project or subsection contains its own folder, with its own <code>.md</code> file, and, when appropriate, its own <code>media</code> folder (for images, videos, etc.). Separating each project into a separate folder will help to better organize and compartmentalize individual projects.</p> <pre><code>\u251c\u2500\u2500 docs\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 about\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 cooking.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 cycling.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 fishing.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 index.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 index.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 media\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 Crab_Nebula.jpg\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 projects\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 data_project\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 index.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 media\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 example.csv\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 index.md\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 python_project\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 python_project.ipynb\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 reasearch\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 all_publications.md\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 highlights\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 awesome_paper\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 index.md\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 media\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 einstein.png\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 large_mwl_paper\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u251c\u2500\u2500 index.md\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u2514\u2500\u2500 media\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0         \u2514\u2500\u2500 coma_cluster.png\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 index.md\n\u251c\u2500\u2500 javascripts\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 mathjax.js\n\u251c\u2500\u2500 mkdocs.yml\n</code></pre>"},{"location":"Misc/websites/#adding-media","title":"Adding Media","text":"<p>Media, such as images, can be included using the following syntax:</p> <p><pre><code>![Name of image](./path/to/image.png)\n</code></pre> The image path is a relative path, but can also link to a web-hosted image. </p> <p>Links can be added using a similar syntax, with the <code>!</code> removed: <pre><code>[Text to display](www.website.com)\n</code></pre> or <pre><code>[Internal Link](./path/to/internal/page.md)\n</code></pre></p> <p>Images and be aligned and resized using something like: <pre><code>![Image name](./path/to/image.png){align=\"left\": style=\"height:150;width:150px\"} \n</code></pre> This will align the image to the left of the page and resize the image to be 150x150 pixels (px). </p> <p>This requires the following additions to the <code>mkdocs.yml</code> file: <pre><code>...\nmarkdown_extensions:\n  ...\n  - attr_list   # For adding attributes such as right/left align to images\n  - md_in_html  # Using Figures\n  ...\n</code></pre></p> <p><code>md_in_html</code> allows us to wrap images in \"figures\" (similar to \\(\\LaTeX\\) figures), which provides centering and captions:</p> <pre><code>&lt;figure markdown&gt;\n  ![Image Name](./path/to/image.png){: style=\"height:300px;width:500px\"}\n  &lt;!-- Within fig caption normal markdown linking doesn't work, instead use a href attribute --&gt;\n  &lt;figcaption&gt;Centered and resized image of the Coma Cluster (source &lt;a href=\"https://esahubble.org/images/potw1849a/\"&gt;ESA&lt;/a&gt;) &lt;/figcaption&gt;\n&lt;/figure&gt;\n</code></pre>"},{"location":"Misc/websites/#rendering-latex","title":"Rendering Latex","text":"<p>\\(\\LaTeX\\) equations can be rendered inline (<code>$ equation $</code>) or in display mode (<code>$$ equation $$</code>). To enable this rendering, we'll use a small bit of JavaScript. Add the file <code>mathjax.js</code>:</p> <p><pre><code>    window.MathJax = {\n    tex: {\n      inlineMath: [[\"\\\\(\", \"\\\\)\"]],\n      displayMath: [[\"\\\\[\", \"\\\\]\"]],\n      processEscapes: true,\n      processEnvironments: true\n    },\n    options: {\n      ignoreHtmlClass: \".*|\",\n      processHtmlClass: \"arithmatex\"\n    }\n  };\n\n  document$.subscribe(() =&gt; { \n    MathJax.typesetPromise()\n  })\n</code></pre> Create a <code>javascripts</code> directory in the project directory. Additionally, add the following to the <code>mkdocs.yml</code> file:</p> <pre><code>extra_javascript:\n  - javascripts/mathjax.js\n  - https://polyfill.io/v3/polyfill.min.js?features=es6\n  - https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\n</code></pre> <p>Where <code>mathjax.js</code> is located at <code>project/javascripts/mathjax.js</code>.</p>"},{"location":"Misc/websites/#including-a-bibliography","title":"Including a Bibliography","text":"<p>Using the <code>bibtex</code> plugin and the <code>footnote</code> Markdown extension, one can use citations throughout the website. Ensure you have <code>mkdocs-bibtex</code> and <code>pandoc</code> installed.</p> <pre><code>pip install mkdocs-bibtex\nmamba install pandoc\n</code></pre> <p>Adding the following sections to our <code>mkdocs.yml</code> file:</p> <pre><code>plugins:\n    ...\n    - bibtex:\n        bib_file: \"/path/to/bibtex.bib\"\n        # Optional\n        csl_file: \"/path/to/harvard-cite-them-right-10th-edition.csl\"\n    ...\n\n...\n\nmarkdown_extensions:\n  ...\n  - footnotes\n</code></pre> <p>This assumes we have a <code>.bib</code> file somewhere accessible to our project. Optionally, a <code>csl_file</code> can be specified (CSL = Citation Style Language). CSL files can be found here.</p> <p>Assuming we have an entry in our <code>.bib</code> file like: <pre><code>@ARTICLE{2024ApJ...960...75P,\n...\n}\n</code></pre> We can cite this anywhere on our website using: <pre><code>(remove the \"\\\")\n[@\\2024ApJ...960...75P]\n</code></pre></p> <p>This will render a citation like \\(\\rightarrow\\) [@2024ApJ...960...75P], with the reference appearing at the bottom of the page as a footnote.</p> <p>The entire contents of a <code>.bib</code> file can be displayed with the <code>[\\]full_bibliography</code> command (remove the <code>[]</code>). Note: I've found that I need to have at least 1 citation somewhere within the website in order for the entire bibliography to render.</p>"},{"location":"Misc/websites/#deploying-website","title":"Deploying Website","text":""},{"location":"Misc/websites/#deploying-to-physicsmcgillca","title":"Deploying to physics.mcgill.ca","text":"<p>Anyone with an account on the physics.mcgill.ca cluster can host a website at physics.mcgill.ca/~username. To do this, connect to the cluster and clone the GitHub repo onto the machine. Build the webpage using:</p> <p><pre><code>mkdocs build\n</code></pre> This produces a folder called <code>site</code>. Copy the contents of this folder to <code>~/WWW/</code>. \"Push\" this to the web server using:</p> <p><pre><code>ssh -x www.hep webpush\n</code></pre> After a few moments, the updated website will be available at <code>physics.mcgill.ca/~your_username/</code>. If you copied the directory to something like <code>~/WWW/awesome/</code>, then the page will appear at <code>physics.mcgill.ca/~your_username/awesome/</code>.</p>"},{"location":"Misc/websites/#deploying-to-github-pages","title":"Deploying to GitHub Pages","text":"<p>Every GitHub account has access to a \"GitHub Pages\" domain. GitHub Pages is a static site hosting service. We can deploy to GitHub Pages from MkDocs by simply running:</p> <pre><code>mkdocs gh-deploy\n</code></pre> <p>This renders the markdown to HTML and creates a new branch called <code>gh-pages</code>. </p> <p>After a few moments, the page will be rendered at <code>https://github_username.github.io/repo_name/</code>. For example https://tsi-mcgill.github.io/website-template/. You can access this template on GitHub (see https://github.com/tsi-mcgill/website-template).</p> <p>If you are cloning an existing template, be sure to create a new repo on <code>GitHub</code> and add this repo as the remote origin. For example: <pre><code>git clone git@github.com:tsi-mcgill/website-template.git\nmv website-template website-example\ncd website-example\ngit remote set-url origin git@github.com:steob92/website-example.git\n</code></pre></p> <p>Alternatively click on the \"Use this template\" dropdown menu and follow the instructions. </p> <p>The GitHub Page can be managed from the Settings of the GitHub repo. </p>"},{"location":"Misc/websites/#themes","title":"Themes","text":"<p>MkDocs has many supported themes to allow for customization to one's desire. Changing the theme is handled from the <code>mkdocs.yml</code> file. See here for examples of how to change the theme and here for a list of popular third-party themes.</p>"},{"location":"Python/","title":"Python","text":"<ul> <li>Introduction to Python (Delivered as part of Hackathon)</li> <li>Introduction to Gaia Data (with Python)</li> <li>Benchmarking and Profiling</li> <li>Threading And Multiprocessing</li> <li>Object Orientated Programming and Packaging</li> <li>Scientific Programming in Python (Delivered as part of Phys602)</li> <li>Calculating \\(\\pi\\) using Monte Carlo Methods (Delivered as part of Hackathon)</li> <li>N-Body Simulation (Delivered as part of Hackathon)</li> <li>Introduction to Machine Learning in Astronomy (Delivered as part of Hackathon)</li> </ul>"},{"location":"Python/BenchmarkingAndProfiling-Empty/","title":"Benchmarking and Profiling","text":"In\u00a0[\u00a0]: Copied! <pre>import time\nimport timeit\nimport numpy as np\nimport matplotlib.pyplot as plt\n</pre> import time import timeit import numpy as np import matplotlib.pyplot as plt In\u00a0[\u00a0]: Copied! <pre>%%timeit\ntest = 0\nfor i in range(100000):\n    test += 1\n</pre> %%timeit test = 0 for i in range(100000):     test += 1 <p>We can also measure the time of the cell from one iteration using <code>%%time</code></p> In\u00a0[\u00a0]: Copied! <pre>%%time\ntest = 0\nfor i in range(100000):\n    test += 1\n</pre> %%time test = 0 for i in range(100000):     test += 1 <p>We see a number of different times listed here:</p> <ul> <li><code>Wall time</code>: This is the time that has passed during the running of the code</li> <li><code>CPU time</code>: This is the time that the CPU was busy. This also includes time when the CPU has \"swapped\" its attention to a different process.</li> <li><code>user</code>: This is the time that the process has been running on the CPU but outside of the system kernel</li> <li><code>sys</code> : This is the time taken inside the system kernel. This could indicate that the code requires a lot of system calls rather than calls written by the developer.</li> </ul> <p>As a rule of thumb:</p> <ul> <li>CPU Time / Wall Time ~ 1. The process spent most of it's time using the CPU. A faster CPU could improve performance.</li> <li>CPU Time / Wall Time &lt; 1. This suggest the process has spent time waiting. This could be resource allocations, network/hardware IO, locks (to prevent data races) etc. The smaller the number, the more time spent waiting. 0.9 would suggest 10% of the time the CPU is idle.</li> <li>CPU Time / Wall Time &gt; 1. This suggests that we are utilizing multiple processors when running our code.</li> </ul> <p>If we are running with N processors:</p> <ul> <li>CPU Time / Wall Time ~ N. The process is well distributed across all CPUs with little to no idle time.</li> <li>CPU Time / Wall Time &lt; N. This suggest the process has spent time waiting. This could be resource allocations, network/hardware IO, locks (to prevent data races) etc.</li> </ul> <p>We can use <code>%timeit</code> to time an single line or process, averaging over multiple runs:</p> In\u00a0[\u00a0]: Copied! <pre>print (\"This line won't be timed\")\n# Either will this line\ntest = np.random.random(1000)\n# This line will be timed\n%timeit test = np.random.random(1000)\n</pre> print (\"This line won't be timed\") # Either will this line test = np.random.random(1000) # This line will be timed %timeit test = np.random.random(1000) <p>Or we can use <code>%time</code> to time an single line or process, with no averaging:</p> In\u00a0[\u00a0]: Copied! <pre>print (\"This line won't be timed\")\n# Either will this line\ntest = np.random.random(1000)\n# This line will be timed\n%time test = np.random.random(1000)\n</pre> print (\"This line won't be timed\") # Either will this line test = np.random.random(1000) # This line will be timed %time test = np.random.random(1000) <p>Let's look at comparing two functions. Consider vector addition. In a pure Python, we would use a for loop to iterate over each element. We could also use numpy's inbuild functions to perform the addition/</p> In\u00a0[\u00a0]: Copied! <pre>def add_two_arrays(a : np.ndarray,b : np.ndarray) -&gt; np.ndarray:\n    \"\"\"Add two numpy arrays togeter using a for loop\n\n    Args:\n        a : array 1 to be added\n        b : array 2 to be added\n\n    Returns:\n        array of a + b\n    \"\"\"\n    c = np.zeros(len(a))\n    for i in range(len(a)):\n        c[i] = a[i] + b[i]\n    return c\n\ndef add_two_arrays_numpy(a : np.ndarray, b : np.ndarray) -&gt; np.ndarray:\n    \"\"\"Add two numpy arrays togeter using vectorization\n\n    Args:\n        a : array 1 to be added\n        b : array 2 to be added\n\n    Returns:\n        array of a + b\n    \"\"\"\n    c = a + b\n    return c\n</pre> def add_two_arrays(a : np.ndarray,b : np.ndarray) -&gt; np.ndarray:     \"\"\"Add two numpy arrays togeter using a for loop      Args:         a : array 1 to be added         b : array 2 to be added      Returns:         array of a + b     \"\"\"     c = np.zeros(len(a))     for i in range(len(a)):         c[i] = a[i] + b[i]     return c  def add_two_arrays_numpy(a : np.ndarray, b : np.ndarray) -&gt; np.ndarray:     \"\"\"Add two numpy arrays togeter using vectorization      Args:         a : array 1 to be added         b : array 2 to be added      Returns:         array of a + b     \"\"\"     c = a + b     return c In\u00a0[\u00a0]: Copied! <pre>test_x = np.random.random(1000)\ntest_y = np.random.random(1000)\n\nprint(\"Pure Python:\")\n%timeit test_z = add_two_arrays(test_x, test_y)\nprint(\"Numpy:\")\n%timeit test_z = add_two_arrays_numpy(test_x, test_y)\n</pre> test_x = np.random.random(1000) test_y = np.random.random(1000)  print(\"Pure Python:\") %timeit test_z = add_two_arrays(test_x, test_y) print(\"Numpy:\") %timeit test_z = add_two_arrays_numpy(test_x, test_y)  <p>We see a dramatic improvement by using NumPy arrays to add two vectors together. This is because NumPy is a highly optimized library, with the low-level code written in C++. Is this always the case?</p> In\u00a0[\u00a0]: Copied! <pre>def time_addition(n):\n    \n    test_x = np.random.random(n)\n    test_y = np.random.random(n)\n    \n    n_repeat = 10\n    \n    # pure python\n    t_start = time.time()\n    for _ in range(n_repeat):\n        _ = add_two_arrays(test_x, test_y)\n    t_pure = np.median(time.time() - t_start)\n\n    # numpy\n    t_start = time.time()\n    for _ in range(n_repeat):\n        _ = add_two_arrays_numpy(test_x, test_y)\n    t_numpy = np.median(time.time() - t_start)\n\n    return t_pure, t_numpy\n</pre> def time_addition(n):          test_x = np.random.random(n)     test_y = np.random.random(n)          n_repeat = 10          # pure python     t_start = time.time()     for _ in range(n_repeat):         _ = add_two_arrays(test_x, test_y)     t_pure = np.median(time.time() - t_start)      # numpy     t_start = time.time()     for _ in range(n_repeat):         _ = add_two_arrays_numpy(test_x, test_y)     t_numpy = np.median(time.time() - t_start)      return t_pure, t_numpy In\u00a0[\u00a0]: Copied! <pre>n_array = np.arange(20)\ntimes_py = np.zeros(n_array.shape)\ntimes_np = np.zeros(n_array.shape)\n\nfor i, n in enumerate(n_array):\n    times_py[i], times_np[i] = time_addition(int(2**n))\n</pre> n_array = np.arange(20) times_py = np.zeros(n_array.shape) times_np = np.zeros(n_array.shape)  for i, n in enumerate(n_array):     times_py[i], times_np[i] = time_addition(int(2**n))   In\u00a0[\u00a0]: Copied! <pre>plt.plot(2 ** n_array, times_py, \"C0s:\", label = \"Pure Python\")\nplt.plot(2 ** n_array, times_np, \"C1o--\", label = \"Numpy\")\nplt.loglog()\nplt.grid()\nplt.legend()\nplt.xlabel(\"Array Length\")\nplt.ylabel(\"Run Time [s]\");\n</pre> plt.plot(2 ** n_array, times_py, \"C0s:\", label = \"Pure Python\") plt.plot(2 ** n_array, times_np, \"C1o--\", label = \"Numpy\") plt.loglog() plt.grid() plt.legend() plt.xlabel(\"Array Length\") plt.ylabel(\"Run Time [s]\");   <p>At very small array sizes the performance is comparable. This is likely due to the overhead of calling the low level NumPy code. The NumPy perfomance saturates around $10^2$ - $10^3$ before scaling in a comparatable fashion to pure python, however it is still orders of magnitude faster. This perfomance will be system dependent!</p> <p>It might be easy to assume for the above example that NumPy is always better than pure Python, but this isn't always the case and we should always consider our specific needs.</p> <p>If we where writing code to operate on small (O($10^1$)), we may be faster to use a pure python implementation rather than importing NumPy and introducing an additional overhead.</p> <p>Let's look at an expensive function that uses a MC method to estimate $\\pi$.</p> <p>We're going to throw random points $x, y \\in [0,1]$ into a square of length 1. This will give an estimate of the the area of the square:</p> <p>$$ A_{square} \\propto N_{total}$$ The area of a circle is given by: $$ A_{circle} = \\pi r^2$$ Since we're throwing random numbers in the range $[0,1]$, we will fill a quatre of a circle. $$ A_{circle} \\propto 4 * \\tilde{N_{within}}$$ Where $\\tilde{N_{within}}$ is the number of samples within 1 of the origin (or $x^2 + y^2 &lt;1$).</p> <p>Combing these, we can estimate $pi$ as: $$ \\pi \\approx 4 \\frac{N_{within}}{N_{total}}$$</p> In\u00a0[\u00a0]: Copied! <pre>def monte_carlo_pi(nsamples):\n    acc = 0\n    for i in range(nsamples):\n        x = np.random.random()\n        y = np.random.random()\n        if (x ** 2 + y ** 2) &lt; 1.0:\n            acc += 1\n    return 4.0 * acc / nsamples\n</pre> def monte_carlo_pi(nsamples):     acc = 0     for i in range(nsamples):         x = np.random.random()         y = np.random.random()         if (x ** 2 + y ** 2) &lt; 1.0:             acc += 1     return 4.0 * acc / nsamples In\u00a0[\u00a0]: Copied! <pre>n_range = np.arange(7)\n\npi_diff = [np.abs(np.pi - monte_carlo_pi(10**n)) for n in n_range]\n</pre> n_range = np.arange(7)  pi_diff = [np.abs(np.pi - monte_carlo_pi(10**n)) for n in n_range] In\u00a0[\u00a0]: Copied! <pre>plt.plot(10**n_range, pi_diff)\nplt.xlabel(\"Number of Random Samples\")\nplt.ylabel(\"$|\\pi_{NumPy} - \\pi_{MC}|$\")\nplt.grid()\nplt.xscale('log')\n#plt.yscale('log')\n</pre> plt.plot(10**n_range, pi_diff) plt.xlabel(\"Number of Random Samples\") plt.ylabel(\"$|\\pi_{NumPy} - \\pi_{MC}|$\") plt.grid() plt.xscale('log') #plt.yscale('log') <p>We can see that as $N_{total}$ increases we tend to the NumPy value of $\\pi$. We obtain deminising returns at around $10^4$ samples. Let's choose this as our benchmark point.</p> In\u00a0[\u00a0]: Copied! <pre>n_total = 10 ** 4\n</pre> n_total = 10 ** 4 <p>How long does this take to run?</p> In\u00a0[\u00a0]: Copied! <pre>%timeit monte_carlo_pi(n_total)\n</pre> %timeit monte_carlo_pi(n_total) In\u00a0[\u00a0]: Copied! <pre>n_run = 100\nn_repeat = 10\nt_original = np.array(\n    timeit.repeat(f\"monte_carlo_pi({n_total})\", \"from __main__ import monte_carlo_pi\", repeat = n_repeat,  number = n_run)\n) / n_run\n</pre> n_run = 100 n_repeat = 10 t_original = np.array(     timeit.repeat(f\"monte_carlo_pi({n_total})\", \"from __main__ import monte_carlo_pi\", repeat = n_repeat,  number = n_run) ) / n_run In\u00a0[\u00a0]: Copied! <pre>print(f\"Original Runtime = {np.mean(t_original) : 0.2e} +/- {np.std(t_original) :0.2e} s\")\n</pre> print(f\"Original Runtime = {np.mean(t_original) : 0.2e} +/- {np.std(t_original) :0.2e} s\")  <p>If we want to speed up this calculation, we should first find what is the major bottleneck.</p> In\u00a0[\u00a0]: Copied! <pre>%load_ext line_profiler\n</pre> %load_ext line_profiler In\u00a0[\u00a0]: Copied! <pre>%lprun -f monte_carlo_pi monte_carlo_pi(n_total)\n</pre> %lprun -f monte_carlo_pi monte_carlo_pi(n_total) <p>The above shows a breakdown of where we're spending time. We have each line of the function broken down. We can see that the allocations of <code>acc</code> and the return of <code>4.0 * acc / nsamples</code> are expensive operations, but this only happens once per execution. Since these only happen once there may be some \"noise\" effecting these estimates.</p> <p><code>for i in range(nsamples):</code> takes 8% of our time. This is pretty significant. Python is also nutoriously bad at looping. If we can avoid looping in Python we should.</p> <p>We also see that <code>acc += 1</code> doesn't happen every iteration, and we spend ~20% of our time checking <code>if (x ** 2 + y ** 2) &lt; 1.0</code>. We can see that the dominate part of the time is spent generating random numbers (<code>x/y = np.random.random()</code>) which takes ~60% of the run time.</p> <p>If we were looking to speed up this function, we should work target the most expensive parts first, this would suggest targeting:</p> <pre><code>        x = np.random.random()\n        y = np.random.random()\n        if (x ** 2 + y ** 2) &lt; 1.0:\n</code></pre> <p>Which constitutes 80% or our run time. Since these are ran 10000 times, it might make more sense to somehow only run this once. But how...</p> In\u00a0[\u00a0]: Copied! <pre>def monte_carlo_pi_pure_numpy(nsamples):\n</pre> def monte_carlo_pi_pure_numpy(nsamples):  In\u00a0[\u00a0]: Copied! <pre>%timeit monte_carlo_pi_pure_numpy(n_total)\n</pre> %timeit monte_carlo_pi_pure_numpy(n_total) In\u00a0[\u00a0]: Copied! <pre>n_run = 1000\nn_repeat = 10\nt_optimized = np.array(\n    timeit.repeat(f\"monte_carlo_pi_pure_numpy({n_total})\", \"from __main__ import monte_carlo_pi_pure_numpy\", repeat = n_repeat,  number = n_run)\n) / n_run\nt_optimized\n</pre> n_run = 1000 n_repeat = 10 t_optimized = np.array(     timeit.repeat(f\"monte_carlo_pi_pure_numpy({n_total})\", \"from __main__ import monte_carlo_pi_pure_numpy\", repeat = n_repeat,  number = n_run) ) / n_run t_optimized In\u00a0[\u00a0]: Copied! <pre>print(f\"Optimized Runtime = {np.mean(t_optimized) : 0.2e} +/- {np.std(t_optimized) :0.2e} s\")\n</pre> print(f\"Optimized Runtime = {np.mean(t_optimized) : 0.2e} +/- {np.std(t_optimized) :0.2e} s\")  In\u00a0[\u00a0]: Copied! <pre>%lprun -f monte_carlo_pi_pure_numpy monte_carlo_pi_pure_numpy(n_total)\n</pre> %lprun -f monte_carlo_pi_pure_numpy monte_carlo_pi_pure_numpy(n_total) In\u00a0[\u00a0]: Copied! <pre>speedup =  np.mean(t_original) / np.mean(t_optimized)\nprint (f\"Optmization speeds up the code by a factor of {speedup:0.2f}\")\n</pre> speedup =  np.mean(t_original) / np.mean(t_optimized) print (f\"Optmization speeds up the code by a factor of {speedup:0.2f}\") In\u00a0[\u00a0]: Copied! <pre># !pip install numba\n</pre> # !pip install numba In\u00a0[\u00a0]: Copied! <pre>from numba import jit\n</pre> from numba import jit In\u00a0[\u00a0]: Copied! <pre>def monte_carlo_pi_pure_numpy_jit(nsamples):\n</pre> def monte_carlo_pi_pure_numpy_jit(nsamples):  In\u00a0[\u00a0]: Copied! <pre>t1 = time.time()\nmonte_carlo_pi_pure_numpy_jit(n_total)\ntelap = time.time() - t1\nprint (f\"This took {telap}s\")\n</pre> t1 = time.time() monte_carlo_pi_pure_numpy_jit(n_total) telap = time.time() - t1 print (f\"This took {telap}s\") In\u00a0[\u00a0]: Copied! <pre>t1 = time.time()\nmonte_carlo_pi_pure_numpy_jit(n_total)\ntelap = time.time() - t1\nprint (f\"This took {telap}s\")\n</pre> t1 = time.time() monte_carlo_pi_pure_numpy_jit(n_total) telap = time.time() - t1 print (f\"This took {telap}s\") In\u00a0[\u00a0]: Copied! <pre>n_run = 1000\nn_repeat = 10\nt_jitted = np.array(\n    timeit.repeat(f\"monte_carlo_pi_pure_numpy_jit({n_total})\", \"from __main__ import monte_carlo_pi_pure_numpy_jit\", repeat = n_repeat,  number = n_run)\n) / n_run\n# t_jitted\n</pre> n_run = 1000 n_repeat = 10 t_jitted = np.array(     timeit.repeat(f\"monte_carlo_pi_pure_numpy_jit({n_total})\", \"from __main__ import monte_carlo_pi_pure_numpy_jit\", repeat = n_repeat,  number = n_run) ) / n_run # t_jitted In\u00a0[\u00a0]: Copied! <pre>print(f\"jitted Runtime = {np.mean(t_jitted) : 0.2e} +/- {np.std(t_jitted) :0.2e} s\")\n</pre> print(f\"jitted Runtime = {np.mean(t_jitted) : 0.2e} +/- {np.std(t_jitted) :0.2e} s\")  In\u00a0[\u00a0]: Copied! <pre>speedup =  np.mean(t_original) / np.mean(t_jitted)\nprint (f\"Optmization speeds up the code by a factor of {speedup:0.2f}\")\n</pre> speedup =  np.mean(t_original) / np.mean(t_jitted) print (f\"Optmization speeds up the code by a factor of {speedup:0.2f}\") In\u00a0[\u00a0]: Copied! <pre>def monte_carlo_pi_pure_numpy_jit_parallel(nsamples):\n</pre> def monte_carlo_pi_pure_numpy_jit_parallel(nsamples):  In\u00a0[\u00a0]: Copied! <pre>t1 = time.time()\nmonte_carlo_pi_pure_numpy_jit_parallel(n_total)\ntelap = time.time() - t1\nprint (f\"This took {telap}s\")\n</pre> t1 = time.time() monte_carlo_pi_pure_numpy_jit_parallel(n_total) telap = time.time() - t1 print (f\"This took {telap}s\") In\u00a0[\u00a0]: Copied! <pre>n_run = 10000\nn_repeat = 10\nt_jitted_parallel = np.array(\n    timeit.repeat(f\"monte_carlo_pi_pure_numpy_jit_parallel({n_total})\", \"from __main__ import monte_carlo_pi_pure_numpy_jit_parallel\", repeat = n_repeat,  number = n_run)\n) / n_run\nt_jitted_parallel\n</pre> n_run = 10000 n_repeat = 10 t_jitted_parallel = np.array(     timeit.repeat(f\"monte_carlo_pi_pure_numpy_jit_parallel({n_total})\", \"from __main__ import monte_carlo_pi_pure_numpy_jit_parallel\", repeat = n_repeat,  number = n_run) ) / n_run t_jitted_parallel In\u00a0[\u00a0]: Copied! <pre>print(f\"jitted parallel Runtime = {np.mean(t_jitted_parallel) : 0.2e} +/- {np.std(t_jitted_parallel) :0.2e} s\")\n</pre> print(f\"jitted parallel Runtime = {np.mean(t_jitted_parallel) : 0.2e} +/- {np.std(t_jitted_parallel) :0.2e} s\")  In\u00a0[\u00a0]: Copied! <pre>speedup =  np.mean(t_original) / np.mean(t_jitted_parallel)\nprint (f\"Optmization speeds up the code by a factor of {speedup:0.2f}\")\n</pre> speedup =  np.mean(t_original) / np.mean(t_jitted_parallel) print (f\"Optmization speeds up the code by a factor of {speedup:0.2f}\") In\u00a0[\u00a0]: Copied! <pre>def monte_carlo_pi_pure_numpy_jit_parallel_nopython(nsamples):\n</pre> def monte_carlo_pi_pure_numpy_jit_parallel_nopython(nsamples):  In\u00a0[\u00a0]: Copied! <pre>t1 = time.time()\nmonte_carlo_pi_pure_numpy_jit_parallel_nopython(n_total)\ntelap = time.time() - t1\nprint (f\"This took {telap}s\")\n</pre> t1 = time.time() monte_carlo_pi_pure_numpy_jit_parallel_nopython(n_total) telap = time.time() - t1 print (f\"This took {telap}s\") In\u00a0[\u00a0]: Copied! <pre>n_run = 10000\nn_repeat = 10\nt_jitted_parallel_nopython = np.array(\n    timeit.repeat(f\"monte_carlo_pi_pure_numpy_jit_parallel_nopython({n_total})\", \"from __main__ import monte_carlo_pi_pure_numpy_jit_parallel_nopython\", repeat = n_repeat,  number = n_run)\n) / n_run\nt_jitted_parallel_nopython\n</pre> n_run = 10000 n_repeat = 10 t_jitted_parallel_nopython = np.array(     timeit.repeat(f\"monte_carlo_pi_pure_numpy_jit_parallel_nopython({n_total})\", \"from __main__ import monte_carlo_pi_pure_numpy_jit_parallel_nopython\", repeat = n_repeat,  number = n_run) ) / n_run t_jitted_parallel_nopython In\u00a0[\u00a0]: Copied! <pre>print(f\"jitted parallel no python Runtime = {np.mean(t_jitted_parallel_nopython) : 0.2e} +/- {np.std(t_jitted_parallel_nopython) :0.2e} s\")\n</pre> print(f\"jitted parallel no python Runtime = {np.mean(t_jitted_parallel_nopython) : 0.2e} +/- {np.std(t_jitted_parallel_nopython) :0.2e} s\")  In\u00a0[\u00a0]: Copied! <pre>speedup =  np.mean(t_original) / np.mean(t_jitted_parallel_nopython)\nprint (f\"Optmization speeds up the code by a factor of {speedup:0.2f}\")\n</pre> speedup =  np.mean(t_original) / np.mean(t_jitted_parallel_nopython) print (f\"Optmization speeds up the code by a factor of {speedup:0.2f}\") In\u00a0[\u00a0]: Copied! <pre>def monte_carlo_pi_revisit(nsamples):\n</pre> def monte_carlo_pi_revisit(nsamples):  In\u00a0[\u00a0]: Copied! <pre>t1 = time.time()\nmonte_carlo_pi_revisit(n_total)\ntelap = time.time() - t1\nprint (f\"This took {telap}s\")\n</pre> t1 = time.time() monte_carlo_pi_revisit(n_total) telap = time.time() - t1 print (f\"This took {telap}s\") In\u00a0[\u00a0]: Copied! <pre>n_run = 10000\nn_repeat = 10\nt_jitted_revisit = np.array(\n    timeit.repeat(f\"monte_carlo_pi_revisit({n_total})\", \"from __main__ import monte_carlo_pi_revisit\", repeat = n_repeat,  number = n_run)\n) / n_run\nt_jitted_revisit\n</pre> n_run = 10000 n_repeat = 10 t_jitted_revisit = np.array(     timeit.repeat(f\"monte_carlo_pi_revisit({n_total})\", \"from __main__ import monte_carlo_pi_revisit\", repeat = n_repeat,  number = n_run) ) / n_run t_jitted_revisit In\u00a0[\u00a0]: Copied! <pre>print(f\"jitted parallel no python Runtime = {np.mean(t_jitted_revisit) : 0.2e} +/- {np.std(t_jitted_revisit) :0.2e} s\")\n</pre> print(f\"jitted parallel no python Runtime = {np.mean(t_jitted_revisit) : 0.2e} +/- {np.std(t_jitted_revisit) :0.2e} s\")  In\u00a0[\u00a0]: Copied! <pre>speedup =  np.mean(t_original) / np.mean(t_jitted_revisit)\nprint (f\"Optmization speeds up the code by a factor of {speedup:0.2f}\")\n</pre> speedup =  np.mean(t_original) / np.mean(t_jitted_revisit) print (f\"Optmization speeds up the code by a factor of {speedup:0.2f}\") In\u00a0[\u00a0]: Copied! <pre>speedup =  np.mean(t_original) / np.mean(t_optimized)\nprint (f\"Pure NumPy speeds up the code by a factor of {speedup:0.2f}\")\n\nspeedup =  np.mean(t_original) / np.mean(t_jitted)\nprint (f\"Compiled NumPy speeds up the code by a factor of {speedup:0.2f}\")\n\nspeedup =  np.mean(t_original) / np.mean(t_jitted_parallel)\nprint (f\"Parallel annd compiled NumPy speeds up the code by a factor of {speedup:0.2f}\")\n\nspeedup =  np.mean(t_original) / np.mean(t_jitted_parallel_nopython)\nprint (f\"Parallel annd compiled NumPy with 'no python' speeds up the code by a factor of {speedup:0.2f}\")\n\nspeedup =  np.mean(t_original) / np.mean(t_jitted_revisit)\nprint (f\"Parallel and complied python speeds up the code by a factor of {speedup:0.2f}\")\n</pre> speedup =  np.mean(t_original) / np.mean(t_optimized) print (f\"Pure NumPy speeds up the code by a factor of {speedup:0.2f}\")  speedup =  np.mean(t_original) / np.mean(t_jitted) print (f\"Compiled NumPy speeds up the code by a factor of {speedup:0.2f}\")  speedup =  np.mean(t_original) / np.mean(t_jitted_parallel) print (f\"Parallel annd compiled NumPy speeds up the code by a factor of {speedup:0.2f}\")  speedup =  np.mean(t_original) / np.mean(t_jitted_parallel_nopython) print (f\"Parallel annd compiled NumPy with 'no python' speeds up the code by a factor of {speedup:0.2f}\")  speedup =  np.mean(t_original) / np.mean(t_jitted_revisit) print (f\"Parallel and complied python speeds up the code by a factor of {speedup:0.2f}\") <p>The compiled and parallel implementations offer the best performance. If we don't have access to a multi-core system, we achieve x100 performace by simply compiling the function with numba and jit.</p> In\u00a0[\u00a0]: Copied! <pre># if you have issues with async switch to v0.61\n# pip install memory_profiler==0.61\n</pre> # if you have issues with async switch to v0.61 # pip install memory_profiler==0.61 In\u00a0[1]: Copied! <pre>%load_ext memory_profiler\n# to automatically reload changed files\n%load_ext autoreload\n%autoreload 2\n</pre> %load_ext memory_profiler # to automatically reload changed files %load_ext autoreload %autoreload 2 In\u00a0[2]: Copied! <pre>%memit big_array = [i for i in range(10**7)]\n</pre> %memit big_array = [i for i in range(10**7)] <pre>peak memory: 445.73 MiB, increment: 383.50 MiB\n</pre> <p>If we want to try and find what processes are using a lot of memory, we can line profile a function.</p> In\u00a0[3]: Copied! <pre>%%writefile function_to_profile.py\nimport time\n\ndef expensive_function():\n    \n    x = [ 120 for i in range(100) ]\n    b = [ x * 100 for i in range(1000) ]\n    big_list = [ i for i in range(10**5) ]\n    another_big_list = [ i for i in range(10**5) ]\n    time.sleep(0.01)\n\n    x = [ 42 for i in range(1000) ]\n</pre> %%writefile function_to_profile.py import time  def expensive_function():          x = [ 120 for i in range(100) ]     b = [ x * 100 for i in range(1000) ]     big_list = [ i for i in range(10**5) ]     another_big_list = [ i for i in range(10**5) ]     time.sleep(0.01)      x = [ 42 for i in range(1000) ]  <pre>Overwriting function_to_profile.py\n</pre> In\u00a0[4]: Copied! <pre>from function_to_profile import expensive_function\n%mprun -f expensive_function expensive_function()\n</pre> from function_to_profile import expensive_function %mprun -f expensive_function expensive_function() <pre>\n</pre> <pre>Filename: /home/obriens/Documents/websites/mcgill_site/docs/Tutorials/Python/function_to_profile.py\n\nLine #    Mem usage    Increment  Occurrences   Line Contents\n=============================================================\n     3    446.0 MiB    446.0 MiB           1   def expensive_function():\n     4                                             \n     5    446.0 MiB      0.0 MiB         103       x = [ 120 for i in range(100) ]\n     6    522.0 MiB     76.0 MiB        1003       b = [ x * 100 for i in range(1000) ]\n     7    525.6 MiB      3.6 MiB      100003       big_list = [ i for i in range(10**5) ]\n     8    530.2 MiB      4.6 MiB      100003       another_big_list = [ i for i in range(10**5) ]\n     9    530.2 MiB      0.0 MiB           1       time.sleep(0.01)\n    10                                         \n    11    530.2 MiB      0.0 MiB        1003       x = [ 42 for i in range(1000) ]</pre> <p>This can often be difficult to parse and understand, in a jupyter notebook. I prefer to use <code>memory_profiler</code> via the command line interface (which we can still call from jupyter!).</p> In\u00a0[5]: Copied! <pre>%%writefile initial_code.py\n\nimport numpy as np\nimport pandas as pd\nimport time\n\n@profile\ndef load_data():\n    df = pd.read_csv(\"./gaia3.csv\")\n    return df\n\n@profile\ndef filter_data(df):\n    parallax = df[\"parallax\"]\n    parallax_err = df[\"parallax_over_error\"]\n    good_mask = parallax_err &gt; 20\n    return good_mask\n    \n@profile\ndef get_distance(df):\n    distance = 1./ (df[\"parallax\"] * 1e-3)\n    return distance\n\n@profile\ndef assign_distance(df, distance):\n    df[\"distance\"] = distance\n\n\n\n@profile\ndef filter_distance(df):\n    distance = df[\"distance\"]\n    good_mask = distance &lt; 10\n    return good_mask\n\n\n@profile\ndef apply_filter(df, data_filter):\n    data = df[data_filter]\n    return data\n\n\n\nif __name__ == '__main__':\n\n    data = load_data()\n    time.sleep(0.1)\n    \n    data_filter = filter_data(data)\n    time.sleep(0.1)\n    \n    data_filtered = apply_filter(data, data_filter)\n    time.sleep(0.1)\n    \n    distance = get_distance(data_filtered)\n    time.sleep(0.1)\n    \n    assign_distance(data_filtered, distance)\n    time.sleep(0.1)\n    \n    distance_filter = filter_distance(data_filtered)\n    time.sleep(0.1)\n    \n    data_10pc = apply_filter(data_filtered, distance_filter)\n    time.sleep(0.1)\n</pre> %%writefile initial_code.py  import numpy as np import pandas as pd import time  @profile def load_data():     df = pd.read_csv(\"./gaia3.csv\")     return df  @profile def filter_data(df):     parallax = df[\"parallax\"]     parallax_err = df[\"parallax_over_error\"]     good_mask = parallax_err &gt; 20     return good_mask      @profile def get_distance(df):     distance = 1./ (df[\"parallax\"] * 1e-3)     return distance  @profile def assign_distance(df, distance):     df[\"distance\"] = distance    @profile def filter_distance(df):     distance = df[\"distance\"]     good_mask = distance &lt; 10     return good_mask   @profile def apply_filter(df, data_filter):     data = df[data_filter]     return data    if __name__ == '__main__':      data = load_data()     time.sleep(0.1)          data_filter = filter_data(data)     time.sleep(0.1)          data_filtered = apply_filter(data, data_filter)     time.sleep(0.1)          distance = get_distance(data_filtered)     time.sleep(0.1)          assign_distance(data_filtered, distance)     time.sleep(0.1)          distance_filter = filter_distance(data_filtered)     time.sleep(0.1)          data_10pc = apply_filter(data_filtered, distance_filter)     time.sleep(0.1)    <pre>Overwriting initial_code.py\n</pre> In\u00a0[6]: Copied! <pre>!mprof run --interval 0.005 --python python initial_code.py\n!mprof plot --output initial_code_profile.png\n</pre> !mprof run --interval 0.005 --python python initial_code.py !mprof plot --output initial_code_profile.png <pre>mprof: Sampling memory every 0.005s\nrunning new process\nrunning as a Python program...\ninitial_code.py:25: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df[\"distance\"] = distance\nUsing last profile data.\n</pre> In\u00a0[7]: Copied! <pre>from IPython.display import Image, display\n</pre> from IPython.display import Image, display In\u00a0[8]: Copied! <pre>display(Image(\"./initial_code_profile.png\"))\n</pre> display(Image(\"./initial_code_profile.png\")) <p>From the above we can see a few things:</p> <ol> <li>The file we read in is pretty big. The dataframe takes up ~250 MiB.</li> <li>Filtering the data has a negligable effect on the memory usage.</li> <li>We see a huge spike when calling <code>apply_filter</code> for the first time.</li> <li>We see a second smaller spike when calling <code>get_distance</code>.</li> </ol> <p>A plot like this is useful when we're trying to decide on what resources we need to run this program. We could likely run this on most machines as ~400 MiB is attainable by most personal laptops. If we wanted to submit this job to a cluster, we'd want at least ~450 MiB to provide some breathing room.</p> <p>If we want to target areas to reduce memory usage, we should target the <code>apply_filter</code> and <code>get_distance</code> functions. Additionally we can look at ways to reduce the dataset size (we'll come back to this...).</p> <p>Let's look at <code>apply_filter</code>:</p> <pre>@profile\ndef apply_filter(df, data_filter):\n    data = df[data_filter]\n    return data\n...\n    data_filtered = apply_filter(data, data_filter)\n...\n    data_10pc = apply_filter(data_filtered, distance_filter)\n...\n</pre> <p>We are using the <code>apply_filter</code>. This function takes in a <code>pandas.dataframe</code> and a boolean array and returns a new <code>pandas.dataframe</code>. Let's start by overwriting the data in the function:</p> <pre>@profile\ndef apply_filter(df, data_filter):\n    df = df[data_filter]\n    return df\n</pre> <p>Additionally we'll overwrite all the instances of <code>data</code></p> In\u00a0[13]: Copied! <pre>%%writefile initial_code_iteration1.py\n\nimport numpy as np\nimport pandas as pd\nimport time\n\n@profile\ndef load_data():\n    df = pd.read_csv(\"./gaia3.csv\")\n    return df\n\n@profile\ndef filter_data(df):\n    parallax = df[\"parallax\"]\n    parallax_err = df[\"parallax_over_error\"]\n    good_mask = parallax_err &gt; 20\n    return good_mask\n    \n@profile\ndef get_distance(df):\n    distance = 1./ (df[\"parallax\"] * 1e-3)\n    return distance\n\n@profile\ndef assign_distance(df, distance):\n    df[\"distance\"] = distance\n\n\n\n@profile\ndef filter_distance(df):\n    distance = df[\"distance\"]\n    good_mask = distance &lt; 10\n    return good_mask\n\n\n@profile\ndef apply_filter(df, data_filter):\n    df = df[data_filter]\n    return df\n\n\n\nif __name__ == '__main__':\n\n    data = load_data()\n    time.sleep(0.1)\n    \n    data_filter = filter_data(data)\n    time.sleep(0.1)\n    \n    data_filtered = apply_filter(data, data_filter)\n    time.sleep(0.1)\n    \n    distance = get_distance(data_filtered)\n    time.sleep(0.1)\n    \n    assign_distance(data_filtered, distance)\n    time.sleep(0.1)\n    \n    distance_filter = filter_distance(data_filtered)\n    time.sleep(0.1)\n    \n    data_10pc = apply_filter(data_filtered, distance_filter)\n    time.sleep(0.1)\n</pre> %%writefile initial_code_iteration1.py  import numpy as np import pandas as pd import time  @profile def load_data():     df = pd.read_csv(\"./gaia3.csv\")     return df  @profile def filter_data(df):     parallax = df[\"parallax\"]     parallax_err = df[\"parallax_over_error\"]     good_mask = parallax_err &gt; 20     return good_mask      @profile def get_distance(df):     distance = 1./ (df[\"parallax\"] * 1e-3)     return distance  @profile def assign_distance(df, distance):     df[\"distance\"] = distance    @profile def filter_distance(df):     distance = df[\"distance\"]     good_mask = distance &lt; 10     return good_mask   @profile def apply_filter(df, data_filter):     df = df[data_filter]     return df    if __name__ == '__main__':      data = load_data()     time.sleep(0.1)          data_filter = filter_data(data)     time.sleep(0.1)          data_filtered = apply_filter(data, data_filter)     time.sleep(0.1)          distance = get_distance(data_filtered)     time.sleep(0.1)          assign_distance(data_filtered, distance)     time.sleep(0.1)          distance_filter = filter_distance(data_filtered)     time.sleep(0.1)          data_10pc = apply_filter(data_filtered, distance_filter)     time.sleep(0.1)    <pre>Overwriting initial_code_iteration1.py\n</pre> In\u00a0[14]: Copied! <pre>!mprof run --interval 0.005 --python python initial_code_iteration1.py\n!mprof plot --output initial_code_iteration1.png\n</pre> !mprof run --interval 0.005 --python python initial_code_iteration1.py !mprof plot --output initial_code_iteration1.png <pre>mprof: Sampling memory every 0.005s\nrunning new process\nrunning as a Python program...\ninitial_code_iteration1.py:25: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df[\"distance\"] = distance\nUsing last profile data.\n</pre> In\u00a0[15]: Copied! <pre>display(Image(\"./initial_code_iteration1.png\"))\n</pre> display(Image(\"./initial_code_iteration1.png\")) <p>This is an improvement, but we still see a spike at ~400 MiB when <code>apply_filter</code> is called for the first time. Why is this? Let's look at the function again:</p> <pre>def apply_filter(df, data_filter):\n    df = df[data_filter]\n    return df\n</pre> <p>When calling <code>df = df[data_filter]</code> we first make a copy of the data using <code>df[data_filter]</code> before assigning it to <code>df</code>. Let's look at a more memory efficient way to do this:</p> <pre>def apply_filter(df, data_filter):\n    # Assign NaN to all the RA of the filtered events\n    df[\"ra\"][data_filter] = np.nan\n    # Drop the entries with NaN, inplace = True meaning overwrite\n    df.dropna(inplace = True)\n</pre> <p>Since we've overwritten the original dataframe, we don't need to return anything...</p> In\u00a0[\u00a0]: Copied! <pre>%%writefile initial_code_iteration2.py\n</pre> %%writefile initial_code_iteration2.py  In\u00a0[\u00a0]: Copied! <pre>!mprof run --interval 0.005 --python python initial_code_iteration2.py\n!mprof plot --output initial_code_iteration2.png\n</pre> !mprof run --interval 0.005 --python python initial_code_iteration2.py !mprof plot --output initial_code_iteration2.png In\u00a0[\u00a0]: Copied! <pre>display(Image(\"./initial_code_iteration2.png\"))\n</pre> display(Image(\"./initial_code_iteration2.png\")) <p>This looks a lot better, but we still have that inital overhead of the dataset, is there anything we can do here?</p> <p>Let's look at the data.</p> In\u00a0[\u00a0]: Copied! <pre>!head gaia3.csv\n</pre> !head gaia3.csv <p>We have a lot of percision in our measurements. Lets compare the measured values to the uncertainty on these values. Let's limit ourself to the first 100 values. By default, <code>pandas</code> will use 64 bit percision when reading in floats. This might be overkill.</p> In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\ndf = pd.read_csv(\"gaia3.csv\", nrows=100)\ndf.head()\n</pre> import pandas as pd df = pd.read_csv(\"gaia3.csv\", nrows=100) df.head() <p>Do we need this percision? In this case it looks like the errors on our data are already pretty large.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\ndf_f32 = pd.read_csv(\"gaia3.csv\", nrows=100, dtype=np.float32)\ndf_f32.head()\n</pre> import numpy as np df_f32 = pd.read_csv(\"gaia3.csv\", nrows=100, dtype=np.float32) df_f32.head() In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nplt.hist(df[\"parallax\"] - df_f32[\"parallax\"])\n</pre> import matplotlib.pyplot as plt plt.hist(df[\"parallax\"] - df_f32[\"parallax\"]) In\u00a0[\u00a0]: Copied! <pre>df_f16 = pd.read_csv(\"gaia3.csv\", nrows=100, dtype=np.float16)\ndf_f16.head()\n</pre> df_f16 = pd.read_csv(\"gaia3.csv\", nrows=100, dtype=np.float16) df_f16.head() In\u00a0[\u00a0]: Copied! <pre>plt.hist(df[\"parallax\"] - df_f16[\"parallax\"])\n</pre> plt.hist(df[\"parallax\"] - df_f16[\"parallax\"]) <p>For a 32 bit float the uncertainy introduce is O($10^{-7}$), whereas for a 16 bit float the uncertainy is O($10^{-3}$).  Let's start off by looking at using 32 bit floats instead when creating the pandas dataframe:</p> <pre>def load_data():\n    df = pd.read_csv(\"./gaia3.csv\", dtype = np.float32)\n    return df\n</pre> In\u00a0[\u00a0]: Copied! <pre>%%writefile initial_code_iteration3.py\n</pre> %%writefile initial_code_iteration3.py  In\u00a0[\u00a0]: Copied! <pre>!mprof run --interval 0.005 --python python initial_code_iteration3.py\n!mprof plot --output initial_code_iteration3.png\n</pre> !mprof run --interval 0.005 --python python initial_code_iteration3.py !mprof plot --output initial_code_iteration3.png In\u00a0[\u00a0]: Copied! <pre>display(Image(\"./initial_code_iteration3.png\"))\n</pre> display(Image(\"./initial_code_iteration3.png\")) <p>We see a big improvement, with the peak memory usage &lt; 250 MiB. Since we're limited by the uncertinty in the measurements, we might decide to lower our precision again:</p> <pre>def load_data():\n    df = pd.read_csv(\"./gaia3.csv\", dtype = np.float16)\n    return df\n</pre> In\u00a0[\u00a0]: Copied! <pre>%%writefile initial_code_iteration4.py\n</pre> %%writefile initial_code_iteration4.py  In\u00a0[\u00a0]: Copied! <pre>!mprof run --interval 0.005 --python python initial_code_iteration4.py\n!mprof plot --output initial_code_iteration4.png\n</pre> !mprof run --interval 0.005 --python python initial_code_iteration4.py !mprof plot --output initial_code_iteration4.png In\u00a0[\u00a0]: Copied! <pre>display(Image(\"./initial_code_iteration4.png\"))\n</pre> display(Image(\"./initial_code_iteration4.png\")) <p>Before we start reading the the dataframe we already have ~75 MiB. Can we reduce this?</p> <p>NumPy is a pretty big library, let's only import what we need.</p> In\u00a0[\u00a0]: Copied! <pre>%%writefile initial_code_iteration5.py\n</pre> %%writefile initial_code_iteration5.py  In\u00a0[\u00a0]: Copied! <pre>!mprof run --interval 0.005 --python python initial_code_iteration5.py\n!mprof plot --output initial_code_iteration5.png\n</pre> !mprof run --interval 0.005 --python python initial_code_iteration5.py !mprof plot --output initial_code_iteration5.png In\u00a0[\u00a0]: Copied! <pre>display(Image(\"./initial_code_iteration5.png\"))\n</pre> display(Image(\"./initial_code_iteration5.png\")) <p>This brings us below 175 MiB. There are other steps we could look at:</p> <ul> <li>Running in batches to reduce the overall usage.</li> <li>Only loading in the columns needed for analysis</li> <li>Using a different package/backend?</li> </ul> In\u00a0[\u00a0]: Copied! <pre>def if_function(x):\n    if x &lt; 0:\n        return 0\n    else:\n        return np.sqrt(x)\n</pre> def if_function(x):     if x &lt; 0:         return 0     else:         return np.sqrt(x)  In\u00a0[\u00a0]: Copied! <pre>x = np.random.random(10**5)\n</pre> x = np.random.random(10**5) In\u00a0[\u00a0]: Copied! <pre>%timeit y = if_function(x)\n</pre> %timeit y = if_function(x) <p>We might be tempted to simply rewrite this function with a for loop. But remember, Python is terrible when looping...</p> In\u00a0[\u00a0]: Copied! <pre>def loop_function(x):\n    ret = np.zeros(len(x))\n    for i in range(len(x)):\n        if x[i] &lt; 0:\n            ret[i] =  0\n        else:\n            ret[i] = np.sqrt(x[i])\n    return ret\n</pre> def loop_function(x):     ret = np.zeros(len(x))     for i in range(len(x)):         if x[i] &lt; 0:             ret[i] =  0         else:             ret[i] = np.sqrt(x[i])     return ret  In\u00a0[\u00a0]: Copied! <pre>%timeit y = loop_function(x)\n</pre> %timeit y = loop_function(x) In\u00a0[\u00a0]: Copied! <pre>vec_function = np.vectorize(if_function)\n</pre> vec_function = np.vectorize(if_function) In\u00a0[\u00a0]: Copied! <pre>%timeit y = vec_function(x)\n</pre> %timeit y = vec_function(x) In\u00a0[\u00a0]: Copied! <pre>#first in first out cache wrapper\ndef fifo(func):\n    # Define the cache size \n    cache_size = 5\n    # Use a dictionary as the cache\n    cache = {}\n\n    def wrapper(*args, **kwargs):\n        # tokenize the arguments\n        token = \"\"\n        for arg in args:\n            token += str(arg)\n        for arg in kwargs:\n            token += str(arg)\n\n        # If still have the token\n        if token in cache:\n            return cache[token]\n\n        # Call function\n        result = func(*args, **kwargs)\n        cache[token] = result\n\n        # Check if we need to remove items from cache\n        if len(cache) &gt; 5:\n            cache[next(iter(cache))]\n            \n        # Return the results of the function\n        return result\n    return wrapper\n</pre> #first in first out cache wrapper def fifo(func):     # Define the cache size      cache_size = 5     # Use a dictionary as the cache     cache = {}      def wrapper(*args, **kwargs):         # tokenize the arguments         token = \"\"         for arg in args:             token += str(arg)         for arg in kwargs:             token += str(arg)          # If still have the token         if token in cache:             return cache[token]          # Call function         result = func(*args, **kwargs)         cache[token] = result          # Check if we need to remove items from cache         if len(cache) &gt; 5:             cache[next(iter(cache))]                      # Return the results of the function         return result     return wrapper In\u00a0[\u00a0]: Copied! <pre>def multiply_string(my_string, rept):\n    time.sleep(0.5)\n    return my_string * rept\n</pre> def multiply_string(my_string, rept):     time.sleep(0.5)     return my_string * rept   In\u00a0[\u00a0]: Copied! <pre>%time multiply_string(\"hello\", 5)\n</pre> %time multiply_string(\"hello\", 5)  In\u00a0[\u00a0]: Copied! <pre>%time multiply_string(\"hello\", 5)\n</pre> %time multiply_string(\"hello\", 5) In\u00a0[\u00a0]: Copied! <pre>@fifo\ndef multiply_string_cache(my_string, rept):\n    time.sleep(0.5)\n    return my_string * rept\n</pre> @fifo def multiply_string_cache(my_string, rept):     time.sleep(0.5)     return my_string * rept In\u00a0[\u00a0]: Copied! <pre>%time multiply_string_cache(\"goodbye\", 5)\n</pre> %time multiply_string_cache(\"goodbye\", 5)  In\u00a0[\u00a0]: Copied! <pre>%time multiply_string_cache(\"goodbye\", 5)\n</pre> %time multiply_string_cache(\"goodbye\", 5) <p>Now that we know how the cache works, we can use some blackboxed python functions.</p> <p>Let's use a Least recently used) cache. This will track the number of times we call a set of args. When the cache fills up, the least used item will be removed.</p> In\u00a0[\u00a0]: Copied! <pre>from functools import lru_cache\n\n@lru_cache(maxsize=5)\ndef multiply_string_lru(my_string, rept):\n    time.sleep(0.5)\n    return my_string * rept\n</pre> from functools import lru_cache  @lru_cache(maxsize=5) def multiply_string_lru(my_string, rept):     time.sleep(0.5)     return my_string * rept In\u00a0[\u00a0]: Copied! <pre>%time multiply_string_lru(\"goodbye\", 5)\n</pre> %time multiply_string_lru(\"goodbye\", 5)  In\u00a0[\u00a0]: Copied! <pre>%time multiply_string_lru(\"goodbye\", 5)\n</pre> %time multiply_string_lru(\"goodbye\", 5) In\u00a0[\u00a0]: Copied! <pre>def py_fib(x):\n    if x in [0,1] :\n        return 1\n    else:\n        return py_fib(x - 1) + py_fib(x - 2)\n</pre> def py_fib(x):     if x in [0,1] :         return 1     else:         return py_fib(x - 1) + py_fib(x - 2)  In\u00a0[\u00a0]: Copied! <pre>%timeit py_fib(10)\n</pre> %timeit py_fib(10) In\u00a0[\u00a0]: Copied! <pre>from numba import jit\n\n@jit( nopython = True )\ndef py_fib(x):\n    if x in [0,1] :\n        return 1\n    else:\n        return py_fib(x - 1) + py_fib(x - 2)\n</pre> from numba import jit  @jit( nopython = True ) def py_fib(x):     if x in [0,1] :         return 1     else:         return py_fib(x - 1) + py_fib(x - 2)  In\u00a0[\u00a0]: Copied! <pre>py_fib(10)\n</pre> py_fib(10) In\u00a0[\u00a0]: Copied! <pre>%timeit py_fib(10)\n</pre> %timeit py_fib(10) <p>We can use packages like Cython to write C++ code in Python. Or we can directly import a precompiled library.</p> <p>We can make C++ functions accessible in Python by wrapping our code in a <code>extern \"C\"</code> block.</p> <pre>extern \"C\" {\n\n    double my_function(... some arguments...){\n        ... some code ...\n    }\n}\n</pre> <p>Why <code>extern \"C\"</code>? One of the differences between C and C++ is that C++ allows us to overload functions. This means that two functions can have the same name, but accept different arguements. For example let say we have some functionality that we would like to make common across different data types:</p> <pre>double get_sqrt_integer( int a );\ndouble get_sqrt_double( double a );\n</pre> <p>The user shouldn't need to worry if the argument is an <code>int</code> or a <code>double</code>. So we would \"overload\" the function like so:</p> <pre>double get_sqrt( int a );\ndouble get_sqrt( double a );\n</pre> <p>So the user only ever calls <code>get_sqrt</code> and the compile handles the logic on whether it is a <code>double</code> or an <code>int</code>. The compile does this by giving each of the function versions unique names that are generated at compile time.</p> <p>By using <code>extern \"C\"</code> we are reverting back to the <code>C</code> method, which will explicitly asign the name we give the function, but it means that we can no longer overload the function. We therefore would need to run:</p> <pre>extern \"C\" {   \n    double get_sqrt_integer( int a );\n    double get_sqrt_double( double a );\n}\n</pre> <p>When analyzing the compiled library we would see the two functions called <code>get_sqrt_integer</code> and <code>get_sqrt_double</code>.</p> In\u00a0[\u00a0]: Copied! <pre>%%writefile cpp_fib.cpp\nextern \"C\" {\n    int cpp_fib(unsigned int x)\n    {\n        if ( (x == 0) || (x == 1) )\n        {\n            return 1;\n        }\n        else\n        {\n            return cpp_fib(x-1) + cpp_fib(x-2);\n        }\n    }\n}\n</pre> %%writefile cpp_fib.cpp extern \"C\" {     int cpp_fib(unsigned int x)     {         if ( (x == 0) || (x == 1) )         {             return 1;         }         else         {             return cpp_fib(x-1) + cpp_fib(x-2);         }     } } <p>Compile to a shared library</p> In\u00a0[\u00a0]: Copied! <pre># mamba / conda install gxx\n# or\n# mamba /conda install gxx-compiler\n</pre> # mamba / conda install gxx # or # mamba /conda install gxx-compiler In\u00a0[\u00a0]: Copied! <pre>! g++ -fPIC -shared -o cpp_fib.so cpp_fib.cpp -O3\n</pre> ! g++ -fPIC -shared -o cpp_fib.so cpp_fib.cpp -O3 In\u00a0[\u00a0]: Copied! <pre>! ls cpp_*\n</pre> ! ls cpp_* <p>Load in the library with <code>ctypes</code> and \"translate\" the argument types</p> In\u00a0[\u00a0]: Copied! <pre>import ctypes\n\ncpp_lib = ctypes.CDLL(\"./cpp_fib.so\")\n# Access the cpp_fib function from the library\n# and assigne the argument type\ncpp_lib.cpp_fib.argtypes = [ctypes.c_uint] \n# Alias it to cpp_fib\ncpp_fib = cpp_lib.cpp_fib\n</pre> import ctypes  cpp_lib = ctypes.CDLL(\"./cpp_fib.so\") # Access the cpp_fib function from the library # and assigne the argument type cpp_lib.cpp_fib.argtypes = [ctypes.c_uint]  # Alias it to cpp_fib cpp_fib = cpp_lib.cpp_fib  In\u00a0[\u00a0]: Copied! <pre>%timeit cpp_fib(10)\n</pre> %timeit cpp_fib(10) <p>We can also load in Rust code using rustimport</p> <pre><code>mamba/conda install rustimport\n</code></pre> <p>And the <code>pyo3</code> rust crate:</p> In\u00a0[\u00a0]: Copied! <pre>%load_ext rustimport\n</pre> %load_ext rustimport <p>use <code>--release</code> to complie an optimized binary rather than a debug binary.</p> In\u00a0[\u00a0]: Copied! <pre>%%rustimport  --release --force\nuse pyo3::prelude::*;\n\n#[pyfunction]\nfn rust_fib(x: usize) -&gt; usize {\n    match x {\n        1 =&gt; 1,\n        0 =&gt; 1,\n        _ =&gt; rust_fib(x - 1) + rust_fib(x - 2)\n    }\n}\n</pre> %%rustimport  --release --force use pyo3::prelude::*;  #[pyfunction] fn rust_fib(x: usize) -&gt; usize {     match x {         1 =&gt; 1,         0 =&gt; 1,         _ =&gt; rust_fib(x - 1) + rust_fib(x - 2)     } } In\u00a0[\u00a0]: Copied! <pre>%timeit rust_fib(10)\n</pre> %timeit rust_fib(10) In\u00a0[\u00a0]: Copied! <pre># fib_sqe = [py_fib(i) for i in range(10)]\n# fib_sqe\n</pre> # fib_sqe = [py_fib(i) for i in range(10)] # fib_sqe In\u00a0[\u00a0]: Copied! <pre># fib_sqe = [cpp_fib(i) for i in range(10)]\n# fib_sqe\n</pre> # fib_sqe = [cpp_fib(i) for i in range(10)] # fib_sqe In\u00a0[\u00a0]: Copied! <pre># fib_sqe = [rust_fib(i) for i in range(10)]\n# fib_sqe\n</pre> # fib_sqe = [rust_fib(i) for i in range(10)] # fib_sqe In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Python/BenchmarkingAndProfiling-Empty/#benchmarking-and-profiling","title":"Benchmarking and Profiling\u00b6","text":"<p>Benchmarking and Profiling are crucial techniques used to quantify the performance of code and measure improvements in its execution. In Python, understanding the performance of different implementations or comparing the efficiency of algorithms is essential, especially in scenarios where optimization is vital.</p> <p>When trying to optimize code, it is important to remember that the time required to optimize the code, might be better spent on other tasks. This is to say, when developing code the primary goal is to deliver working code. Delivering fast and efficent code is a secondary goal. With this in mind, it is important not to prematurely optimize code. One can spend an incredible ammount of time trying to optimize code before it even works! Futhermore, one could spend house of work to save only seconds of run time. A balance needs to be struck!</p> <p>A suitable development workflow would look something like this:</p> <ol> <li>Get a working instance of the code.</li> <li>Determine suitable benchmarks for evaluating the code (run time, iterations/second, iterations/cpu cores, percision on output, memory usage etc), and set target performance goals (e.g. total run time, total memory budget, etc).</li> <li>Evaluate the performance using the predefined benchmarks.</li> <li>If performance goals aren't met, profile the code to determine where the bottle necks are and whether changes can be made.</li> <li>Implement suitable changes to the code and repeat from step 3.</li> </ol> <p>Optimizing code can be a very time consuming process. There is a balance that needs to be made between acceptable run time and developer hours needed. Making mulitple changes to the code at once can also introduce bugs that might be difficult to debug. Either make small iterative changes to the code, or make sure the code passes pre-defined tests after each development cycle.</p> <p>Python provides the <code>timeit</code> module, which enables the measurement of execution time for specific code segments. This module offers a simple yet effective way to evaluate the performance of individual code snippets or functions:</p> <ul> <li><code>%%time</code> will time the contents of an entire cell with a single run</li> <li><code>%%timeit</code> will time the contents of an entire cell, averaging over multiples run of that cell</li> <li><code>%time</code> will time a signle line with a single run</li> <li><code>%timeit</code> will time a single line, averaging over multiples run of that cell</li> </ul> <p><code>timeit</code> will also suspend some features, such as the garbage collector, to get an accurate measurement of the code's runtime.</p>"},{"location":"Python/BenchmarkingAndProfiling-Empty/#line-profiling","title":"Line profiling\u00b6","text":"<p>Line profiling is a useful tool for finding bottlenecks within a block of code. There are many profilers one can use:</p> <ul> <li>cProfiles</li> <li>line_profiler</li> <li>todo get more</li> </ul> <p>For this example, we'll use line_profiler</p> <pre><code>mamba/conda install line_profiler\n</code></pre> <p>or</p> <pre><code>pip install line_profiler\n</code></pre>"},{"location":"Python/BenchmarkingAndProfiling-Empty/#can-we-do-better","title":"Can we do better?\u00b6","text":"<p>We're already using highly optimized NumPy which is based on C/C++ code. However we're still interpting slow bytecode at run time and running on one core.</p>"},{"location":"Python/BenchmarkingAndProfiling-Empty/#summary","title":"Summary\u00b6","text":"<p>We've looked at both how to speed up a simple function by first using pure NumPy, then moving to compiled versions, before moving to a parallel implementation:</p>"},{"location":"Python/BenchmarkingAndProfiling-Empty/#memory-profiling","title":"Memory Profiling\u00b6","text":"<p>So far we've only considered the run time performance. This is only one aspect of performance. Memory usage is often a limiting factor in performance.</p> <p>We can use the <code>memory_profile</code> to profile the memory of our code. This works by sampling the memory usage at intervals while the code is running.</p> <p>Similar to <code>%%timeit</code> and <code>%timeit</code> we can use <code>%%memit</code> and <code>%memit</code> to get the memory usage of a cell and a single line.</p>"},{"location":"Python/BenchmarkingAndProfiling-Empty/#summary","title":"Summary\u00b6","text":"<p>We're managed to reduce the memory usage from ~450 MiB to &lt; 175 MiB. We achieved this by:</p> <ul> <li>Profiling the memory usage to see what to target</li> <li>Avoiding copying the data</li> <li>Using inbuild functions to delete elements rather than doing by copy</li> <li>Changing the percision of the data we're using</li> <li>Reducing the import overhead</li> </ul> <p>There is still a lot of redundency in the above code, we could explore other options:</p> <ul> <li>Rewriting parts of the code in more memory efficient languages (Rust, C++).</li> <li>Reducing the import overhead by reducing the complexity of the code and doing more steps in pure python.</li> <li>Using more memory efficient libraries (e.g. Polars).</li> <li>Spliting the dataset into chunks and loading in smaller bits at a time.</li> </ul> <p>We also need to consider the development cost in finding low-memory solutions. Reducing the memory usage by 10% might be better than spending hours trying to reduce it by 20%.</p> <p>The methods we've used here also have trade offs:</p> <ul> <li>Going from f64 -&gt; f16 is a lossy form of compression. We have lost information that we cannot simply get back.</li> <li>We've thrown away data after it's read in and filtered. We can no longer compare to the original dataset without rewriting code.</li> <li>We've decreased the complexity, we can no longer access functions like <code>numpy.sum</code> without explicitly importing them first.</li> </ul> <p>When working with clusters we can use tools such as <code>memory_profiler</code> to estimate the requirements of the job we're about to submit. This can prevent our job crashing if we haven't requested enough memory.</p>"},{"location":"Python/BenchmarkingAndProfiling-Empty/#cheap-tricks-to-generally-speed-up-your-code","title":"Cheap Tricks to \"generally\" speed up your code\u00b6","text":""},{"location":"Python/BenchmarkingAndProfiling-Empty/#vectorization-is-your-friend","title":"Vectorization is your friend\u00b6","text":"<p>Vecorizing functions allow them to be applied to NumPy arrays, without having to write a for loop.</p>"},{"location":"Python/BenchmarkingAndProfiling-Empty/#caching-frequently-used-function-calls","title":"Caching frequently used function calls\u00b6","text":"<p>When you're expecting to repeat function calls with the same arguements, cache the results.</p>"},{"location":"Python/BenchmarkingAndProfiling-Empty/#already-have-working-code-from-a-more-efficient-language-import-it","title":"Already have working code from a more efficient language? Import it!\u00b6","text":"<p>Python has lots of tools to import code from different languages</p>"},{"location":"Python/BenchmarkingAndProfiling-Empty/#dont-reinvent-the-wheel","title":"Don't reinvent the wheel!\u00b6","text":"<p>Python is one of the most popular languages. A lot of super talented people have spent a lot of time optimizing python and wrapping low lower level languages in Python code. Use libraries such as:</p> <ul> <li>NumPy</li> <li>SciPy</li> <li>Pandas</li> </ul>"},{"location":"Python/BenchmarkingAndProfiling/","title":"Benchmarking and Profiling","text":"In\u00a0[29]: Copied! <pre>import time\nimport timeit\nimport numpy as np\nimport matplotlib.pyplot as plt\n</pre> import time import timeit import numpy as np import matplotlib.pyplot as plt In\u00a0[30]: Copied! <pre>%%timeit\ntest = 0\nfor i in range(100000):\n    test += 1\n</pre> %%timeit test = 0 for i in range(100000):     test += 1 <pre>2.48 ms \u00b1 31.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</pre> <p>We can also measure the time of the cell from one iteration using <code>%%time</code></p> In\u00a0[31]: Copied! <pre>%%time\ntest = 0\nfor i in range(100000):\n    test += 1\n</pre> %%time test = 0 for i in range(100000):     test += 1 <pre>CPU times: user 4.59 ms, sys: 0 ns, total: 4.59 ms\nWall time: 4.61 ms\n</pre> <p>We see a number of different times listed here:</p> <ul> <li><code>Wall time</code>: This is the time that has passed during the running of the code</li> <li><code>CPU time</code>: This is the time that the CPU was busy. This also includes time when the CPU has \"swapped\" its attention to a different process.</li> <li><code>user</code>: This is the time that the process has been running on the CPU but outside of the system kernel</li> <li><code>sys</code> : This is the time taken inside the system kernel. This could indicate that the code requires a lot of system calls rather than calls written by the developer.</li> </ul> <p>As a rule of thumb:</p> <ul> <li>CPU Time / Wall Time ~ 1. The process spent most of it's time using the CPU. A faster CPU could improve performance.</li> <li>CPU Time / Wall Time &lt; 1. This suggest the process has spent time waiting. This could be resource allocations, network/hardware IO, locks (to prevent data races) etc. The smaller the number, the more time spent waiting. 0.9 would suggest 10% of the time the CPU is idle.</li> <li>CPU Time / Wall Time &gt; 1. This suggests that we are utilizing multiple processors when running our code.</li> </ul> <p>If we are running with N processors:</p> <ul> <li>CPU Time / Wall Time ~ N. The process is well distributed across all CPUs with little to no idle time.</li> <li>CPU Time / Wall Time &lt; N. This suggest the process has spent time waiting. This could be resource allocations, network/hardware IO, locks (to prevent data races) etc.</li> </ul> <p>We can use <code>%timeit</code> to time an single line or process, averaging over multiple runs:</p> In\u00a0[32]: Copied! <pre>print (\"This line won't be timed\")\n# Either will this line\ntest = np.random.random(1000)\n# This line will be timed\n%timeit test = np.random.random(1000)\n</pre> print (\"This line won't be timed\") # Either will this line test = np.random.random(1000) # This line will be timed %timeit test = np.random.random(1000) <pre>This line won't be timed\n4.14 \u00b5s \u00b1 87.6 ns per loop (mean \u00b1 std. dev. of 7 runs, 100,000 loops each)\n</pre> <p>Or we can use <code>%time</code> to time an single line or process, with no averaging:</p> In\u00a0[33]: Copied! <pre>print (\"This line won't be timed\")\n# Either will this line\ntest = np.random.random(1000)\n# This line will be timed\n%time test = np.random.random(1000)\n</pre> print (\"This line won't be timed\") # Either will this line test = np.random.random(1000) # This line will be timed %time test = np.random.random(1000) <pre>This line won't be timed\nCPU times: user 100 \u00b5s, sys: 4 \u00b5s, total: 104 \u00b5s\nWall time: 78.7 \u00b5s\n</pre> <p>Let's look at comparing two functions. Consider vector addition. In a pure Python, we would use a for loop to iterate over each element. We could also use numpy's inbuild functions to perform the addition/</p> In\u00a0[34]: Copied! <pre>def add_two_arrays(a : np.ndarray,b : np.ndarray) -&gt; np.ndarray:\n    \"\"\"Add two numpy arrays togeter using a for loop\n\n    Args:\n        a : array 1 to be added\n        b : array 2 to be added\n\n    Returns:\n        array of a + b\n    \"\"\"\n    c = np.zeros(len(a))\n    for i in range(len(a)):\n        c[i] = a[i] + b[i]\n    return c\n\ndef add_two_arrays_numpy(a : np.ndarray, b : np.ndarray) -&gt; np.ndarray:\n    \"\"\"Add two numpy arrays togeter using vectorization\n\n    Args:\n        a : array 1 to be added\n        b : array 2 to be added\n\n    Returns:\n        array of a + b\n    \"\"\"\n    c = a + b\n    return c\n</pre> def add_two_arrays(a : np.ndarray,b : np.ndarray) -&gt; np.ndarray:     \"\"\"Add two numpy arrays togeter using a for loop      Args:         a : array 1 to be added         b : array 2 to be added      Returns:         array of a + b     \"\"\"     c = np.zeros(len(a))     for i in range(len(a)):         c[i] = a[i] + b[i]     return c  def add_two_arrays_numpy(a : np.ndarray, b : np.ndarray) -&gt; np.ndarray:     \"\"\"Add two numpy arrays togeter using vectorization      Args:         a : array 1 to be added         b : array 2 to be added      Returns:         array of a + b     \"\"\"     c = a + b     return c In\u00a0[35]: Copied! <pre>test_x = np.random.random(1000)\ntest_y = np.random.random(1000)\n\nprint(\"Pure Python:\")\n%timeit test_z = add_two_arrays(test_x, test_y)\nprint(\"Numpy:\")\n%timeit test_z = add_two_arrays_numpy(test_x, test_y)\n</pre> test_x = np.random.random(1000) test_y = np.random.random(1000)  print(\"Pure Python:\") %timeit test_z = add_two_arrays(test_x, test_y) print(\"Numpy:\") %timeit test_z = add_two_arrays_numpy(test_x, test_y)  <pre>Pure Python:\n155 \u00b5s \u00b1 742 ns per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)\nNumpy:\n</pre> <pre>\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[35], line 7\n      5 get_ipython().run_line_magic('timeit', 'test_z = add_two_arrays(test_x, test_y)')\n      6 print(\"Numpy:\")\n----&gt; 7 get_ipython().run_line_magic('timeit', 'test_z = add_two_arrays_numpy(test_x, test_y)')\n\nFile ~/miniforge3/envs/workshop/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2480, in InteractiveShell.run_line_magic(self, magic_name, line, _stack_depth)\n   2478     kwargs['local_ns'] = self.get_local_scope(stack_depth)\n   2479 with self.builtin_trap:\n-&gt; 2480     result = fn(*args, **kwargs)\n   2482 # The code below prevents the output from being displayed\n   2483 # when using magics with decorator @output_can_be_silenced\n   2484 # when the last Python token in the expression is a ';'.\n   2485 if getattr(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, False):\n\nFile ~/miniforge3/envs/workshop/lib/python3.10/site-packages/IPython/core/magics/execution.py:1189, in ExecutionMagics.timeit(self, line, cell, local_ns)\n   1186         if time_number &gt;= 0.2:\n   1187             break\n-&gt; 1189 all_runs = timer.repeat(repeat, number)\n   1190 best = min(all_runs) / number\n   1191 worst = max(all_runs) / number\n\nFile ~/miniforge3/envs/workshop/lib/python3.10/timeit.py:206, in Timer.repeat(self, repeat, number)\n    204 r = []\n    205 for i in range(repeat):\n--&gt; 206     t = self.timeit(number)\n    207     r.append(t)\n    208 return r\n\nFile ~/miniforge3/envs/workshop/lib/python3.10/site-packages/IPython/core/magics/execution.py:173, in Timer.timeit(self, number)\n    171 gc.disable()\n    172 try:\n--&gt; 173     timing = self.inner(it, self.timer)\n    174 finally:\n    175     if gcold:\n\nFile &lt;magic-timeit&gt;:1, in inner(_it, _timer)\n\nKeyboardInterrupt: </pre> <p>We see a dramatic improvement by using NumPy arrays to add two vectors together. This is because NumPy is a highly optimized library, with the low-level code written in C++. Is this always the case?</p> In\u00a0[\u00a0]: Copied! <pre>def time_addition(n):\n    \n    test_x = np.random.random(n)\n    test_y = np.random.random(n)\n    \n    n_repeat = 10\n    \n    # pure python\n    t_start = time.time()\n    for _ in range(n_repeat):\n        _ = add_two_arrays(test_x, test_y)\n    t_pure = np.median(time.time() - t_start)\n\n    # numpy\n    t_start = time.time()\n    for _ in range(n_repeat):\n        _ = add_two_arrays_numpy(test_x, test_y)\n    t_numpy = np.median(time.time() - t_start)\n\n    return t_pure, t_numpy\n</pre> def time_addition(n):          test_x = np.random.random(n)     test_y = np.random.random(n)          n_repeat = 10          # pure python     t_start = time.time()     for _ in range(n_repeat):         _ = add_two_arrays(test_x, test_y)     t_pure = np.median(time.time() - t_start)      # numpy     t_start = time.time()     for _ in range(n_repeat):         _ = add_two_arrays_numpy(test_x, test_y)     t_numpy = np.median(time.time() - t_start)      return t_pure, t_numpy In\u00a0[17]: Copied! <pre>n_array = np.arange(20)\ntimes_py = np.zeros(n_array.shape)\ntimes_np = np.zeros(n_array.shape)\n\nfor i, n in enumerate(n_array):\n    times_py[i], times_np[i] = time_addition(int(2**n))\n</pre> n_array = np.arange(20) times_py = np.zeros(n_array.shape) times_np = np.zeros(n_array.shape)  for i, n in enumerate(n_array):     times_py[i], times_np[i] = time_addition(int(2**n))   In\u00a0[18]: Copied! <pre>plt.plot(2 ** n_array, times_py, \"C0s:\", label = \"Pure Python\")\nplt.plot(2 ** n_array, times_np, \"C1o--\", label = \"Numpy\")\nplt.loglog()\nplt.grid()\nplt.legend()\nplt.xlabel(\"Array Length\")\nplt.ylabel(\"Run Time [s]\");\n</pre> plt.plot(2 ** n_array, times_py, \"C0s:\", label = \"Pure Python\") plt.plot(2 ** n_array, times_np, \"C1o--\", label = \"Numpy\") plt.loglog() plt.grid() plt.legend() plt.xlabel(\"Array Length\") plt.ylabel(\"Run Time [s]\");   <p>At very small array sizes the performance is comparable. This is likely due to the overhead of calling the low level NumPy code. The NumPy perfomance saturates around $10^2$ - $10^3$ before scaling in a comparatable fashion to pure python, however it is still orders of magnitude faster. This perfomance will be system dependent!</p> <p>It might be easy to assume for the above example that NumPy is always better than pure Python, but this isn't always the case and we should always consider our specific needs.</p> <p>If we where writing code to operate on small (O($10^1$)), we may be faster to use a pure python implementation rather than importing NumPy and introducing an additional overhead.</p> <p>Let's look at an expensive function that uses a MC method to estimate $\\pi$.</p> <p>We're going to throw random points $x, y \\in [0,1]$ into a square of length 1. This will give an estimate of the the area of the square:</p> <p>$$ A_{square} \\propto N_{total}$$ The area of a circle is given by: $$ A_{circle} = \\pi r^2$$ Since we're throwing random numbers in the range $[0,1]$, we will fill a quatre of a circle. $$ A_{circle} \\propto 4 * \\tilde{N_{within}}$$ Where $\\tilde{N_{within}}$ is the number of samples within 1 of the origin (or $x^2 + y^2 &lt;1$).</p> <p>Combing these, we can estimate $pi$ as: $$ \\pi \\approx 4 \\frac{N_{within}}{N_{total}}$$</p> In\u00a0[37]: Copied! <pre>def monte_carlo_pi(nsamples):\n    acc = 0\n    for i in range(nsamples):\n        x = np.random.random()\n        y = np.random.random()\n        if (x ** 2 + y ** 2) &lt; 1.0:\n            acc += 1\n    return 4.0 * acc / nsamples\n</pre> def monte_carlo_pi(nsamples):     acc = 0     for i in range(nsamples):         x = np.random.random()         y = np.random.random()         if (x ** 2 + y ** 2) &lt; 1.0:             acc += 1     return 4.0 * acc / nsamples In\u00a0[38]: Copied! <pre>n_range = np.arange(7)\n\npi_diff = [np.abs(np.pi - monte_carlo_pi(10**n)) for n in n_range]\n</pre> n_range = np.arange(7)  pi_diff = [np.abs(np.pi - monte_carlo_pi(10**n)) for n in n_range] In\u00a0[39]: Copied! <pre>plt.plot(10**n_range, pi_diff)\nplt.xlabel(\"Number of Random Samples\")\nplt.ylabel(\"$|\\pi_{NumPy} - \\pi_{MC}|$\")\nplt.grid()\nplt.xscale('log')\n#plt.yscale('log')\n</pre> plt.plot(10**n_range, pi_diff) plt.xlabel(\"Number of Random Samples\") plt.ylabel(\"$|\\pi_{NumPy} - \\pi_{MC}|$\") plt.grid() plt.xscale('log') #plt.yscale('log') <p>We can see that as $N_{total}$ increases we tend to the NumPy value of $\\pi$. We obtain deminising returns at around $10^4$ samples. Let's choose this as our benchmark point.</p> In\u00a0[40]: Copied! <pre>n_total = 10 ** 4\n</pre> n_total = 10 ** 4 <p>How long does this take to run?</p> In\u00a0[41]: Copied! <pre>%timeit monte_carlo_pi(n_total)\n</pre> %timeit monte_carlo_pi(n_total) <pre>6.14 ms \u00b1 51.1 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</pre> In\u00a0[42]: Copied! <pre>n_run = 100\nn_repeat = 10\nt_original = np.array(\n    timeit.repeat(f\"monte_carlo_pi({n_total})\", \"from __main__ import monte_carlo_pi\", repeat = n_repeat,  number = n_run)\n) / n_run\n</pre> n_run = 100 n_repeat = 10 t_original = np.array(     timeit.repeat(f\"monte_carlo_pi({n_total})\", \"from __main__ import monte_carlo_pi\", repeat = n_repeat,  number = n_run) ) / n_run In\u00a0[43]: Copied! <pre>print(f\"Original Runtime = {np.mean(t_original) : 0.2e} +/- {np.std(t_original) :0.2e} s\")\n</pre> print(f\"Original Runtime = {np.mean(t_original) : 0.2e} +/- {np.std(t_original) :0.2e} s\")  <pre>Original Runtime =  6.36e-03 +/- 5.86e-04 s\n</pre> <p>If we want to speed up this calculation, we should first find what is the major bottleneck.</p> In\u00a0[45]: Copied! <pre>%load_ext line_profiler\n</pre> %load_ext line_profiler <pre>The line_profiler extension is already loaded. To reload it, use:\n  %reload_ext line_profiler\n</pre> In\u00a0[46]: Copied! <pre>%lprun -f monte_carlo_pi monte_carlo_pi(n_total)\n</pre> %lprun -f monte_carlo_pi monte_carlo_pi(n_total) <pre>Timer unit: 1e-09 s\n\nTotal time: 0.0180272 s\nFile: /tmp/ipykernel_235029/1024659408.py\nFunction: monte_carlo_pi at line 1\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n     1                                           def monte_carlo_pi(nsamples):\n     2         1        852.0    852.0      0.0      acc = 0\n     3     10001    1441541.0    144.1      8.0      for i in range(nsamples):\n     4     10000    5764246.0    576.4     32.0          x = np.random.random()\n     5     10000    5585385.0    558.5     31.0          y = np.random.random()\n     6     10000    3567354.0    356.7     19.8          if (x ** 2 + y ** 2) &lt; 1.0:\n     7      7911    1666182.0    210.6      9.2              acc += 1\n     8         1       1623.0   1623.0      0.0      return 4.0 * acc / nsamples</pre> <p>The above shows a breakdown of where we're spending time. We have each line of the function broken down. We can see that the allocations of <code>acc</code> and the return of <code>4.0 * acc / nsamples</code> are expensive operations, but this only happens once per execution. Since these only happen once there may be some \"noise\" effecting these estimates.</p> <p><code>for i in range(nsamples):</code> takes 8% of our time. This is pretty significant. Python is also nutoriously bad at looping. If we can avoid looping in Python we should.</p> <p>We also see that <code>acc += 1</code> doesn't happen every iteration, and we spend ~20% of our time checking <code>if (x ** 2 + y ** 2) &lt; 1.0</code>. We can see that the dominate part of the time is spent generating random numbers (<code>x/y = np.random.random()</code>) which takes ~60% of the run time.</p> <p>If we were looking to speed up this function, we should work target the most expensive parts first, this would suggest targeting:</p> <pre><code>        x = np.random.random()\n        y = np.random.random()\n        if (x ** 2 + y ** 2) &lt; 1.0:\n</code></pre> <p>Which constitutes 80% or our run time. Since these are ran 10000 times, it might make more sense to somehow only run this once. But how...</p> In\u00a0[47]: Copied! <pre>def monte_carlo_pi_pure_numpy(nsamples):\n    # Create random arrays with required number of samples\n    # Use the highly optimized numpy functions instead of for loop\n    x = np.random.random(nsamples)\n    y = np.random.random(nsamples)\n    # Do the filtering and calculation in the one line\n    return 4.0 *np.sum((x ** 2 + y ** 2) &lt; 1.0) / nsamples\n</pre> def monte_carlo_pi_pure_numpy(nsamples):     # Create random arrays with required number of samples     # Use the highly optimized numpy functions instead of for loop     x = np.random.random(nsamples)     y = np.random.random(nsamples)     # Do the filtering and calculation in the one line     return 4.0 *np.sum((x ** 2 + y ** 2) &lt; 1.0) / nsamples In\u00a0[48]: Copied! <pre>%timeit monte_carlo_pi_pure_numpy(n_total)\n</pre> %timeit monte_carlo_pi_pure_numpy(n_total) <pre>90.5 \u00b5s \u00b1 3.92 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each)\n</pre> In\u00a0[31]: Copied! <pre>n_run = 1000\nn_repeat = 10\nt_optimized = np.array(\n    timeit.repeat(f\"monte_carlo_pi_pure_numpy({n_total})\", \"from __main__ import monte_carlo_pi_pure_numpy\", repeat = n_repeat,  number = n_run)\n) / n_run\nt_optimized\n</pre> n_run = 1000 n_repeat = 10 t_optimized = np.array(     timeit.repeat(f\"monte_carlo_pi_pure_numpy({n_total})\", \"from __main__ import monte_carlo_pi_pure_numpy\", repeat = n_repeat,  number = n_run) ) / n_run t_optimized Out[31]: <pre>array([1.30074857e-04, 1.22065353e-04, 8.42903380e-05, 8.41132850e-05,\n       8.37166480e-05, 8.39098730e-05, 8.36135550e-05, 8.34919960e-05,\n       8.35091390e-05, 8.30124940e-05])</pre> In\u00a0[32]: Copied! <pre>print(f\"Optimized Runtime = {np.mean(t_optimized) : 0.2e} +/- {np.std(t_optimized) :0.2e} s\")\n</pre> print(f\"Optimized Runtime = {np.mean(t_optimized) : 0.2e} +/- {np.std(t_optimized) :0.2e} s\")  <pre>Optimized Runtime =  9.22e-05 +/- 1.70e-05 s\n</pre> In\u00a0[52]: Copied! <pre>%lprun -f monte_carlo_pi_pure_numpy monte_carlo_pi_pure_numpy(n_total)\n</pre> %lprun -f monte_carlo_pi_pure_numpy monte_carlo_pi_pure_numpy(n_total) <pre>Timer unit: 1e-09 s\n\nTotal time: 0.000826387 s\nFile: /tmp/ipykernel_235029/764224049.py\nFunction: monte_carlo_pi_pure_numpy at line 1\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n     1                                           def monte_carlo_pi_pure_numpy(nsamples):\n     2                                               # Create random arrays with required number of samples\n     3                                               # Use the highly optimized numpy functions instead of for loop\n     4         1     184658.0 184658.0     22.3      x = np.random.random(nsamples)\n     5         1     180490.0 180490.0     21.8      y = np.random.random(nsamples)\n     6                                               # Do the filtering and calculation in the one line\n     7         1     461239.0 461239.0     55.8      return 4.0 *np.sum((x ** 2 + y ** 2) &lt; 1.0) / nsamples</pre> In\u00a0[37]: Copied! <pre>speedup =  np.mean(t_original) / np.mean(t_optimized)\nprint (f\"Optmization speeds up the code by a factor of {speedup:0.2f}\")\n</pre> speedup =  np.mean(t_original) / np.mean(t_optimized) print (f\"Optmization speeds up the code by a factor of {speedup:0.2f}\") <pre>Optmization speeds up the code by a factor of 65.55\n</pre> In\u00a0[41]: Copied! <pre># !pip install numba\n</pre> # !pip install numba In\u00a0[53]: Copied! <pre>from numba import jit\n</pre> from numba import jit In\u00a0[54]: Copied! <pre>@jit\ndef monte_carlo_pi_pure_numpy_jit(nsamples):\n    # Create random arrays with required number of samples\n    # Use the highly optimized numpy functions instead of for loop\n    x = np.random.random(nsamples)\n    y = np.random.random(nsamples)\n    # Do the filtering and calculation in the one line\n    return 4.0 *np.sum((x ** 2 + y ** 2) &lt; 1.0) / nsamples\n</pre> @jit def monte_carlo_pi_pure_numpy_jit(nsamples):     # Create random arrays with required number of samples     # Use the highly optimized numpy functions instead of for loop     x = np.random.random(nsamples)     y = np.random.random(nsamples)     # Do the filtering and calculation in the one line     return 4.0 *np.sum((x ** 2 + y ** 2) &lt; 1.0) / nsamples In\u00a0[55]: Copied! <pre>t1 = time.time()\nmonte_carlo_pi_pure_numpy_jit(n_total)\ntelap = time.time() - t1\nprint (f\"This took {telap}s\")\n</pre> t1 = time.time() monte_carlo_pi_pure_numpy_jit(n_total) telap = time.time() - t1 print (f\"This took {telap}s\") <pre>This took 0.818145751953125s\n</pre> In\u00a0[56]: Copied! <pre>t1 = time.time()\nmonte_carlo_pi_pure_numpy_jit(n_total)\ntelap = time.time() - t1\nprint (f\"This took {telap}s\")\n</pre> t1 = time.time() monte_carlo_pi_pure_numpy_jit(n_total) telap = time.time() - t1 print (f\"This took {telap}s\") <pre>This took 0.00010251998901367188s\n</pre> In\u00a0[57]: Copied! <pre>n_run = 1000\nn_repeat = 10\nt_jitted = np.array(\n    timeit.repeat(f\"monte_carlo_pi_pure_numpy_jit({n_total})\", \"from __main__ import monte_carlo_pi_pure_numpy_jit\", repeat = n_repeat,  number = n_run)\n) / n_run\n# t_jitted\n</pre> n_run = 1000 n_repeat = 10 t_jitted = np.array(     timeit.repeat(f\"monte_carlo_pi_pure_numpy_jit({n_total})\", \"from __main__ import monte_carlo_pi_pure_numpy_jit\", repeat = n_repeat,  number = n_run) ) / n_run # t_jitted In\u00a0[58]: Copied! <pre>print(f\"jitted Runtime = {np.mean(t_jitted) : 0.2e} +/- {np.std(t_jitted) :0.2e} s\")\n</pre> print(f\"jitted Runtime = {np.mean(t_jitted) : 0.2e} +/- {np.std(t_jitted) :0.2e} s\")  <pre>jitted Runtime =  6.72e-05 +/- 1.59e-05 s\n</pre> In\u00a0[60]: Copied! <pre>speedup =  np.mean(t_original) / np.mean(t_jitted)\nprint (f\"Optmization speeds up the code by a factor of {speedup:0.2f}\")\n</pre> speedup =  np.mean(t_original) / np.mean(t_jitted) print (f\"Optmization speeds up the code by a factor of {speedup:0.2f}\") <pre>Optmization speeds up the code by a factor of 94.69\n</pre> In\u00a0[61]: Copied! <pre>@jit( parallel = True )\ndef monte_carlo_pi_pure_numpy_jit_parallel(nsamples):\n    # Create random arrays with required number of samples\n    # Use the highly optimized numpy functions instead of for loop\n    x = np.random.random(nsamples)\n    y = np.random.random(nsamples)\n    # Do the filtering and calculation in the one line\n    return 4.0 *np.sum((x ** 2 + y ** 2) &lt; 1.0) / nsamples\n</pre> @jit( parallel = True ) def monte_carlo_pi_pure_numpy_jit_parallel(nsamples):     # Create random arrays with required number of samples     # Use the highly optimized numpy functions instead of for loop     x = np.random.random(nsamples)     y = np.random.random(nsamples)     # Do the filtering and calculation in the one line     return 4.0 *np.sum((x ** 2 + y ** 2) &lt; 1.0) / nsamples In\u00a0[62]: Copied! <pre>t1 = time.time()\nmonte_carlo_pi_pure_numpy_jit_parallel(n_total)\ntelap = time.time() - t1\nprint (f\"This took {telap}s\")\n</pre> t1 = time.time() monte_carlo_pi_pure_numpy_jit_parallel(n_total) telap = time.time() - t1 print (f\"This took {telap}s\") <pre>This took 0.7153468132019043s\n</pre> In\u00a0[63]: Copied! <pre>n_run = 10000\nn_repeat = 10\nt_jitted_parallel = np.array(\n    timeit.repeat(f\"monte_carlo_pi_pure_numpy_jit_parallel({n_total})\", \"from __main__ import monte_carlo_pi_pure_numpy_jit_parallel\", repeat = n_repeat,  number = n_run)\n) / n_run\nt_jitted_parallel\n</pre> n_run = 10000 n_repeat = 10 t_jitted_parallel = np.array(     timeit.repeat(f\"monte_carlo_pi_pure_numpy_jit_parallel({n_total})\", \"from __main__ import monte_carlo_pi_pure_numpy_jit_parallel\", repeat = n_repeat,  number = n_run) ) / n_run t_jitted_parallel Out[63]: <pre>array([3.53246284e-05, 1.69790315e-05, 2.01592582e-05, 2.66108533e-05,\n       6.16496429e-05, 2.76365722e-05, 3.48632386e-05, 1.14208586e-05,\n       1.32643553e-05, 1.14757513e-05])</pre> In\u00a0[64]: Copied! <pre>print(f\"jitted parallel Runtime = {np.mean(t_jitted_parallel) : 0.2e} +/- {np.std(t_jitted_parallel) :0.2e} s\")\n</pre> print(f\"jitted parallel Runtime = {np.mean(t_jitted_parallel) : 0.2e} +/- {np.std(t_jitted_parallel) :0.2e} s\")  <pre>jitted parallel Runtime =  2.59e-05 +/- 1.46e-05 s\n</pre> In\u00a0[65]: Copied! <pre>speedup =  np.mean(t_original) / np.mean(t_jitted_parallel)\nprint (f\"Optmization speeds up the code by a factor of {speedup:0.2f}\")\n</pre> speedup =  np.mean(t_original) / np.mean(t_jitted_parallel) print (f\"Optmization speeds up the code by a factor of {speedup:0.2f}\") <pre>Optmization speeds up the code by a factor of 245.19\n</pre> In\u00a0[66]: Copied! <pre>@jit( parallel = True, nopython = True)\ndef monte_carlo_pi_pure_numpy_jit_parallel_nopython(nsamples):\n    # Create random arrays with required number of samples\n    # Use the highly optimized numpy functions instead of for loop\n    x = np.random.random(nsamples)\n    y = np.random.random(nsamples)\n    # Do the filtering and calculation in the one line\n    return 4.0 *np.sum((x ** 2 + y ** 2) &lt; 1.0) / nsamples\n</pre> @jit( parallel = True, nopython = True) def monte_carlo_pi_pure_numpy_jit_parallel_nopython(nsamples):     # Create random arrays with required number of samples     # Use the highly optimized numpy functions instead of for loop     x = np.random.random(nsamples)     y = np.random.random(nsamples)     # Do the filtering and calculation in the one line     return 4.0 *np.sum((x ** 2 + y ** 2) &lt; 1.0) / nsamples In\u00a0[67]: Copied! <pre>t1 = time.time()\nmonte_carlo_pi_pure_numpy_jit_parallel_nopython(n_total)\ntelap = time.time() - t1\nprint (f\"This took {telap}s\")\n</pre> t1 = time.time() monte_carlo_pi_pure_numpy_jit_parallel_nopython(n_total) telap = time.time() - t1 print (f\"This took {telap}s\") <pre>This took 0.31411242485046387s\n</pre> In\u00a0[68]: Copied! <pre>n_run = 10000\nn_repeat = 10\nt_jitted_parallel_nopython = np.array(\n    timeit.repeat(f\"monte_carlo_pi_pure_numpy_jit_parallel_nopython({n_total})\", \"from __main__ import monte_carlo_pi_pure_numpy_jit_parallel_nopython\", repeat = n_repeat,  number = n_run)\n) / n_run\nt_jitted_parallel_nopython\n</pre> n_run = 10000 n_repeat = 10 t_jitted_parallel_nopython = np.array(     timeit.repeat(f\"monte_carlo_pi_pure_numpy_jit_parallel_nopython({n_total})\", \"from __main__ import monte_carlo_pi_pure_numpy_jit_parallel_nopython\", repeat = n_repeat,  number = n_run) ) / n_run t_jitted_parallel_nopython Out[68]: <pre>array([3.03248594e-05, 1.24157844e-05, 1.27079011e-05, 1.13436051e-05,\n       1.17766736e-05, 3.14462926e-05, 1.97302514e-05, 1.40485196e-05,\n       1.42128084e-05, 1.49775431e-05])</pre> In\u00a0[69]: Copied! <pre>print(f\"jitted parallel no python Runtime = {np.mean(t_jitted_parallel_nopython) : 0.2e} +/- {np.std(t_jitted_parallel_nopython) :0.2e} s\")\n</pre> print(f\"jitted parallel no python Runtime = {np.mean(t_jitted_parallel_nopython) : 0.2e} +/- {np.std(t_jitted_parallel_nopython) :0.2e} s\")  <pre>jitted parallel no python Runtime =  1.73e-05 +/- 7.16e-06 s\n</pre> In\u00a0[70]: Copied! <pre>speedup =  np.mean(t_original) / np.mean(t_jitted_parallel_nopython)\nprint (f\"Optmization speeds up the code by a factor of {speedup:0.2f}\")\n</pre> speedup =  np.mean(t_original) / np.mean(t_jitted_parallel_nopython) print (f\"Optmization speeds up the code by a factor of {speedup:0.2f}\") <pre>Optmization speeds up the code by a factor of 367.65\n</pre> In\u00a0[71]: Copied! <pre>from numba import prange\n@jit(nopython = True, parallel = True)\ndef monte_carlo_pi_revisit(nsamples):\n    acc = 0\n    for i in prange(nsamples):\n        x = np.random.random()\n        y = np.random.random()\n        if (x ** 2 + y ** 2) &lt; 1.0:\n            acc += 1\n    return 4.0 * acc / nsamples\n</pre> from numba import prange @jit(nopython = True, parallel = True) def monte_carlo_pi_revisit(nsamples):     acc = 0     for i in prange(nsamples):         x = np.random.random()         y = np.random.random()         if (x ** 2 + y ** 2) &lt; 1.0:             acc += 1     return 4.0 * acc / nsamples In\u00a0[72]: Copied! <pre>t1 = time.time()\nmonte_carlo_pi_revisit(n_total)\ntelap = time.time() - t1\nprint (f\"This took {telap}s\")\n</pre> t1 = time.time() monte_carlo_pi_revisit(n_total) telap = time.time() - t1 print (f\"This took {telap}s\") <pre>This took 0.2649538516998291s\n</pre> In\u00a0[73]: Copied! <pre>n_run = 10000\nn_repeat = 10\nt_jitted_revisit = np.array(\n    timeit.repeat(f\"monte_carlo_pi_revisit({n_total})\", \"from __main__ import monte_carlo_pi_revisit\", repeat = n_repeat,  number = n_run)\n) / n_run\nt_jitted_revisit\n</pre> n_run = 10000 n_repeat = 10 t_jitted_revisit = np.array(     timeit.repeat(f\"monte_carlo_pi_revisit({n_total})\", \"from __main__ import monte_carlo_pi_revisit\", repeat = n_repeat,  number = n_run) ) / n_run t_jitted_revisit Out[73]: <pre>array([1.48544039e-05, 1.88295904e-05, 1.98919663e-05, 1.77156133e-05,\n       1.22228075e-05, 1.14322552e-05, 1.59235416e-05, 1.14099232e-05,\n       4.08112965e-05, 4.06835035e-05])</pre> In\u00a0[74]: Copied! <pre>print(f\"jitted parallel no python Runtime = {np.mean(t_jitted_revisit) : 0.2e} +/- {np.std(t_jitted_revisit) :0.2e} s\")\n</pre> print(f\"jitted parallel no python Runtime = {np.mean(t_jitted_revisit) : 0.2e} +/- {np.std(t_jitted_revisit) :0.2e} s\")  <pre>jitted parallel no python Runtime =  2.04e-05 +/- 1.06e-05 s\n</pre> In\u00a0[75]: Copied! <pre>speedup =  np.mean(t_original) / np.mean(t_jitted_revisit)\nprint (f\"Optmization speeds up the code by a factor of {speedup:0.2f}\")\n</pre> speedup =  np.mean(t_original) / np.mean(t_jitted_revisit) print (f\"Optmization speeds up the code by a factor of {speedup:0.2f}\") <pre>Optmization speeds up the code by a factor of 312.10\n</pre> In\u00a0[77]: Copied! <pre># speedup =  np.mean(t_original) / np.mean(t_optimized)\n# print (f\"Pure NumPy speeds up the code by a factor of {speedup:0.2f}\")\n\nspeedup =  np.mean(t_original) / np.mean(t_jitted)\nprint (f\"Compiled NumPy speeds up the code by a factor of {speedup:0.2f}\")\n\nspeedup =  np.mean(t_original) / np.mean(t_jitted_parallel)\nprint (f\"Parallel annd compiled NumPy speeds up the code by a factor of {speedup:0.2f}\")\n\nspeedup =  np.mean(t_original) / np.mean(t_jitted_parallel_nopython)\nprint (f\"Parallel annd compiled NumPy with 'no python' speeds up the code by a factor of {speedup:0.2f}\")\n\nspeedup =  np.mean(t_original) / np.mean(t_jitted_revisit)\nprint (f\"Parallel and complied python speeds up the code by a factor of {speedup:0.2f}\")\n</pre> # speedup =  np.mean(t_original) / np.mean(t_optimized) # print (f\"Pure NumPy speeds up the code by a factor of {speedup:0.2f}\")  speedup =  np.mean(t_original) / np.mean(t_jitted) print (f\"Compiled NumPy speeds up the code by a factor of {speedup:0.2f}\")  speedup =  np.mean(t_original) / np.mean(t_jitted_parallel) print (f\"Parallel annd compiled NumPy speeds up the code by a factor of {speedup:0.2f}\")  speedup =  np.mean(t_original) / np.mean(t_jitted_parallel_nopython) print (f\"Parallel annd compiled NumPy with 'no python' speeds up the code by a factor of {speedup:0.2f}\")  speedup =  np.mean(t_original) / np.mean(t_jitted_revisit) print (f\"Parallel and complied python speeds up the code by a factor of {speedup:0.2f}\") <pre>Compiled NumPy speeds up the code by a factor of 94.69\nParallel annd compiled NumPy speeds up the code by a factor of 245.19\nParallel annd compiled NumPy with 'no python' speeds up the code by a factor of 367.65\nParallel and complied python speeds up the code by a factor of 312.10\n</pre> <p>The compiled and parallel implementations offer the best performance. If we don't have access to a multi-core system, we achieve x100 performace by simply compiling the function with numba and jit.</p> In\u00a0[171]: Copied! <pre># if you have issues with async switch to v0.61\n# pip install memory_profiler==0.61\n</pre> # if you have issues with async switch to v0.61 # pip install memory_profiler==0.61 In\u00a0[17]: Copied! <pre>%load_ext memory_profiler\n# to automatically reload changed files\n%load_ext autoreload\n%autoreload 2\n</pre> %load_ext memory_profiler # to automatically reload changed files %load_ext autoreload %autoreload 2 <pre>The memory_profiler extension is already loaded. To reload it, use:\n  %reload_ext memory_profiler\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</pre> In\u00a0[18]: Copied! <pre>%memit big_array = [i for i in range(10**7)]\n</pre> %memit big_array = [i for i in range(10**7)] <pre>peak memory: 780.84 MiB, increment: 324.75 MiB\n</pre> <p>If we want to try and find what processes are using a lot of memory, we can line profile a function.</p> In\u00a0[19]: Copied! <pre>%%writefile function_to_profile.py\nimport time\n\ndef expensive_function():\n    \n    x = [ 120 for i in range(100) ]\n    b = [ x * 100 for i in range(1000) ]\n    big_list = [ i for i in range(10**5) ]\n    another_big_list = [ i for i in range(10**5) ]\n    time.sleep(0.01)\n\n    x = [ 42 for i in range(1000) ]\n</pre> %%writefile function_to_profile.py import time  def expensive_function():          x = [ 120 for i in range(100) ]     b = [ x * 100 for i in range(1000) ]     big_list = [ i for i in range(10**5) ]     another_big_list = [ i for i in range(10**5) ]     time.sleep(0.01)      x = [ 42 for i in range(1000) ]  <pre>Overwriting function_to_profile.py\n</pre> In\u00a0[20]: Copied! <pre>from function_to_profile import expensive_function\n%mprun -f expensive_function expensive_function()\n</pre> from function_to_profile import expensive_function %mprun -f expensive_function expensive_function() <pre>\n</pre> <pre>Filename: /home/obriens/Documents/websites/mcgill_site/docs/Tutorials/Python/function_to_profile.py\n\nLine #    Mem usage    Increment  Occurrences   Line Contents\n=============================================================\n     3    455.5 MiB    455.5 MiB           1   def expensive_function():\n     4                                             \n     5    455.5 MiB      0.0 MiB         103       x = [ 120 for i in range(100) ]\n     6    531.0 MiB     75.5 MiB        1003       b = [ x * 100 for i in range(1000) ]\n     7    533.7 MiB      2.6 MiB      100003       big_list = [ i for i in range(10**5) ]\n     8    538.0 MiB      4.4 MiB      100003       another_big_list = [ i for i in range(10**5) ]\n     9    538.0 MiB      0.0 MiB           1       time.sleep(0.01)\n    10                                         \n    11    538.0 MiB      0.0 MiB        1003       x = [ 42 for i in range(1000) ]</pre> <p>This can often be difficult to parse and understand, in a jupyter notebook. I prefer to use <code>memory_profiler</code> via the command line interface (which we can still call from jupyter!).</p> In\u00a0[1]: Copied! <pre>%%writefile initial_code.py\n\nimport numpy as np\nimport pandas as pd\nimport time\n\n@profile\ndef load_data():\n    df = pd.read_csv(\"./gaia3.csv\")\n    return df\n\n@profile\ndef filter_data(df):\n    parallax = df[\"parallax\"]\n    parallax_err = df[\"parallax_over_error\"]\n    good_mask = parallax_err &gt; 20\n    return good_mask\n    \n@profile\ndef get_distance(df):\n    distance = 1./ (df[\"parallax\"] * 1e-3)\n    return distance\n\n@profile\ndef assign_distance(df, distance):\n    df[\"distance\"] = distance\n\n\n\n@profile\ndef filter_distance(df):\n    distance = df[\"distance\"]\n    good_mask = distance &lt; 10\n    return good_mask\n\n\n@profile\ndef apply_filter(df, data_filter):\n    data = df[data_filter]\n    return data\n\n\n\nif __name__ == '__main__':\n\n    data = load_data()\n    time.sleep(0.1)\n    \n    data_filter = filter_data(data)\n    time.sleep(0.1)\n    \n    data_filtered = apply_filter(data, data_filter)\n    time.sleep(0.1)\n    \n    distance = get_distance(data_filtered)\n    time.sleep(0.1)\n    \n    assign_distance(data_filtered, distance)\n    time.sleep(0.1)\n    \n    distance_filter = filter_distance(data_filtered)\n    time.sleep(0.1)\n    \n    data_10pc = apply_filter(data_filtered, distance_filter)\n    time.sleep(0.1)\n</pre> %%writefile initial_code.py  import numpy as np import pandas as pd import time  @profile def load_data():     df = pd.read_csv(\"./gaia3.csv\")     return df  @profile def filter_data(df):     parallax = df[\"parallax\"]     parallax_err = df[\"parallax_over_error\"]     good_mask = parallax_err &gt; 20     return good_mask      @profile def get_distance(df):     distance = 1./ (df[\"parallax\"] * 1e-3)     return distance  @profile def assign_distance(df, distance):     df[\"distance\"] = distance    @profile def filter_distance(df):     distance = df[\"distance\"]     good_mask = distance &lt; 10     return good_mask   @profile def apply_filter(df, data_filter):     data = df[data_filter]     return data    if __name__ == '__main__':      data = load_data()     time.sleep(0.1)          data_filter = filter_data(data)     time.sleep(0.1)          data_filtered = apply_filter(data, data_filter)     time.sleep(0.1)          distance = get_distance(data_filtered)     time.sleep(0.1)          assign_distance(data_filtered, distance)     time.sleep(0.1)          distance_filter = filter_distance(data_filtered)     time.sleep(0.1)          data_10pc = apply_filter(data_filtered, distance_filter)     time.sleep(0.1)    <pre>Overwriting initial_code.py\n</pre> In\u00a0[2]: Copied! <pre>!mprof run --interval 0.005 --python python initial_code.py\n!mprof plot --output initial_code_profile.png\n</pre> !mprof run --interval 0.005 --python python initial_code.py !mprof plot --output initial_code_profile.png <pre>mprof: Sampling memory every 0.005s\nrunning new process\nrunning as a Python program...\ninitial_code.py:25: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df[\"distance\"] = distance\nUsing last profile data.\n</pre> In\u00a0[3]: Copied! <pre>from IPython.display import Image, display\n</pre> from IPython.display import Image, display In\u00a0[4]: Copied! <pre>display(Image(\"./initial_code_profile.png\"))\n</pre> display(Image(\"./initial_code_profile.png\")) <p>From the above we can see a few things:</p> <ol> <li>The file we read in is pretty big. The dataframe takes up ~250 MiB.</li> <li>Filtering the data has a negligable effect on the memory usage.</li> <li>We see a huge spike when calling <code>apply_filter</code> for the first time.</li> <li>We see a second smaller spike when calling <code>get_distance</code>.</li> </ol> <p>A plot like this is useful when we're trying to decide on what resources we need to run this program. We could likely run this on most machines as ~400 MiB is attainable by most personal laptops. If we wanted to submit this job to a cluster, we'd want at least ~450 MiB to provide some breathing room.</p> <p>If we want to target areas to reduce memory usage, we should target the <code>apply_filter</code> and <code>get_distance</code> functions. Additionally we can look at ways to reduce the dataset size (we'll come back to this...).</p> <p>Let's look at <code>apply_filter</code>:</p> <pre>@profile\ndef apply_filter(df, data_filter):\n    data = df[data_filter]\n    return data\n...\n    data_filtered = apply_filter(data, data_filter)\n...\n    data_10pc = apply_filter(data_filtered, distance_filter)\n...\n</pre> <p>We are using the <code>apply_filter</code>. This function takes in a <code>pandas.dataframe</code> and a boolean array and returns a new <code>pandas.dataframe</code>. Let's start by overwriting the data in the function:</p> <pre>@profile\ndef apply_filter(df, data_filter):\n    df = df[data_filter]\n    return df\n</pre> <p>Additionally we'll overwrite all the instances of <code>data</code></p> In\u00a0[5]: Copied! <pre>%%writefile initial_code_iteration1.py\n\nimport numpy as np\nimport pandas as pd\nimport time\n\n@profile\ndef load_data():\n    df = pd.read_csv(\"./gaia3.csv\")\n    return df\n\n@profile\ndef filter_data(df):\n    parallax = df[\"parallax\"]\n    parallax_err = df[\"parallax_over_error\"]\n    good_mask = parallax_err &gt; 20\n    return good_mask\n    \n@profile\ndef get_distance(df):\n    distance = 1./ (df[\"parallax\"] * 1e-3)\n    return distance\n\n@profile\ndef assign_distance(df, distance):\n    df[\"distance\"] = distance\n\n\n\n@profile\ndef filter_distance(df):\n    distance = df[\"distance\"]\n    good_mask = distance &lt; 10\n    return good_mask\n\n\n@profile\ndef apply_filter(df, data_filter):\n    df = df[data_filter]\n    return df\n\n\n\nif __name__ == '__main__':\n\n    data = load_data()\n    time.sleep(0.1)\n    \n    data_filter = filter_data(data)\n    time.sleep(0.1)\n    \n    data = apply_filter(data, data_filter)\n    time.sleep(0.1)\n    \n    distance = get_distance(data)\n    time.sleep(0.1)\n    \n    assign_distance(data, distance)\n    time.sleep(0.1)\n    \n    distance_filter = filter_distance(data)\n    time.sleep(0.1)\n    \n    data = apply_filter(data, distance_filter)\n    time.sleep(0.1)\n</pre> %%writefile initial_code_iteration1.py  import numpy as np import pandas as pd import time  @profile def load_data():     df = pd.read_csv(\"./gaia3.csv\")     return df  @profile def filter_data(df):     parallax = df[\"parallax\"]     parallax_err = df[\"parallax_over_error\"]     good_mask = parallax_err &gt; 20     return good_mask      @profile def get_distance(df):     distance = 1./ (df[\"parallax\"] * 1e-3)     return distance  @profile def assign_distance(df, distance):     df[\"distance\"] = distance    @profile def filter_distance(df):     distance = df[\"distance\"]     good_mask = distance &lt; 10     return good_mask   @profile def apply_filter(df, data_filter):     df = df[data_filter]     return df    if __name__ == '__main__':      data = load_data()     time.sleep(0.1)          data_filter = filter_data(data)     time.sleep(0.1)          data = apply_filter(data, data_filter)     time.sleep(0.1)          distance = get_distance(data)     time.sleep(0.1)          assign_distance(data, distance)     time.sleep(0.1)          distance_filter = filter_distance(data)     time.sleep(0.1)          data = apply_filter(data, distance_filter)     time.sleep(0.1)    <pre>Overwriting initial_code_iteration1.py\n</pre> In\u00a0[6]: Copied! <pre>!mprof run --interval 0.005 --python python initial_code_iteration1.py\n!mprof plot --output initial_code_iteration1.png\n</pre> !mprof run --interval 0.005 --python python initial_code_iteration1.py !mprof plot --output initial_code_iteration1.png <pre>mprof: Sampling memory every 0.005s\nrunning new process\nrunning as a Python program...\nUsing last profile data.\n</pre> In\u00a0[7]: Copied! <pre>display(Image(\"./initial_code_iteration1.png\"))\n</pre> display(Image(\"./initial_code_iteration1.png\")) <p>This is an improvement, but we still see a spike at ~400 MiB when <code>apply_filter</code> is called for the first time. Why is this? Let's look at the function again:</p> <pre>def apply_filter(df, data_filter):\n    df = df[data_filter]\n    return df\n</pre> <p>When calling <code>df = df[data_filter]</code> we first make a copy of the data using <code>df[data_filter]</code> before assigning it to <code>df</code>. Let's look at a more memory efficient way to do this:</p> <pre>def apply_filter(df, data_filter):\n    # Assign NaN to all the RA of the filtered events\n    df[\"ra\"][data_filter] = np.nan\n    # Drop the entries with NaN, inplace = True meaning overwrite\n    df.dropna(inplace = True)\n</pre> <p>Since we've overwritten the original dataframe, we don't need to return anything...</p> In\u00a0[8]: Copied! <pre>%%writefile initial_code_iteration2.py\n\nimport numpy as np\nimport pandas as pd\nimport time\n\n@profile\ndef load_data():\n    df = pd.read_csv(\"./gaia3.csv\")\n    return df\n\n@profile\ndef filter_data(df):\n    parallax = df[\"parallax\"]\n    parallax_err = df[\"parallax_over_error\"]\n    good_mask = parallax_err &gt; 20\n    return good_mask\n    \n@profile\ndef get_distance(df):\n    distance = 1./ (df[\"parallax\"] * 1e-3)\n    return distance\n\n@profile\ndef assign_distance(df, distance):\n    df[\"distance\"] = distance\n\n\n\n@profile\ndef filter_distance(df):\n    distance = df[\"distance\"]\n    good_mask = distance &lt; 10\n    return good_mask\n\n\n@profile\ndef apply_filter(df, data_filter):\n    # Assign NaN to all the RA of the filtered events\n    df[\"ra\"][data_filter] = np.nan\n    # Drop the entries with NaN, inplace = True meaning overwrite\n    df.dropna(inplace = True)\n    return df\n\n\n\nif __name__ == '__main__':\n\n    data = load_data()\n    time.sleep(0.1)\n    \n    data_filter = filter_data(data)\n    time.sleep(0.1)\n    \n    apply_filter(data, data_filter)\n    time.sleep(0.1)\n    \n    distance = get_distance(data)\n    time.sleep(0.1)\n    \n    assign_distance(data, distance)\n    time.sleep(0.1)\n    \n    distance_filter = filter_distance(data)\n    time.sleep(0.1)\n    \n    apply_filter(data, distance_filter)\n    time.sleep(0.1)\n</pre> %%writefile initial_code_iteration2.py  import numpy as np import pandas as pd import time  @profile def load_data():     df = pd.read_csv(\"./gaia3.csv\")     return df  @profile def filter_data(df):     parallax = df[\"parallax\"]     parallax_err = df[\"parallax_over_error\"]     good_mask = parallax_err &gt; 20     return good_mask      @profile def get_distance(df):     distance = 1./ (df[\"parallax\"] * 1e-3)     return distance  @profile def assign_distance(df, distance):     df[\"distance\"] = distance    @profile def filter_distance(df):     distance = df[\"distance\"]     good_mask = distance &lt; 10     return good_mask   @profile def apply_filter(df, data_filter):     # Assign NaN to all the RA of the filtered events     df[\"ra\"][data_filter] = np.nan     # Drop the entries with NaN, inplace = True meaning overwrite     df.dropna(inplace = True)     return df    if __name__ == '__main__':      data = load_data()     time.sleep(0.1)          data_filter = filter_data(data)     time.sleep(0.1)          apply_filter(data, data_filter)     time.sleep(0.1)          distance = get_distance(data)     time.sleep(0.1)          assign_distance(data, distance)     time.sleep(0.1)          distance_filter = filter_distance(data)     time.sleep(0.1)          apply_filter(data, distance_filter)     time.sleep(0.1)    <pre>Overwriting initial_code_iteration2.py\n</pre> In\u00a0[9]: Copied! <pre>!mprof run --interval 0.005 --python python initial_code_iteration2.py\n!mprof plot --output initial_code_iteration2.png\n</pre> !mprof run --interval 0.005 --python python initial_code_iteration2.py !mprof plot --output initial_code_iteration2.png <pre>mprof: Sampling memory every 0.005s\nrunning new process\nrunning as a Python program...\ninitial_code_iteration2.py:39: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  df[\"ra\"][data_filter] = np.nan\ninitial_code_iteration2.py:39: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df[\"ra\"][data_filter] = np.nan\ninitial_code_iteration2.py:39: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  df[\"ra\"][data_filter] = np.nan\ninitial_code_iteration2.py:39: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df[\"ra\"][data_filter] = np.nan\nUsing last profile data.\n</pre> In\u00a0[10]: Copied! <pre>display(Image(\"./initial_code_iteration2.png\"))\n</pre> display(Image(\"./initial_code_iteration2.png\")) <p>This looks a lot better, but we still have that inital overhead of the dataset, is there anything we can do here?</p> <p>Let's look at the data.</p> In\u00a0[12]: Copied! <pre>!head gaia3.csv\n</pre> !head gaia3.csv <pre>,ra,dec,parallax,parallax_over_error,bp_rp,phot_g_mean_mag,pm,pmra,pmra_error,pmdec,pmdec_error\n0,251.49255411977842,-50.74953285089441,6.383755900057115,103.153015,0.79550457,10.54673,32.09387,31.380714480358343,0.05963018,-6.728077202639662,0.045103732\n1,251.58997660680922,-50.63863518231249,13.398257319108815,750.62354,2.1655512,13.470652,98.00884,-49.35844067323052,0.017024113,-84.67277009494555,0.012675992\n2,251.09313867055343,-51.007071500182086,6.106723881526058,448.8808,1.2691355,12.529914,9.986012,5.650983532208639,0.013577581,8.233275210204,0.009894044\n3,251.74646120955111,-51.20105760802903,7.481941724988636,87.46144,2.5889482,16.969418,147.46635,10.335826592962476,0.08570085,-147.10370008642158,0.05650899\n4,250.18815359198393,-51.2730504962615,8.606532814900987,163.01225,2.755577,16.33911,5.618203,-1.3052739918459513,0.06419676,5.464472928347609,0.04363508\n5,250.32171790439398,-51.39721173654795,5.948736623759927,103.88973,0.010123253,7.615107,31.96969,-16.618858274260234,0.06304326,-27.31070564371402,0.04115518\n6,250.63769352342914,-51.83570545712104,25.8932028451182,1238.9082,2.691968,13.436164,434.55444,-58.46042596499814,0.021226894,-430.60414864152074,0.015450544\n7,250.1119445586245,-51.26003101959269,6.36085302396653,131.53783,2.53827,16.118967,77.69121,-52.38320393098749,0.049328525,-57.37528741330969,0.037503965\n8,250.18507947208107,-51.477673785228106,19.588654431566606,768.66254,0.44006157,6.22464,131.83145,11.454067954459212,0.028156618,131.33292721367764,0.019740378\n</pre> <p>We have a lot of percision in our measurements. Lets compare the measured values to the uncertainty on these values. Let's limit ourself to the first 100 values. By default, <code>pandas</code> will use 64 bit percision when reading in floats. This might be overkill.</p> In\u00a0[13]: Copied! <pre>import pandas as pd\ndf = pd.read_csv(\"gaia3.csv\", nrows=100)\ndf.head()\n</pre> import pandas as pd df = pd.read_csv(\"gaia3.csv\", nrows=100) df.head() Out[13]: Unnamed: 0 ra dec parallax parallax_over_error bp_rp phot_g_mean_mag pm pmra pmra_error pmdec pmdec_error 0 0 251.492554 -50.749533 6.383756 103.153015 0.795505 10.546730 32.093870 31.380714 0.059630 -6.728077 0.045104 1 1 251.589977 -50.638635 13.398257 750.623540 2.165551 13.470652 98.008840 -49.358441 0.017024 -84.672770 0.012676 2 2 251.093139 -51.007072 6.106724 448.880800 1.269135 12.529914 9.986012 5.650984 0.013578 8.233275 0.009894 3 3 251.746461 -51.201058 7.481942 87.461440 2.588948 16.969418 147.466350 10.335827 0.085701 -147.103700 0.056509 4 4 250.188154 -51.273050 8.606533 163.012250 2.755577 16.339110 5.618203 -1.305274 0.064197 5.464473 0.043635 <p>Do we need this percision? In this case it looks like the errors on our data are already pretty large.</p> In\u00a0[15]: Copied! <pre>import numpy as np\ndf_f32 = pd.read_csv(\"gaia3.csv\", nrows=100, dtype=np.float32)\ndf_f32.head()\n</pre> import numpy as np df_f32 = pd.read_csv(\"gaia3.csv\", nrows=100, dtype=np.float32) df_f32.head() Out[15]: Unnamed: 0 ra dec parallax parallax_over_error bp_rp phot_g_mean_mag pm pmra pmra_error pmdec pmdec_error 0 0.0 251.492554 -50.749535 6.383756 103.153015 0.795505 10.546730 32.093868 31.380714 0.059630 -6.728077 0.045104 1 1.0 251.589981 -50.638634 13.398257 750.623535 2.165551 13.470652 98.008842 -49.358440 0.017024 -84.672768 0.012676 2 2.0 251.093140 -51.007072 6.106724 448.880798 1.269135 12.529914 9.986012 5.650983 0.013578 8.233275 0.009894 3 3.0 251.746460 -51.201057 7.481942 87.461441 2.588948 16.969418 147.466354 10.335827 0.085701 -147.103699 0.056509 4 4.0 250.188156 -51.273052 8.606533 163.012253 2.755577 16.339109 5.618203 -1.305274 0.064197 5.464473 0.043635 In\u00a0[16]: Copied! <pre>import matplotlib.pyplot as plt\nplt.hist(df[\"parallax\"] - df_f32[\"parallax\"])\n</pre> import matplotlib.pyplot as plt plt.hist(df[\"parallax\"] - df_f32[\"parallax\"]) Out[16]: <pre>(array([ 1.,  0.,  1., 15., 32., 32., 15.,  2.,  1.,  1.]),\n array([-8.90233363e-07, -7.14498678e-07, -5.38763994e-07, -3.63029309e-07,\n        -1.87294625e-07, -1.15599406e-08,  1.64174744e-07,  3.39909428e-07,\n         5.15644113e-07,  6.91378797e-07,  8.67113481e-07]),\n &lt;BarContainer object of 10 artists&gt;)</pre> In\u00a0[17]: Copied! <pre>df_f16 = pd.read_csv(\"gaia3.csv\", nrows=100, dtype=np.float16)\ndf_f16.head()\n</pre> df_f16 = pd.read_csv(\"gaia3.csv\", nrows=100, dtype=np.float16) df_f16.head() Out[17]: Unnamed: 0 ra dec parallax parallax_over_error bp_rp phot_g_mean_mag pm pmra pmra_error pmdec pmdec_error 0 0.0 251.500 -50.75000 6.382812 103.1250 0.795410 10.546875 32.093750 31.375000 0.059631 -6.726562 0.045105 1 1.0 251.625 -50.62500 13.398438 750.5000 2.166016 13.468750 98.000000 -49.343750 0.017029 -84.687500 0.012672 2 2.0 251.125 -51.00000 6.105469 449.0000 1.269531 12.531250 9.984375 5.652344 0.013580 8.234375 0.009895 3 3.0 251.750 -51.18750 7.480469 87.4375 2.589844 16.968750 147.500000 10.335938 0.085693 -147.125000 0.056519 4 4.0 250.250 -51.28125 8.609375 163.0000 2.755859 16.343750 5.617188 -1.305664 0.064209 5.464844 0.043640 In\u00a0[18]: Copied! <pre>plt.hist(df[\"parallax\"] - df_f16[\"parallax\"])\n</pre> plt.hist(df[\"parallax\"] - df_f16[\"parallax\"]) Out[18]: <pre>(array([ 2.,  0.,  8., 13., 36., 23., 12.,  5.,  0.,  1.]),\n array([-0.00563744, -0.00440876, -0.00318007, -0.00195138, -0.0007227 ,\n         0.00050599,  0.00173468,  0.00296337,  0.00419205,  0.00542074,\n         0.00664943]),\n &lt;BarContainer object of 10 artists&gt;)</pre> <p>For a 32 bit float the uncertainy introduce is O($10^{-7}$), whereas for a 16 bit float the uncertainy is O($10^{-3}$).  Let's start off by looking at using 32 bit floats instead when creating the pandas dataframe:</p> <pre>def load_data():\n    df = pd.read_csv(\"./gaia3.csv\", dtype = np.float32)\n    return df\n</pre> In\u00a0[19]: Copied! <pre>%%writefile initial_code_iteration3.py\n\nimport numpy as np\nimport pandas as pd\nimport time\n\n@profile\ndef load_data():\n    df = pd.read_csv(\"./gaia3.csv\", dtype = np.float32)\n    return df\n\n@profile\ndef filter_data(df):\n    parallax = df[\"parallax\"]\n    parallax_err = df[\"parallax_over_error\"]\n    good_mask = parallax_err &gt; 20\n    return good_mask\n    \n@profile\ndef get_distance(df):\n    distance = 1./ (df[\"parallax\"] * 1e-3)\n    return distance\n\n@profile\ndef assign_distance(df, distance):\n    df[\"distance\"] = distance\n\n\n\n@profile\ndef filter_distance(df):\n    distance = df[\"distance\"]\n    good_mask = distance &lt; 10\n    return good_mask\n\n\n@profile\ndef apply_filter(df, data_filter):\n    # Assign NaN to all the RA of the filtered events\n    df[\"ra\"][data_filter] = np.nan\n    # Drop the entries with NaN, inplace = True meaning overwrite\n    df.dropna(inplace = True)\n    return df\n\n\n\nif __name__ == '__main__':\n\n    data = load_data()\n    time.sleep(0.1)\n    \n    data_filter = filter_data(data)\n    time.sleep(0.1)\n    \n    apply_filter(data, data_filter)\n    time.sleep(0.1)\n    \n    distance = get_distance(data)\n    time.sleep(0.1)\n    \n    assign_distance(data, distance)\n    time.sleep(0.1)\n    \n    distance_filter = filter_distance(data)\n    time.sleep(0.1)\n    \n    apply_filter(data, distance_filter)\n    time.sleep(0.1)\n</pre> %%writefile initial_code_iteration3.py  import numpy as np import pandas as pd import time  @profile def load_data():     df = pd.read_csv(\"./gaia3.csv\", dtype = np.float32)     return df  @profile def filter_data(df):     parallax = df[\"parallax\"]     parallax_err = df[\"parallax_over_error\"]     good_mask = parallax_err &gt; 20     return good_mask      @profile def get_distance(df):     distance = 1./ (df[\"parallax\"] * 1e-3)     return distance  @profile def assign_distance(df, distance):     df[\"distance\"] = distance    @profile def filter_distance(df):     distance = df[\"distance\"]     good_mask = distance &lt; 10     return good_mask   @profile def apply_filter(df, data_filter):     # Assign NaN to all the RA of the filtered events     df[\"ra\"][data_filter] = np.nan     # Drop the entries with NaN, inplace = True meaning overwrite     df.dropna(inplace = True)     return df    if __name__ == '__main__':      data = load_data()     time.sleep(0.1)          data_filter = filter_data(data)     time.sleep(0.1)          apply_filter(data, data_filter)     time.sleep(0.1)          distance = get_distance(data)     time.sleep(0.1)          assign_distance(data, distance)     time.sleep(0.1)          distance_filter = filter_distance(data)     time.sleep(0.1)          apply_filter(data, distance_filter)     time.sleep(0.1)    <pre>Overwriting initial_code_iteration3.py\n</pre> In\u00a0[20]: Copied! <pre>!mprof run --interval 0.005 --python python initial_code_iteration3.py\n!mprof plot --output initial_code_iteration3.png\n</pre> !mprof run --interval 0.005 --python python initial_code_iteration3.py !mprof plot --output initial_code_iteration3.png <pre>mprof: Sampling memory every 0.005s\nrunning new process\nrunning as a Python program...\ninitial_code_iteration3.py:39: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  df[\"ra\"][data_filter] = np.nan\ninitial_code_iteration3.py:39: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  df[\"ra\"][data_filter] = np.nan\nUsing last profile data.\n</pre> In\u00a0[22]: Copied! <pre>display(Image(\"./initial_code_iteration3.png\"))\n</pre> display(Image(\"./initial_code_iteration3.png\")) <p>We see a big improvement, with the peak memory usage &lt; 250 MiB. Since we're limited by the uncertinty in the measurements, we might decide to lower our precision again:</p> <pre>def load_data():\n    df = pd.read_csv(\"./gaia3.csv\", dtype = np.float16)\n    return df\n</pre> In\u00a0[23]: Copied! <pre>%%writefile initial_code_iteration4.py\n\nimport numpy as np\nimport pandas as pd\nimport time\n\n@profile\ndef load_data():\n    df = pd.read_csv(\"./gaia3.csv\", dtype = np.float16)\n    return df\n\n@profile\ndef filter_data(df):\n    parallax = df[\"parallax\"]\n    parallax_err = df[\"parallax_over_error\"]\n    good_mask = parallax_err &gt; 20\n    return good_mask\n    \n@profile\ndef get_distance(df):\n    distance = 1./ (df[\"parallax\"] * 1e-3)\n    return distance\n\n@profile\ndef assign_distance(df, distance):\n    df[\"distance\"] = distance\n\n\n\n@profile\ndef filter_distance(df):\n    distance = df[\"distance\"]\n    good_mask = distance &lt; 10\n    return good_mask\n\n\n@profile\ndef apply_filter(df, data_filter):\n    # Assign NaN to all the RA of the filtered events\n    df[\"ra\"][data_filter] = np.nan\n    # Drop the entries with NaN, inplace = True meaning overwrite\n    df.dropna(inplace = True)\n    return df\n\n\n\nif __name__ == '__main__':\n\n    data = load_data()\n    time.sleep(0.1)\n    \n    data_filter = filter_data(data)\n    time.sleep(0.1)\n    \n    apply_filter(data, data_filter)\n    time.sleep(0.1)\n    \n    distance = get_distance(data)\n    time.sleep(0.1)\n    \n    assign_distance(data, distance)\n    time.sleep(0.1)\n    \n    distance_filter = filter_distance(data)\n    time.sleep(0.1)\n    \n    apply_filter(data, distance_filter)\n    time.sleep(0.1)\n</pre> %%writefile initial_code_iteration4.py  import numpy as np import pandas as pd import time  @profile def load_data():     df = pd.read_csv(\"./gaia3.csv\", dtype = np.float16)     return df  @profile def filter_data(df):     parallax = df[\"parallax\"]     parallax_err = df[\"parallax_over_error\"]     good_mask = parallax_err &gt; 20     return good_mask      @profile def get_distance(df):     distance = 1./ (df[\"parallax\"] * 1e-3)     return distance  @profile def assign_distance(df, distance):     df[\"distance\"] = distance    @profile def filter_distance(df):     distance = df[\"distance\"]     good_mask = distance &lt; 10     return good_mask   @profile def apply_filter(df, data_filter):     # Assign NaN to all the RA of the filtered events     df[\"ra\"][data_filter] = np.nan     # Drop the entries with NaN, inplace = True meaning overwrite     df.dropna(inplace = True)     return df    if __name__ == '__main__':      data = load_data()     time.sleep(0.1)          data_filter = filter_data(data)     time.sleep(0.1)          apply_filter(data, data_filter)     time.sleep(0.1)          distance = get_distance(data)     time.sleep(0.1)          assign_distance(data, distance)     time.sleep(0.1)          distance_filter = filter_distance(data)     time.sleep(0.1)          apply_filter(data, distance_filter)     time.sleep(0.1)    <pre>Overwriting initial_code_iteration4.py\n</pre> In\u00a0[24]: Copied! <pre>!mprof run --interval 0.005 --python python initial_code_iteration4.py\n!mprof plot --output initial_code_iteration4.png\n</pre> !mprof run --interval 0.005 --python python initial_code_iteration4.py !mprof plot --output initial_code_iteration4.png <pre>mprof: Sampling memory every 0.005s\nrunning new process\nrunning as a Python program...\n/home/obriens/miniforge3/envs/workshop/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:234: RuntimeWarning: overflow encountered in cast\n  chunks = self._reader.read_low_memory(nrows)\ninitial_code_iteration4.py:39: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  df[\"ra\"][data_filter] = np.nan\ninitial_code_iteration4.py:39: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  df[\"ra\"][data_filter] = np.nan\nUsing last profile data.\n</pre> In\u00a0[25]: Copied! <pre>display(Image(\"./initial_code_iteration4.png\"))\n</pre> display(Image(\"./initial_code_iteration4.png\")) <p>Before we start reading the the dataframe we already have ~75 MiB. Can we reduce this?</p> <p>NumPy is a pretty big library, let's only import what we need.</p> In\u00a0[26]: Copied! <pre>%%writefile initial_code_iteration5.py\n\nfrom numpy import float16, nan\nimport pandas as pd\nfrom time import sleep\n\n@profile\ndef load_data():\n    df = pd.read_csv(\"./gaia3.csv\", dtype = float16)\n    return df\n\n@profile\ndef filter_data(df):\n    parallax = df[\"parallax\"]\n    parallax_err = df[\"parallax_over_error\"]\n    good_mask = parallax_err &gt; 20\n    return good_mask\n    \n@profile\ndef get_distance(df):\n    distance = 1./ (df[\"parallax\"] * 1e-3)\n    return distance\n\n@profile\ndef assign_distance(df, distance):\n    df[\"distance\"] = distance\n\n\n\n@profile\ndef filter_distance(df):\n    distance = df[\"distance\"]\n    good_mask = distance &lt; 10\n    return good_mask\n\n\n@profile\ndef apply_filter(df, data_filter):\n    # Assign NaN to all the RA of the filtered events\n    df[\"ra\"][data_filter] = nan\n    # Drop the entries with NaN, inplace = True meaning overwrite\n    df.dropna(inplace = True)\n    return df\n\n\n\nif __name__ == '__main__':\n\n    data = load_data()\n    sleep(0.1)\n    \n    data_filter = filter_data(data)\n    sleep(0.1)\n    \n    apply_filter(data, data_filter)\n    sleep(0.1)\n    \n    distance = get_distance(data)\n    sleep(0.1)\n    \n    assign_distance(data, distance)\n    sleep(0.1)\n    \n    distance_filter = filter_distance(data)\n    sleep(0.1)\n    \n    apply_filter(data, distance_filter)\n    sleep(0.1)\n</pre> %%writefile initial_code_iteration5.py  from numpy import float16, nan import pandas as pd from time import sleep  @profile def load_data():     df = pd.read_csv(\"./gaia3.csv\", dtype = float16)     return df  @profile def filter_data(df):     parallax = df[\"parallax\"]     parallax_err = df[\"parallax_over_error\"]     good_mask = parallax_err &gt; 20     return good_mask      @profile def get_distance(df):     distance = 1./ (df[\"parallax\"] * 1e-3)     return distance  @profile def assign_distance(df, distance):     df[\"distance\"] = distance    @profile def filter_distance(df):     distance = df[\"distance\"]     good_mask = distance &lt; 10     return good_mask   @profile def apply_filter(df, data_filter):     # Assign NaN to all the RA of the filtered events     df[\"ra\"][data_filter] = nan     # Drop the entries with NaN, inplace = True meaning overwrite     df.dropna(inplace = True)     return df    if __name__ == '__main__':      data = load_data()     sleep(0.1)          data_filter = filter_data(data)     sleep(0.1)          apply_filter(data, data_filter)     sleep(0.1)          distance = get_distance(data)     sleep(0.1)          assign_distance(data, distance)     sleep(0.1)          distance_filter = filter_distance(data)     sleep(0.1)          apply_filter(data, distance_filter)     sleep(0.1)    <pre>Overwriting initial_code_iteration5.py\n</pre> In\u00a0[27]: Copied! <pre>!mprof run --interval 0.005 --python python initial_code_iteration5.py\n!mprof plot --output initial_code_iteration5.png\n</pre> !mprof run --interval 0.005 --python python initial_code_iteration5.py !mprof plot --output initial_code_iteration5.png <pre>mprof: Sampling memory every 0.005s\nrunning new process\nrunning as a Python program...\n/home/obriens/miniforge3/envs/workshop/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:234: RuntimeWarning: overflow encountered in cast\n  chunks = self._reader.read_low_memory(nrows)\ninitial_code_iteration5.py:39: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  df[\"ra\"][data_filter] = nan\ninitial_code_iteration5.py:39: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  df[\"ra\"][data_filter] = nan\nUsing last profile data.\n</pre> In\u00a0[28]: Copied! <pre>display(Image(\"./initial_code_iteration5.png\"))\n</pre> display(Image(\"./initial_code_iteration5.png\")) <p>This brings us below 175 MiB. There are other steps we could look at:</p> <ul> <li>Running in batches to reduce the overall usage.</li> <li>Only loading in the columns needed for analysis</li> <li>Using a different package/backend?</li> </ul> In\u00a0[109]: Copied! <pre>def if_function(x):\n    if x &lt; 0:\n        return 0\n    else:\n        return np.sqrt(x)\n</pre> def if_function(x):     if x &lt; 0:         return 0     else:         return np.sqrt(x)  In\u00a0[110]: Copied! <pre>x = np.random.random(10**5)\n</pre> x = np.random.random(10**5) In\u00a0[111]: Copied! <pre>%timeit y = if_function(x)\n</pre> %timeit y = if_function(x) <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[111], line 1\n----&gt; 1 get_ipython().run_line_magic('timeit', 'y = if_function(x)')\n\nFile ~/miniforge3/envs/workshop/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2480, in InteractiveShell.run_line_magic(self, magic_name, line, _stack_depth)\n   2478     kwargs['local_ns'] = self.get_local_scope(stack_depth)\n   2479 with self.builtin_trap:\n-&gt; 2480     result = fn(*args, **kwargs)\n   2482 # The code below prevents the output from being displayed\n   2483 # when using magics with decorator @output_can_be_silenced\n   2484 # when the last Python token in the expression is a ';'.\n   2485 if getattr(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, False):\n\nFile ~/miniforge3/envs/workshop/lib/python3.10/site-packages/IPython/core/magics/execution.py:1185, in ExecutionMagics.timeit(self, line, cell, local_ns)\n   1183 for index in range(0, 10):\n   1184     number = 10 ** index\n-&gt; 1185     time_number = timer.timeit(number)\n   1186     if time_number &gt;= 0.2:\n   1187         break\n\nFile ~/miniforge3/envs/workshop/lib/python3.10/site-packages/IPython/core/magics/execution.py:173, in Timer.timeit(self, number)\n    171 gc.disable()\n    172 try:\n--&gt; 173     timing = self.inner(it, self.timer)\n    174 finally:\n    175     if gcold:\n\nFile &lt;magic-timeit&gt;:1, in inner(_it, _timer)\n\nCell In[109], line 2, in if_function(x)\n      1 def if_function(x):\n----&gt; 2     if x &lt; 0:\n      3         return 0\n      4     else:\n\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()</pre> <p>We might be tempted to simply rewrite this function with a for loop. But remember, Python is terrible when looping...</p> In\u00a0[112]: Copied! <pre>def loop_function(x):\n    ret = np.zeros(len(x))\n    for i in range(len(x)):\n        if x[i] &lt; 0:\n            ret[i] =  0\n        else:\n            ret[i] = np.sqrt(x[i])\n    return ret\n</pre> def loop_function(x):     ret = np.zeros(len(x))     for i in range(len(x)):         if x[i] &lt; 0:             ret[i] =  0         else:             ret[i] = np.sqrt(x[i])     return ret  In\u00a0[113]: Copied! <pre>%timeit y = loop_function(x)\n</pre> %timeit y = loop_function(x) <pre>65.5 ms \u00b1 523 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n</pre> In\u00a0[114]: Copied! <pre>vec_function = np.vectorize(if_function)\n</pre> vec_function = np.vectorize(if_function) In\u00a0[115]: Copied! <pre>%timeit y = vec_function(x)\n</pre> %timeit y = vec_function(x) <pre>54.3 ms \u00b1 422 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n</pre> In\u00a0[116]: Copied! <pre>#first in first out cache wrapper\ndef fifo(func):\n    # Define the cache size \n    cache_size = 5\n    # Use a dictionary as the cache\n    cache = {}\n\n    def wrapper(*args, **kwargs):\n        # tokenize the arguments\n        token = \"\"\n        for arg in args:\n            token += str(arg)\n        for arg in kwargs:\n            token += str(arg)\n\n        # If still have the token\n        if token in cache:\n            return cache[token]\n\n        # Call function\n        result = func(*args, **kwargs)\n        cache[token] = result\n\n        # Check if we need to remove items from cache\n        if len(cache) &gt; 5:\n            cache[next(iter(cache))]\n            \n        # Return the results of the function\n        return result\n    return wrapper\n</pre> #first in first out cache wrapper def fifo(func):     # Define the cache size      cache_size = 5     # Use a dictionary as the cache     cache = {}      def wrapper(*args, **kwargs):         # tokenize the arguments         token = \"\"         for arg in args:             token += str(arg)         for arg in kwargs:             token += str(arg)          # If still have the token         if token in cache:             return cache[token]          # Call function         result = func(*args, **kwargs)         cache[token] = result          # Check if we need to remove items from cache         if len(cache) &gt; 5:             cache[next(iter(cache))]                      # Return the results of the function         return result     return wrapper In\u00a0[117]: Copied! <pre>def multiply_string(my_string, rept):\n    time.sleep(0.5)\n    return my_string * rept\n</pre> def multiply_string(my_string, rept):     time.sleep(0.5)     return my_string * rept   In\u00a0[118]: Copied! <pre>%time multiply_string(\"hello\", 5)\n</pre> %time multiply_string(\"hello\", 5)  <pre>CPU times: user 1.07 ms, sys: 33 \u00b5s, total: 1.1 ms\nWall time: 503 ms\n</pre> Out[118]: <pre>'hellohellohellohellohello'</pre> In\u00a0[119]: Copied! <pre>%time multiply_string(\"hello\", 5)\n</pre> %time multiply_string(\"hello\", 5) <pre>CPU times: user 1.25 ms, sys: 37 \u00b5s, total: 1.29 ms\nWall time: 502 ms\n</pre> Out[119]: <pre>'hellohellohellohellohello'</pre> In\u00a0[120]: Copied! <pre>@fifo\ndef multiply_string_cache(my_string, rept):\n    time.sleep(0.5)\n    return my_string * rept\n</pre> @fifo def multiply_string_cache(my_string, rept):     time.sleep(0.5)     return my_string * rept In\u00a0[121]: Copied! <pre>%time multiply_string_cache(\"goodbye\", 5)\n</pre> %time multiply_string_cache(\"goodbye\", 5)  <pre>CPU times: user 1.2 ms, sys: 36 \u00b5s, total: 1.24 ms\nWall time: 503 ms\n</pre> Out[121]: <pre>'goodbyegoodbyegoodbyegoodbyegoodbye'</pre> In\u00a0[122]: Copied! <pre>%time multiply_string_cache(\"goodbye\", 5)\n</pre> %time multiply_string_cache(\"goodbye\", 5) <pre>CPU times: user 8 \u00b5s, sys: 0 ns, total: 8 \u00b5s\nWall time: 13.1 \u00b5s\n</pre> Out[122]: <pre>'goodbyegoodbyegoodbyegoodbyegoodbye'</pre> <p>Now that we know how the cache works, we can use some blackboxed python functions.</p> <p>Let's use a Least recently used) cache. This will track the number of times we call a set of args. When the cache fills up, the least used item will be removed.</p> In\u00a0[123]: Copied! <pre>from functools import lru_cache\n\n@lru_cache(maxsize=5)\ndef multiply_string_lru(my_string, rept):\n    time.sleep(0.5)\n    return my_string * rept\n</pre> from functools import lru_cache  @lru_cache(maxsize=5) def multiply_string_lru(my_string, rept):     time.sleep(0.5)     return my_string * rept In\u00a0[124]: Copied! <pre>%time multiply_string_lru(\"goodbye\", 5)\n</pre> %time multiply_string_lru(\"goodbye\", 5)  <pre>CPU times: user 952 \u00b5s, sys: 29 \u00b5s, total: 981 \u00b5s\nWall time: 503 ms\n</pre> Out[124]: <pre>'goodbyegoodbyegoodbyegoodbyegoodbye'</pre> In\u00a0[125]: Copied! <pre>%time multiply_string_lru(\"goodbye\", 5)\n</pre> %time multiply_string_lru(\"goodbye\", 5) <pre>CPU times: user 4 \u00b5s, sys: 0 ns, total: 4 \u00b5s\nWall time: 7.39 \u00b5s\n</pre> Out[125]: <pre>'goodbyegoodbyegoodbyegoodbyegoodbye'</pre> In\u00a0[78]: Copied! <pre>def py_fib(x):\n    if x in [0,1] :\n        return 1\n    else:\n        return py_fib(x - 1) + py_fib(x - 2)\n</pre> def py_fib(x):     if x in [0,1] :         return 1     else:         return py_fib(x - 1) + py_fib(x - 2)  In\u00a0[79]: Copied! <pre>%timeit py_fib(10)\n</pre> %timeit py_fib(10) <pre>9.06 \u00b5s \u00b1 93.5 ns per loop (mean \u00b1 std. dev. of 7 runs, 100,000 loops each)\n</pre> In\u00a0[80]: Copied! <pre>from numba import jit\n\n@jit( nopython = True )\ndef py_fib(x):\n    if x in [0,1] :\n        return 1\n    else:\n        return py_fib(x - 1) + py_fib(x - 2)\n</pre> from numba import jit  @jit( nopython = True ) def py_fib(x):     if x in [0,1] :         return 1     else:         return py_fib(x - 1) + py_fib(x - 2)  In\u00a0[81]: Copied! <pre>py_fib(10)\n</pre> py_fib(10) Out[81]: <pre>89</pre> In\u00a0[74]: Copied! <pre>%timeit py_fib(10)\n</pre> %timeit py_fib(10) <pre>362 ns \u00b1 3.35 ns per loop (mean \u00b1 std. dev. of 7 runs, 1,000,000 loops each)\n</pre> <p>We can use packages like Cython to write C++ code in Python. Or we can directly import a precompiled library.</p> <p>We can make C++ functions accessible in Python by wrapping our code in a <code>extern \"C\"</code> block.</p> <pre>extern \"C\" {\n\n    double my_function(... some arguments...){\n        ... some code ...\n    }\n}\n</pre> <p>Why <code>extern \"C\"</code>? One of the differences between C and C++ is that C++ allows us to overload functions. This means that two functions can have the same name, but accept different arguements. For example let say we have some functionality that we would like to make common across different data types:</p> <pre>double get_sqrt_integer( int a );\ndouble get_sqrt_double( double a );\n</pre> <p>The user shouldn't need to worry if the argument is an <code>int</code> or a <code>double</code>. So we would \"overload\" the function like so:</p> <pre>double get_sqrt( int a );\ndouble get_sqrt( double a );\n</pre> <p>So the user only ever calls <code>get_sqrt</code> and the compile handles the logic on whether it is a <code>double</code> or an <code>int</code>. The compile does this by giving each of the function versions unique names that are generated at compile time.</p> <p>By using <code>extern \"C\"</code> we are reverting back to the <code>C</code> method, which will explicitly asign the name we give the function, but it means that we can no longer overload the function. We therefore would need to run:</p> <pre>extern \"C\" {   \n    double get_sqrt_integer( int a );\n    double get_sqrt_double( double a );\n}\n</pre> <p>When analyzing the compiled library we would see the two functions called <code>get_sqrt_integer</code> and <code>get_sqrt_double</code>.</p> In\u00a0[82]: Copied! <pre>%%writefile cpp_fib.cpp\nextern \"C\" {\n    int cpp_fib(unsigned int x)\n    {\n        if ( (x == 0) || (x == 1) )\n        {\n            return 1;\n        }\n        else\n        {\n            return cpp_fib(x-1) + cpp_fib(x-2);\n        }\n    }\n}\n</pre> %%writefile cpp_fib.cpp extern \"C\" {     int cpp_fib(unsigned int x)     {         if ( (x == 0) || (x == 1) )         {             return 1;         }         else         {             return cpp_fib(x-1) + cpp_fib(x-2);         }     } } <pre>Overwriting cpp_fib.cpp\n</pre> <p>Compile to a shared library</p> In\u00a0[76]: Copied! <pre># mamba / conda install gxx\n# or\n# mamba /conda install gxx-compiler\n</pre> # mamba / conda install gxx # or # mamba /conda install gxx-compiler In\u00a0[83]: Copied! <pre>! g++ -fPIC -shared -o cpp_fib.so cpp_fib.cpp -O3\n</pre> ! g++ -fPIC -shared -o cpp_fib.so cpp_fib.cpp -O3 In\u00a0[84]: Copied! <pre>! ls cpp_*\n</pre> ! ls cpp_* <pre>cpp_fib.cpp  cpp_fib.so\n</pre> <p>Load in the library with <code>ctypes</code> and \"translate\" the argument types</p> In\u00a0[85]: Copied! <pre>import ctypes\n\ncpp_lib = ctypes.CDLL(\"./cpp_fib.so\")\n# Access the cpp_fib function from the library\n# and assigne the argument type\ncpp_lib.cpp_fib.argtypes = [ctypes.c_uint] \n# Alias it to cpp_fib\ncpp_fib = cpp_lib.cpp_fib\n</pre> import ctypes  cpp_lib = ctypes.CDLL(\"./cpp_fib.so\") # Access the cpp_fib function from the library # and assigne the argument type cpp_lib.cpp_fib.argtypes = [ctypes.c_uint]  # Alias it to cpp_fib cpp_fib = cpp_lib.cpp_fib  In\u00a0[86]: Copied! <pre>%timeit cpp_fib(10)\n</pre> %timeit cpp_fib(10) <pre>273 ns \u00b1 4.17 ns per loop (mean \u00b1 std. dev. of 7 runs, 1,000,000 loops each)\n</pre> <p>We can also load in Rust code using rustimport</p> <pre><code>mamba/conda install rustimport\n</code></pre> <p>And the <code>pyo3</code> rust crate:</p> In\u00a0[87]: Copied! <pre>%load_ext rustimport\n</pre> %load_ext rustimport <p>use <code>--release</code> to complie an optimized binary rather than a debug binary.</p> In\u00a0[88]: Copied! <pre>%%rustimport  --release --force\nuse pyo3::prelude::*;\n\n#[pyfunction]\nfn rust_fib(x: usize) -&gt; usize {\n    match x {\n        1 =&gt; 1,\n        0 =&gt; 1,\n        _ =&gt; rust_fib(x - 1) + rust_fib(x - 2)\n    }\n}\n</pre> %%rustimport  --release --force use pyo3::prelude::*;  #[pyfunction] fn rust_fib(x: usize) -&gt; usize {     match x {         1 =&gt; 1,         0 =&gt; 1,         _ =&gt; rust_fib(x - 1) + rust_fib(x - 2)     } } <pre>    Updating crates.io index\n Downloading crates ...\n  Downloaded syn v2.0.64\n   Compiling target-lexicon v0.12.14\n   Compiling once_cell v1.19.0\n   Compiling autocfg v1.3.0\n   Compiling proc-macro2 v1.0.82\n   Compiling unicode-ident v1.0.12\n   Compiling libc v0.2.154\n   Compiling parking_lot_core v0.9.10\n   Compiling portable-atomic v1.6.0\n   Compiling cfg-if v1.0.0\n   Compiling scopeguard v1.2.0\n   Compiling smallvec v1.13.2\n   Compiling heck v0.4.1\n   Compiling indoc v2.0.5\n   Compiling unindent v0.2.3\n   Compiling lock_api v0.4.12\n   Compiling memoffset v0.9.1\n   Compiling pyo3-build-config v0.21.2\n   Compiling quote v1.0.36\n   Compiling syn v2.0.64\n   Compiling parking_lot v0.12.2\n   Compiling pyo3-ffi v0.21.2\n   Compiling pyo3 v0.21.2\n   Compiling pyo3-macros-backend v0.21.2\n   Compiling pyo3-macros v0.21.2\n   Compiling _magic_4716e3197ba1bc8f v0.1.0 (/tmp/rustimport/_magic_4716e3197ba1bc8f-860e1651e5b8e95bc6f26cfde7bdc78b/_magic_4716e3197ba1bc8f)\n    Finished release [optimized] target(s) in 8.30s\n</pre> In\u00a0[87]: Copied! <pre>%timeit rust_fib(10)\n</pre> %timeit rust_fib(10) <pre>168 ns \u00b1 19.9 ns per loop (mean \u00b1 std. dev. of 7 runs, 1,000,000 loops each)\n</pre> In\u00a0[88]: Copied! <pre># fib_sqe = [py_fib(i) for i in range(10)]\n# fib_sqe\n</pre> # fib_sqe = [py_fib(i) for i in range(10)] # fib_sqe In\u00a0[89]: Copied! <pre># fib_sqe = [cpp_fib(i) for i in range(10)]\n# fib_sqe\n</pre> # fib_sqe = [cpp_fib(i) for i in range(10)] # fib_sqe In\u00a0[90]: Copied! <pre># fib_sqe = [rust_fib(i) for i in range(10)]\n# fib_sqe\n</pre> # fib_sqe = [rust_fib(i) for i in range(10)] # fib_sqe In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Python/BenchmarkingAndProfiling/#benchmarking-and-profiling","title":"Benchmarking and Profiling\u00b6","text":"<p>Benchmarking and Profiling are crucial techniques used to quantify the performance of code and measure improvements in its execution. In Python, understanding the performance of different implementations or comparing the efficiency of algorithms is essential, especially in scenarios where optimization is vital.</p> <p>When trying to optimize code, it is important to remember that the time required to optimize the code, might be better spent on other tasks. This is to say, when developing code the primary goal is to deliver working code. Delivering fast and efficent code is a secondary goal. With this in mind, it is important not to prematurely optimize code. One can spend an incredible ammount of time trying to optimize code before it even works! Futhermore, one could spend house of work to save only seconds of run time. A balance needs to be struck!</p> <p>A suitable development workflow would look something like this:</p> <ol> <li>Get a working instance of the code.</li> <li>Determine suitable benchmarks for evaluating the code (run time, iterations/second, iterations/cpu cores, percision on output, memory usage etc), and set target performance goals (e.g. total run time, total memory budget, etc).</li> <li>Evaluate the performance using the predefined benchmarks.</li> <li>If performance goals aren't met, profile the code to determine where the bottle necks are and whether changes can be made.</li> <li>Implement suitable changes to the code and repeat from step 3.</li> </ol> <p>Optimizing code can be a very time consuming process. There is a balance that needs to be made between acceptable run time and developer hours needed. Making mulitple changes to the code at once can also introduce bugs that might be difficult to debug. Either make small iterative changes to the code, or make sure the code passes pre-defined tests after each development cycle.</p> <p>Python provides the <code>timeit</code> module, which enables the measurement of execution time for specific code segments. This module offers a simple yet effective way to evaluate the performance of individual code snippets or functions:</p> <ul> <li><code>%%time</code> will time the contents of an entire cell with a single run</li> <li><code>%%timeit</code> will time the contents of an entire cell, averaging over multiples run of that cell</li> <li><code>%time</code> will time a signle line with a single run</li> <li><code>%timeit</code> will time a single line, averaging over multiples run of that cell</li> </ul> <p><code>timeit</code> will also suspend some features, such as the garbage collector, to get an accurate measurement of the code's runtime.</p>"},{"location":"Python/BenchmarkingAndProfiling/#line-profiling","title":"Line profiling\u00b6","text":"<p>Line profiling is a useful tool for finding bottlenecks within a block of code. There are many profilers one can use:</p> <ul> <li>cProfiles</li> <li>line_profiler</li> <li>todo get more</li> </ul> <p>For this example, we'll use line_profiler</p> <pre><code>mamba/conda install line_profiler\n</code></pre> <p>or</p> <pre><code>pip install line_profiler\n</code></pre>"},{"location":"Python/BenchmarkingAndProfiling/#can-we-do-better","title":"Can we do better?\u00b6","text":"<p>We're already using highly optimized NumPy which is based on C/C++ code. However we're still interpting slow bytecode at run time and running on one core.</p>"},{"location":"Python/BenchmarkingAndProfiling/#summary","title":"Summary\u00b6","text":"<p>We've looked at both how to speed up a simple function by first using pure NumPy, then moving to compiled versions, before moving to a parallel implementation:</p>"},{"location":"Python/BenchmarkingAndProfiling/#memory-profiling","title":"Memory Profiling\u00b6","text":"<p>So far we've only considered the run time performance. This is only one aspect of performance. Memory usage is often a limiting factor in performance.</p> <p>We can use the <code>memory_profile</code> to profile the memory of our code. This works by sampling the memory usage at intervals while the code is running.</p> <p>Similar to <code>%%timeit</code> and <code>%timeit</code> we can use <code>%%memit</code> and <code>%memit</code> to get the memory usage of a cell and a single line.</p>"},{"location":"Python/BenchmarkingAndProfiling/#summary","title":"Summary\u00b6","text":"<p>We're managed to reduce the memory usage from ~450 MiB to &lt; 175 MiB. We achieved this by:</p> <ul> <li>Profiling the memory usage to see what to target</li> <li>Avoiding copying the data</li> <li>Using inbuild functions to delete elements rather than doing by copy</li> <li>Changing the percision of the data we're using</li> <li>Reducing the import overhead</li> </ul> <p>There is still a lot of redundency in the above code, we could explore other options:</p> <ul> <li>Rewriting parts of the code in more memory efficient languages (Rust, C++).</li> <li>Reducing the import overhead by reducing the complexity of the code and doing more steps in pure python.</li> <li>Using more memory efficient libraries (e.g. Polars).</li> <li>Spliting the dataset into chunks and loading in smaller bits at a time.</li> </ul> <p>We also need to consider the development cost in finding low-memory solutions. Reducing the memory usage by 10% might be better than spending hours trying to reduce it by 20%.</p> <p>The methods we've used here also have trade offs:</p> <ul> <li>Going from f64 -&gt; f16 is a lossy form of compression. We have lost information that we cannot simply get back.</li> <li>We've thrown away data after it's read in and filtered. We can no longer compare to the original dataset without rewriting code.</li> <li>We've decreased the complexity, we can no longer access functions like <code>numpy.sum</code> without explicitly importing them first.</li> </ul> <p>When working with clusters we can use tools such as <code>memory_profiler</code> to estimate the requirements of the job we're about to submit. This can prevent our job crashing if we haven't requested enough memory.</p>"},{"location":"Python/BenchmarkingAndProfiling/#cheap-tricks-to-generally-speed-up-your-code","title":"Cheap Tricks to \"generally\" speed up your code\u00b6","text":""},{"location":"Python/BenchmarkingAndProfiling/#vectorization-is-your-friend","title":"Vectorization is your friend\u00b6","text":"<p>Vecorizing functions allow them to be applied to NumPy arrays, without having to write a for loop.</p>"},{"location":"Python/BenchmarkingAndProfiling/#caching-frequently-used-function-calls","title":"Caching frequently used function calls\u00b6","text":"<p>When you're expecting to repeat function calls with the same arguements, cache the results.</p>"},{"location":"Python/BenchmarkingAndProfiling/#already-have-working-code-from-a-more-efficient-language-import-it","title":"Already have working code from a more efficient language? Import it!\u00b6","text":"<p>Python has lots of tools to import code from different languages</p>"},{"location":"Python/BenchmarkingAndProfiling/#dont-reinvent-the-wheel","title":"Don't reinvent the wheel!\u00b6","text":"<p>Python is one of the most popular languages. A lot of super talented people have spent a lot of time optimizing python and wrapping low lower level languages in Python code. Use libraries such as:</p> <ul> <li>NumPy</li> <li>SciPy</li> <li>Pandas</li> </ul>"},{"location":"Python/Calculating_pi/","title":"Calculating $\\pi$","text":"In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef plot_circle():\n  ax = plt.subplot(111)\n  # Coordinates of the square\n  x = [-1, 1, 1, -1, -1]\n  y = [-1, -1, 1, 1, -1]\n  plt.plot(x, y)\n\n  # Coordinates of the circle\n  theta = np.linspace(0, 2*np.pi, 100)\n  x_circle = np.cos(theta)\n  y_circle = np.sin(theta)\n  plt.plot(x_circle, y_circle)\n\n  # Add an arrow indicating the radius\n  plt.arrow(0, 0, 1, 0, fc='r', ec = \"r\")\n  plt.text(1.1, 0, 'r = 1')\n  plt.axis('equal')\n  return ax\n\n\nax = plot_circle()\n</pre> import matplotlib.pyplot as plt import numpy as np   def plot_circle():   ax = plt.subplot(111)   # Coordinates of the square   x = [-1, 1, 1, -1, -1]   y = [-1, -1, 1, 1, -1]   plt.plot(x, y)    # Coordinates of the circle   theta = np.linspace(0, 2*np.pi, 100)   x_circle = np.cos(theta)   y_circle = np.sin(theta)   plt.plot(x_circle, y_circle)    # Add an arrow indicating the radius   plt.arrow(0, 0, 1, 0, fc='r', ec = \"r\")   plt.text(1.1, 0, 'r = 1')   plt.axis('equal')   return ax   ax = plot_circle()  <p>We want to count the number of raindrops that fall in the circle and the number that fall in the square.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n</pre> import numpy as np In\u00a0[\u00a0]: Copied! <pre>n = 1000\n\n# Generate n uniform random numbers between 0-5\na = np.random.uniform(0, 5, n)\n\n# Generate n normal/gaussian distributed numbers with a mean of 5 and width of 1\nmean = 5\nstd = 1\nb = np.random.normal(mean, std, n)\n\n# Generate n random integers between 1 and 100\nc = np.random.randint(1, 100, n)\n</pre> n = 1000  # Generate n uniform random numbers between 0-5 a = np.random.uniform(0, 5, n)  # Generate n normal/gaussian distributed numbers with a mean of 5 and width of 1 mean = 5 std = 1 b = np.random.normal(mean, std, n)  # Generate n random integers between 1 and 100 c = np.random.randint(1, 100, n) In\u00a0[\u00a0]: Copied! <pre>fig = plt.figure(figsize=(15, 5))\nax1 = fig.add_subplot(1, 3, 1)\nax2 = fig.add_subplot(1, 3, 2)\nax3 = fig.add_subplot(1, 3, 3)\n\nax1.hist(a, bins=np.linspace(0,5,20))\nax1.set_title('Uniform Distribution')\n\nax2.hist(b, bins=np.linspace(0,10,20))\nax2.set_title('Normal Distribution')\n\nax3.hist(c, bins=np.linspace(0,100,150))\nax3.set_title('Integer Distribution')\n</pre> fig = plt.figure(figsize=(15, 5)) ax1 = fig.add_subplot(1, 3, 1) ax2 = fig.add_subplot(1, 3, 2) ax3 = fig.add_subplot(1, 3, 3)  ax1.hist(a, bins=np.linspace(0,5,20)) ax1.set_title('Uniform Distribution')  ax2.hist(b, bins=np.linspace(0,10,20)) ax2.set_title('Normal Distribution')  ax3.hist(c, bins=np.linspace(0,100,150)) ax3.set_title('Integer Distribution')   Out[\u00a0]: <pre>Text(0.5, 1.0, 'Integer Distribution')</pre> In\u00a0[\u00a0]: Copied! <pre>def my_relationship(x):\n  return x**2 + 2*x + 1\n</pre> def my_relationship(x):   return x**2 + 2*x + 1 In\u00a0[\u00a0]: Copied! <pre>x = np.linspace(-5, 5, 1000)\ny = my_relationship(x)\nplt.plot(x, y)\n</pre> x = np.linspace(-5, 5, 1000) y = my_relationship(x) plt.plot(x, y) Out[\u00a0]: <pre>[&lt;matplotlib.lines.Line2D at 0x7d9e6f183280&gt;]</pre> <p>Let's write a function to get the cumulative distribution function of this dataset.</p> In\u00a0[\u00a0]: Copied! <pre>def get_cdf(y):\n  # Initialize to zero\n  arr = np.zeros(len(y))\n  # Loop over entries\n  arr[0] = y[0]\n  for i in range(len(y)-1):\n    arr[i+1] = y[i+1] + arr[i]\n  return arr/arr[-1]\n</pre> def get_cdf(y):   # Initialize to zero   arr = np.zeros(len(y))   # Loop over entries   arr[0] = y[0]   for i in range(len(y)-1):     arr[i+1] = y[i+1] + arr[i]   return arr/arr[-1] In\u00a0[\u00a0]: Copied! <pre># Plot the cdf\ny_cdf = get_cdf(y)  # Get the cumulative distribution function of y\nplt.plot(x, y_cdf)  # Plot the CDF against x values\nplt.ylabel('Cumulative probability')  # Label for the y-axis\nplt.xlabel('x')  # Label for the x-axis\nplt.grid()  # Add grid for better visualization\nplt.show()  # Display the plot\n</pre> # Plot the cdf y_cdf = get_cdf(y)  # Get the cumulative distribution function of y plt.plot(x, y_cdf)  # Plot the CDF against x values plt.ylabel('Cumulative probability')  # Label for the y-axis plt.xlabel('x')  # Label for the x-axis plt.grid()  # Add grid for better visualization plt.show()  # Display the plot <p>We can use interpolation to find the value of this curve at any point. In particular, we want a way to go from the cumulative distribution function (CDF) to the actual distribution.</p> <p>We'll use <code>scipy</code> and the function interp1d for this purpose.</p> In\u00a0[\u00a0]: Copied! <pre>from scipy.interpolate import interp1d\n\n# We want to provide a number between 0-1 and return the value of x (-5,5) corresponding to that\n# Make sure the boundries are set so that we don't get out of range errors\ninterpolation = interp1d(y_cdf, x, bounds_error=False, fill_value=(x[0], x[-1]))\n</pre> from scipy.interpolate import interp1d  # We want to provide a number between 0-1 and return the value of x (-5,5) corresponding to that # Make sure the boundries are set so that we don't get out of range errors interpolation = interp1d(y_cdf, x, bounds_error=False, fill_value=(x[0], x[-1])) In\u00a0[\u00a0]: Copied! <pre># Generate Random Numbers\nrnd_numbers = np.random.uniform(0, 1, 1000)\nrnd_x = interpolation(rnd_numbers)\n\n\n# Get the plot normalization\n# Normalizing the relationship curve to have equal area under the curve\nnorm_rnd = np.sum(rnd_x)\nnorm_relationship = np.sum(y)\nnorm = norm_relationship / norm_rnd\n\n\nplt.hist(rnd_x, bins=np.linspace(-5,5,20), label = \"Random Numbers\")\nplt.plot(x, norm*y, label = \"Relationship\")\nplt.ylabel('Probability density')\nplt.xlabel('x')\nplt.legend();\n</pre> # Generate Random Numbers rnd_numbers = np.random.uniform(0, 1, 1000) rnd_x = interpolation(rnd_numbers)   # Get the plot normalization # Normalizing the relationship curve to have equal area under the curve norm_rnd = np.sum(rnd_x) norm_relationship = np.sum(y) norm = norm_relationship / norm_rnd   plt.hist(rnd_x, bins=np.linspace(-5,5,20), label = \"Random Numbers\") plt.plot(x, norm*y, label = \"Relationship\") plt.ylabel('Probability density') plt.xlabel('x') plt.legend(); In\u00a0[\u00a0]: Copied! <pre>n = 1000\n\n# Get random x and y coordinates for the rain drops\nrain_drops_x = np.random.uniform(-1, 1, n)\nrain_drops_y = np.random.uniform(-1, 1, n)\n</pre> n = 1000  # Get random x and y coordinates for the rain drops rain_drops_x = np.random.uniform(-1, 1, n) rain_drops_y = np.random.uniform(-1, 1, n)  In\u00a0[\u00a0]: Copied! <pre>ax = plot_circle()\nax.scatter(rain_drops_x, rain_drops_y, s=1)\n</pre> ax = plot_circle() ax.scatter(rain_drops_x, rain_drops_y, s=1) Out[\u00a0]: <pre>&lt;matplotlib.collections.PathCollection at 0x7d9e6e8e1c00&gt;</pre> <p>This is good, we have the random rain drops. How can we count the number of raindrops within the circle and outside of the circle?</p> <p>The equation of a circle is: $$ r^2 = x^2 + y^2 $$</p> <p>So for our circle of radius 1, the outter edge is defined as: $$ x^2 + y^2 = 1$$</p> In\u00a0[\u00a0]: Copied! <pre>counter_in = 0\n\nfor i in range(n):\n  # Recall the equation of a circle!\n  if rain_drops_x[i]**2 + rain_drops_y[i]**2 &lt; 1:\n    counter_in += 1\n\nprint(counter_in)\n</pre> counter_in = 0  for i in range(n):   # Recall the equation of a circle!   if rain_drops_x[i]**2 + rain_drops_y[i]**2 &lt; 1:     counter_in += 1  print(counter_in) <pre>793\n</pre> <p>Using what we derived before, let's estimate $\\pi$</p> In\u00a0[\u00a0]: Copied! <pre>pi_estimate = 4 * counter_in / n\nprint(pi_estimate)\n</pre> pi_estimate = 4 * counter_in / n print(pi_estimate) <pre>3.172\n</pre> <p>This is relatively close, but we wouldn't want to rely on this number to land on the Moon!</p> <p>How can we increase the accuracy? Recall that $$ \\pi \\approx 4 \\times \\frac{A_{circle}}{A_{square}},$$</p> <p>Is only an approximate. As the number of rain drops tends to infinity, then $$ \\pi = 4 \\times \\frac{A_{circle}}{A_{square}}.$$</p> <p>So to increase the accuracy, we need to increase the number of rain drops!</p> In\u00a0[\u00a0]: Copied! <pre>import time\n</pre> import time In\u00a0[\u00a0]: Copied! <pre># Let's function-ize our random number generator\ndef get_rain_drops(n):\n  rain_drops_x = np.random.uniform(-1, 1, n)\n  rain_drops_y = np.random.uniform(-1, 1, n)\n  return rain_drops_x, rain_drops_y\n</pre> # Let's function-ize our random number generator def get_rain_drops(n):   rain_drops_x = np.random.uniform(-1, 1, n)   rain_drops_y = np.random.uniform(-1, 1, n)   return rain_drops_x, rain_drops_y In\u00a0[\u00a0]: Copied! <pre>n_samples = np.array([100, 1000, 10000, 100000, 1000000])\npi_values = np.zeros(len(n_samples))\ntime_values = np.zeros(len(n_samples))\n\nfor j, n in enumerate(n_samples):\n  start = time.time()\n  rain_drops_x, rain_drops_y = get_rain_drops(n)\n  counter_in = 0\n  for i in range(n):\n    if rain_drops_x[i]**2 + rain_drops_y[i]**2 &lt; 1:\n      counter_in += 1\n  pi_values[j] = (4*counter_in/n)\n  time_values[j]= (time.time() - start)\n  print(n, pi_values[j], time_values[j])\n</pre> n_samples = np.array([100, 1000, 10000, 100000, 1000000]) pi_values = np.zeros(len(n_samples)) time_values = np.zeros(len(n_samples))  for j, n in enumerate(n_samples):   start = time.time()   rain_drops_x, rain_drops_y = get_rain_drops(n)   counter_in = 0   for i in range(n):     if rain_drops_x[i]**2 + rain_drops_y[i]**2 &lt; 1:       counter_in += 1   pi_values[j] = (4*counter_in/n)   time_values[j]= (time.time() - start)   print(n, pi_values[j], time_values[j])  <pre>100 2.92 0.00023937225341796875\n1000 3.172 0.0016646385192871094\n10000 3.1548 0.010968208312988281\n100000 3.14744 0.08640599250793457\n1000000 3.142572 0.8315095901489258\n</pre> In\u00a0[\u00a0]: Copied! <pre>plt.plot(n_samples, pi_values)\nplt.axhline(np.pi, color='r', linestyle='--', label='Actual value of pi')\nplt.title('Value of pi vs. Number of rain drops')\nplt.xlabel('Number of rain drops')\nplt.ylabel('Value of pi')\nplt.xscale('log')\n</pre> plt.plot(n_samples, pi_values) plt.axhline(np.pi, color='r', linestyle='--', label='Actual value of pi') plt.title('Value of pi vs. Number of rain drops') plt.xlabel('Number of rain drops') plt.ylabel('Value of pi') plt.xscale('log') <p>From this it looks like we have an accurate value after $10^5$ rain drops. However is this true? Let's look instead at how far away from the true value we are.</p> In\u00a0[\u00a0]: Copied! <pre># multi plot of value of pi, accuracy and run time\nfig, ax = plt.subplots(1,3, figsize = (18,6))\nax[0].plot(n_samples, pi_values)\nax[0].set_xlabel('Number of rain drops')\nax[0].set_ylabel('Value of pi')\nax[0].set_xscale('log')\nax[0].axhline(np.pi, color='r', linestyle='--', label='Actual value of pi')\nax[0].legend()\n\n\nax[1].plot(n_samples, np.abs(pi_values - np.pi))\nax[1].set_xlabel('Number of rain drops')\nax[1].set_ylabel('Simulated vs True')\nax[1].set_xscale('log')\nax[1].set_yscale('log')\n\n\n\nax[2].plot(n_samples, time_values)\nax[2].set_xlabel('Number of rain drops')\nax[2].set_ylabel('Simulation runtime [s]')\nax[2].set_xscale('log')\n</pre> # multi plot of value of pi, accuracy and run time fig, ax = plt.subplots(1,3, figsize = (18,6)) ax[0].plot(n_samples, pi_values) ax[0].set_xlabel('Number of rain drops') ax[0].set_ylabel('Value of pi') ax[0].set_xscale('log') ax[0].axhline(np.pi, color='r', linestyle='--', label='Actual value of pi') ax[0].legend()   ax[1].plot(n_samples, np.abs(pi_values - np.pi)) ax[1].set_xlabel('Number of rain drops') ax[1].set_ylabel('Simulated vs True') ax[1].set_xscale('log') ax[1].set_yscale('log')    ax[2].plot(n_samples, time_values) ax[2].set_xlabel('Number of rain drops') ax[2].set_ylabel('Simulation runtime [s]') ax[2].set_xscale('log')  <p>From the above plot we can say that with $10^5$ rain drops we are accurate to the 2-3 decimal place. However moving to $10^6$ samples increases the runtime.</p> <p>This might be a good time to look at the performance of our simulation.</p> In\u00a0[\u00a0]: Copied! <pre># Start of by further functionizing our monte carlo simulation\ndef calculate_pi_v0(n):\n  rain_drops_x, rain_drops_y = get_rain_drops(n)\n  counter_in = 0\n  for i in range(n):\n    if rain_drops_x[i]**2 + rain_drops_y[i]**2 &lt; 1:\n      counter_in += 1\n  return (4*counter_in/n)\n</pre> # Start of by further functionizing our monte carlo simulation def calculate_pi_v0(n):   rain_drops_x, rain_drops_y = get_rain_drops(n)   counter_in = 0   for i in range(n):     if rain_drops_x[i]**2 + rain_drops_y[i]**2 &lt; 1:       counter_in += 1   return (4*counter_in/n)    In\u00a0[\u00a0]: Copied! <pre>calculate_pi_v0(10000)\n</pre> calculate_pi_v0(10000) Out[\u00a0]: <pre>3.1536</pre> <p>Let's import <code>timeit</code>. This library allows us to time processes in python. It does some things behind the scenes such as pausing processes that might effect the estimation of the run time.</p> <p>We can use \"magic\" to time our function:</p> <ul> <li><code>%timeit</code> will time a single line of code</li> <li><code>%%timeit</code> will time the whole cell</li> <li>The <code>timeit.repeat</code> function will give us a little more control over what happens and allow us to save the results to a variable.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>import timeit\n</pre> import timeit In\u00a0[\u00a0]: Copied! <pre># Get the benchmark time before we change the code\n# We'll run the code 10 times without pausing, repeating the experiment 20 times\nn_run = 10\nn_repeat = 20\nn_drops = 10000\nresults_0 = timeit.repeat(f\"calculate_pi_v0({n_drops})\", \"from __main__ import calculate_pi_v0\",\n                  number=n_run, repeat=n_repeat)\nprint (f\"Baseline: {np.mean(results_0):0.4f} +/- {np.std(results_0):0.4f} seconds\")\n</pre> # Get the benchmark time before we change the code # We'll run the code 10 times without pausing, repeating the experiment 20 times n_run = 10 n_repeat = 20 n_drops = 10000 results_0 = timeit.repeat(f\"calculate_pi_v0({n_drops})\", \"from __main__ import calculate_pi_v0\",                   number=n_run, repeat=n_repeat) print (f\"Baseline: {np.mean(results_0):0.4f} +/- {np.std(results_0):0.4f} seconds\") <pre>Baseline: 0.1024 +/- 0.0325 seconds\n</pre> In\u00a0[\u00a0]: Copied! <pre>!pip install line_profiler\n%load_ext line_profiler\n</pre> !pip install line_profiler %load_ext line_profiler  <pre>Collecting line_profiler\n  Downloading line_profiler-4.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\nDownloading line_profiler-4.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (717 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 717.6/717.6 kB 12.4 MB/s eta 0:00:00\nInstalling collected packages: line_profiler\nSuccessfully installed line_profiler-4.1.3\n</pre> In\u00a0[\u00a0]: Copied! <pre>lprun -f calculate_pi_v0 calculate_pi_v0(n_drops)\n</pre> lprun -f calculate_pi_v0 calculate_pi_v0(n_drops) <p>We should see something like:</p> <pre><code>Timer unit: 1e-09 s\n\nTotal time: 0.0634801 s\nFile: &lt;ipython-input-114-aee2509612d0&gt;\nFunction: calculate_pi at line 2\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n     2                                           def calculate_pi(n):\n     3         1    3837332.0    4e+06      6.0    rain_drops_x, rain_drops_y = get_rain_drops(n)\n     4         1        677.0    677.0      0.0    counter_in = 0\n     5     10001    6192577.0    619.2      9.8    for i in range(n):\n     6     10000   46764473.0   4676.4     73.7      if rain_drops_x[i]**2 + rain_drops_y[i]**2 &lt; 1:\n     7      7846    6681865.0    851.6     10.5        counter_in += 1\n     8         1       3146.0   3146.0      0.0    return (4*counter_in/n)\n</code></pre> <p>This shows line-by-line where we're spending the most of our time</p> <p>Looking at the <code>Time</code> column we can see that generating the random numbers takes up a large portion of the time. However this only happens once, only taking up ~6% of the time (as shown in the <code>Hits</code> and <code>% Time</code> columns).</p> <p>If we focus on the <code>% Time</code> column, we can see that the majority of our time is spent checking if the rain drops are within the circle:</p> <pre><code>     6     10000   46764473.0   4676.4     73.7      if rain_drops_x[i]**2 + rain_drops_y[i]**2 &lt; 1:\n</code></pre> <p>In this case we spend ~70% or our time on this line. We also spend ~10% of our time in the for loop:</p> <pre><code>     5     10001    6192577.0    619.2      9.8    for i in range(n):\n</code></pre> <p>And another ~10% of our time counting the number of rain drops:</p> <pre><code>     7      7846    6681865.0    851.6     10.5        counter_in += 1\n</code></pre> <p>Can we combine these three lines to improve the code?</p> In\u00a0[\u00a0]: Copied! <pre># Generate some random numbers\nx = np.random.uniform(-1, 1, 10)\nprint (x)\n# Get a mask (True/False) where x &gt; 0\nmask = x &gt; 0\nprint(mask)\n\n# Recall that when converting to ints True = 1 and False = 0\nprint(mask*1)\n\n# We can get the sum of numpy arrays using either np.sum or\nprint (mask.sum())\n</pre> # Generate some random numbers x = np.random.uniform(-1, 1, 10) print (x) # Get a mask (True/False) where x &gt; 0 mask = x &gt; 0 print(mask)  # Recall that when converting to ints True = 1 and False = 0 print(mask*1)  # We can get the sum of numpy arrays using either np.sum or print (mask.sum()) <pre>[ 0.72171476 -0.50958269 -0.04502572  0.03438927  0.21523572 -0.21264337\n -0.36754284 -0.54510252 -0.764482   -0.64749491]\n[ True False False  True  True False False False False False]\n[1 0 0 1 1 0 0 0 0 0]\n3\n</pre> <p>With this in mind, let's remove the for loop:</p> In\u00a0[\u00a0]: Copied! <pre>def calculate_pi_v1(n):\n  rain_drops_x, rain_drops_y = get_rain_drops(n)\n  counter_in = 0\n  counter_in = (rain_drops_x**2 + rain_drops_y**2 &lt; 1).sum()\n  return (4*counter_in/n)\n</pre> def calculate_pi_v1(n):   rain_drops_x, rain_drops_y = get_rain_drops(n)   counter_in = 0   counter_in = (rain_drops_x**2 + rain_drops_y**2 &lt; 1).sum()   return (4*counter_in/n)   In\u00a0[\u00a0]: Copied! <pre>calculate_pi_v1(1000)\n</pre> calculate_pi_v1(1000) Out[\u00a0]: <pre>3.188</pre> In\u00a0[\u00a0]: Copied! <pre>results_1 = timeit.repeat(f\"calculate_pi_v1({n_drops})\", \"from __main__ import calculate_pi_v1\",\n                  number=n_run, repeat=n_repeat)\nprint (f\"Baseline: {np.mean(results_0):0.4f} +/- {np.std(results_0):0.4f} seconds\")\nprint (f\"V1: {np.mean(results_1):0.4f} +/- {np.std(results_1):0.4f} seconds -&gt; Speedup: {np.mean(results_0)/np.mean(results_1):0.4f}\")\n</pre> results_1 = timeit.repeat(f\"calculate_pi_v1({n_drops})\", \"from __main__ import calculate_pi_v1\",                   number=n_run, repeat=n_repeat) print (f\"Baseline: {np.mean(results_0):0.4f} +/- {np.std(results_0):0.4f} seconds\") print (f\"V1: {np.mean(results_1):0.4f} +/- {np.std(results_1):0.4f} seconds -&gt; Speedup: {np.mean(results_0)/np.mean(results_1):0.4f}\")  <pre>Baseline: 0.0647 +/- 0.0041 seconds\nV1: 0.0029 +/- 0.0010 seconds -&gt; Speedup: 22.5632\n</pre> <p>Let's profile the function again:</p> In\u00a0[\u00a0]: Copied! <pre>lprun -f calculate_pi_v1 calculate_pi_v1(n_drops)\n</pre> lprun -f calculate_pi_v1 calculate_pi_v1(n_drops) <p>Which gives something like:</p> <pre><code>Timer unit: 1e-09 s\n\nTotal time: 0.00172171 s\nFile: &lt;ipython-input-147-23550d2cf014&gt;\nFunction: calculate_pi_v1 at line 1\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n     1                                           def calculate_pi_v1(n):\n     2         1    1565852.0    2e+06     90.9    rain_drops_x, rain_drops_y = get_rain_drops(n)\n     3         1       1046.0   1046.0      0.1    counter_in = 0\n     4         1     150971.0 150971.0      8.8    counter_in = (rain_drops_x**2 + rain_drops_y**2 &lt; 1).sum()\n     5         1       3844.0   3844.0      0.2    return (4*counter_in/n)\n</code></pre> <p>We've removed the expensive for loop, instead we're now relying on <code>numpy</code>'s highly optimized operations to loop over the array and get the sum.</p> <p>We're now spending 90% of our time in the <code>get_rain_drops</code> function. Let's take a look that in some more detail. We can use <code>%timeit</code> to estimate the runtime:</p> In\u00a0[\u00a0]: Copied! <pre>%timeit get_rain_drops(n_drops)\n</pre> %timeit get_rain_drops(n_drops) <pre>214 \u00b5s \u00b1 5 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n</pre> In\u00a0[\u00a0]: Copied! <pre>lprun -f get_rain_drops get_rain_drops(n_drops)\n</pre> lprun -f get_rain_drops get_rain_drops(n_drops) <pre><code>Timer unit: 1e-09 s\n\nTotal time: 0.00286489 s\nFile: &lt;ipython-input-16-cd75bad8bfb6&gt;\nFunction: get_rain_drops at line 2\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n     2                                           def get_rain_drops(n):\n     3         1     508631.0 508631.0     17.8    rain_drops_x = np.random.uniform(-1, 1, n)\n     4         1    2355321.0    2e+06     82.2    rain_drops_y = np.random.uniform(-1, 1, n)\n     5         1        938.0    938.0      0.0    return rain_drops_x, rain_drops_y\n</code></pre> <p>We're still spending a lot of time generating random numbers. Can we speed this up using some more advanced methods?</p> In\u00a0[\u00a0]: Copied! <pre>from numba import jit, prange\nimport random\n\n\n# Let's functionize our random number generator\n@jit(parallel=True, nogil=True, nopython=True)\ndef get_rain_drops_v2(n):\n  x, y = np.empty(n), np.empty(n)\n  for i in prange(n):\n     x[i] = random.uniform(0,1)\n     y[i] = random.uniform(0,1)\n  return x,y\n\n# A quick run so that the code compiles before we test it\nget_rain_drops_v2(1)\n</pre> from numba import jit, prange import random   # Let's functionize our random number generator @jit(parallel=True, nogil=True, nopython=True) def get_rain_drops_v2(n):   x, y = np.empty(n), np.empty(n)   for i in prange(n):      x[i] = random.uniform(0,1)      y[i] = random.uniform(0,1)   return x,y  # A quick run so that the code compiles before we test it get_rain_drops_v2(1) Out[\u00a0]: <pre>(array([0.19351567]), array([0.68520083]))</pre> In\u00a0[\u00a0]: Copied! <pre>%timeit get_rain_drops_v2(n_drops)\n</pre> %timeit get_rain_drops_v2(n_drops) <pre>169 \u00b5s \u00b1 51.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n</pre> <p>This offers some improvement over the original <code>get_rain_drops</code> functions. Let's see how this impacts our monte carlo simulation:</p> In\u00a0[\u00a0]: Copied! <pre>def calculate_pi_v2(n):\n  rain_drops_x, rain_drops_y = get_rain_drops_v2(n)\n  counter_in = 0\n  counter_in = (rain_drops_x**2 + rain_drops_y**2 &lt; 1).sum()\n  return (4*counter_in/n)\n</pre> def calculate_pi_v2(n):   rain_drops_x, rain_drops_y = get_rain_drops_v2(n)   counter_in = 0   counter_in = (rain_drops_x**2 + rain_drops_y**2 &lt; 1).sum()   return (4*counter_in/n)  In\u00a0[\u00a0]: Copied! <pre>results_2 = timeit.repeat(f\"calculate_pi_v2({n_drops})\", \"from __main__ import calculate_pi_v2\",\n                  number=n_run, repeat=n_repeat)\nprint (f\"Baseline: {np.mean(results_0):0.4f} +/- {np.std(results_0):0.4f} seconds\")\nprint (f\"V1: {np.mean(results_1):0.4f} +/- {np.std(results_1):0.4f} seconds -&gt; Speedup: {np.mean(results_0)/np.mean(results_1):0.4f}\")\nprint (f\"V2: {np.mean(results_2):0.4f} +/- {np.std(results_2):0.4f} seconds -&gt; Speedup: {np.mean(results_0)/np.mean(results_2):0.4f}\")\n</pre> results_2 = timeit.repeat(f\"calculate_pi_v2({n_drops})\", \"from __main__ import calculate_pi_v2\",                   number=n_run, repeat=n_repeat) print (f\"Baseline: {np.mean(results_0):0.4f} +/- {np.std(results_0):0.4f} seconds\") print (f\"V1: {np.mean(results_1):0.4f} +/- {np.std(results_1):0.4f} seconds -&gt; Speedup: {np.mean(results_0)/np.mean(results_1):0.4f}\") print (f\"V2: {np.mean(results_2):0.4f} +/- {np.std(results_2):0.4f} seconds -&gt; Speedup: {np.mean(results_0)/np.mean(results_2):0.4f}\") <pre>Baseline: 0.0647 +/- 0.0041 seconds\nV1: 0.0029 +/- 0.0010 seconds -&gt; Speedup: 22.5632\nV2: 0.0024 +/- 0.0006 seconds -&gt; Speedup: 27.4095\n</pre> <p>This seems to offer an improvement to the code, but there area still some fluctuations in the run speed. We likely near the limit of what we can measure with 5 runs, repeated 20 times.</p> In\u00a0[\u00a0]: Copied! <pre>@jit(parallel=True, nogil=True, nopython=True)\ndef calculate_pi_v3(n):\n  rain_drops_x, rain_drops_y = get_rain_drops_v2(n)\n  return (4*(rain_drops_x**2 + rain_drops_y**2 &lt; 1).sum()/n)\n</pre> @jit(parallel=True, nogil=True, nopython=True) def calculate_pi_v3(n):   rain_drops_x, rain_drops_y = get_rain_drops_v2(n)   return (4*(rain_drops_x**2 + rain_drops_y**2 &lt; 1).sum()/n) In\u00a0[\u00a0]: Copied! <pre>#pre-run to compile the code\ncalculate_pi_v3(n_drops)\n</pre> #pre-run to compile the code calculate_pi_v3(n_drops) Out[\u00a0]: <pre>3.1172</pre> In\u00a0[\u00a0]: Copied! <pre>results_3 = timeit.repeat(f\"calculate_pi_v3({n_drops})\", \"from __main__ import calculate_pi_v3\",\n                  number=n_run, repeat=n_repeat)\nprint (f\"Baseline: {np.mean(results_0):0.4f} +/- {np.std(results_0):0.4f} seconds\")\nprint (f\"V1: {np.mean(results_1):0.4f} +/- {np.std(results_1):0.4f} seconds -&gt; Speedup: {np.mean(results_0)/np.mean(results_1):0.4f}\")\nprint (f\"V2: {np.mean(results_2):0.4f} +/- {np.std(results_2):0.4f} seconds -&gt; Speedup: {np.mean(results_0)/np.mean(results_2):0.4f}\")\nprint (f\"V3: {np.mean(results_3):0.4f} +/- {np.std(results_3):0.4f} seconds -&gt; Speedup: {np.mean(results_0)/np.mean(results_3):0.4f}\")\n</pre> results_3 = timeit.repeat(f\"calculate_pi_v3({n_drops})\", \"from __main__ import calculate_pi_v3\",                   number=n_run, repeat=n_repeat) print (f\"Baseline: {np.mean(results_0):0.4f} +/- {np.std(results_0):0.4f} seconds\") print (f\"V1: {np.mean(results_1):0.4f} +/- {np.std(results_1):0.4f} seconds -&gt; Speedup: {np.mean(results_0)/np.mean(results_1):0.4f}\") print (f\"V2: {np.mean(results_2):0.4f} +/- {np.std(results_2):0.4f} seconds -&gt; Speedup: {np.mean(results_0)/np.mean(results_2):0.4f}\") print (f\"V3: {np.mean(results_3):0.4f} +/- {np.std(results_3):0.4f} seconds -&gt; Speedup: {np.mean(results_0)/np.mean(results_3):0.4f}\") <pre>Baseline: 0.0647 +/- 0.0041 seconds\nV1: 0.0029 +/- 0.0010 seconds -&gt; Speedup: 22.5632\nV2: 0.0024 +/- 0.0006 seconds -&gt; Speedup: 27.4095\nV3: 0.0019 +/- 0.0016 seconds -&gt; Speedup: 33.4014\n</pre> In\u00a0[\u00a0]: Copied! <pre>from matplotlib.animation import FuncAnimation, PillowWriter\n\n# Set up raindrop parameters\nx, y = np.array([]), np.array([])\nn_drops = np.logspace(0, 6, 100).astype(int)\nn_drops[1:] -= n_drops[:-1]\n\n# Create plot figure\nfig, ax = plt.subplots()\nax.set_aspect('equal')\nax.set_xlim(-1.5, 1.5)\nax.set_ylim(-1.5, 1.5)\n\n# Draw circle to represent the unit circle\ncircle = plt.Circle((0, 0), 1, color='blue', alpha=0.2)\nax.add_artist(circle)\n\n# Draw square from (-1, -1) to (1, 1)\nsquare = plt.Rectangle((-1, -1), 2, 2, edgecolor='green', facecolor='none', linewidth=1.5)\nax.add_artist(square)\n\n# Initialize scatter plot and text\nscatter = ax.scatter([], [], s=0.1)\ntext = ax.text(-1.4, -1.4, \"\", color=\"red\")\n\nax.arrow(0, 0, 1, 0, fc='r', ec = \"r\")\nax.text(1.1, 0, 'r = 1')\n\n# Function to update the plot with each batch of raindrops\ndef update(i):\n    global x, y\n\n    # Add a new batch of raindrops\n    x = np.append(x, np.random.uniform(-1, 1, int(n_drops[i])))\n    y = np.append(y, np.random.uniform(-1, 1, int(n_drops[i])))\n\n    # Calculate estimated Pi\n    pi_estimate = 4 * (x**2 + y**2 &lt; 1).sum() / len(x)\n\n    # Update scatter plot data and Pi text\n    scatter.set_offsets(np.c_[x, y])\n    text.set_text(f\"n = {len(x)}\\n\" + r\"$\\pi \\approx$\" + f\"{pi_estimate:.3f}\")\n\n    return scatter, text\n\n# Create animation\nanim = FuncAnimation(fig, update, frames=len(n_drops), blit=True)\n\n# Save animation as GIF\nanim.save(\"raindrops_pi_estimate.gif\", writer=PillowWriter(fps=2))\n</pre> from matplotlib.animation import FuncAnimation, PillowWriter  # Set up raindrop parameters x, y = np.array([]), np.array([]) n_drops = np.logspace(0, 6, 100).astype(int) n_drops[1:] -= n_drops[:-1]  # Create plot figure fig, ax = plt.subplots() ax.set_aspect('equal') ax.set_xlim(-1.5, 1.5) ax.set_ylim(-1.5, 1.5)  # Draw circle to represent the unit circle circle = plt.Circle((0, 0), 1, color='blue', alpha=0.2) ax.add_artist(circle)  # Draw square from (-1, -1) to (1, 1) square = plt.Rectangle((-1, -1), 2, 2, edgecolor='green', facecolor='none', linewidth=1.5) ax.add_artist(square)  # Initialize scatter plot and text scatter = ax.scatter([], [], s=0.1) text = ax.text(-1.4, -1.4, \"\", color=\"red\")  ax.arrow(0, 0, 1, 0, fc='r', ec = \"r\") ax.text(1.1, 0, 'r = 1')  # Function to update the plot with each batch of raindrops def update(i):     global x, y      # Add a new batch of raindrops     x = np.append(x, np.random.uniform(-1, 1, int(n_drops[i])))     y = np.append(y, np.random.uniform(-1, 1, int(n_drops[i])))      # Calculate estimated Pi     pi_estimate = 4 * (x**2 + y**2 &lt; 1).sum() / len(x)      # Update scatter plot data and Pi text     scatter.set_offsets(np.c_[x, y])     text.set_text(f\"n = {len(x)}\\n\" + r\"$\\pi \\approx$\" + f\"{pi_estimate:.3f}\")      return scatter, text  # Create animation anim = FuncAnimation(fig, update, frames=len(n_drops), blit=True)  # Save animation as GIF anim.save(\"raindrops_pi_estimate.gif\", writer=PillowWriter(fps=2))   In\u00a0[\u00a0]: Copied! <pre>from IPython.display import Image\nImage('raindrops_pi_estimate.gif')\n</pre> from IPython.display import Image Image('raindrops_pi_estimate.gif') Out[\u00a0]: <pre>&lt;IPython.core.display.Image object&gt;</pre>"},{"location":"Python/Calculating_pi/#calculating-pi","title":"Calculating $\\pi$\u00b6","text":"<p>Learning goals:</p> <ul> <li>Scientific Programming</li> <li>Working with <code>numpy</code></li> <li>Data visualization with <code>matplotlib</code></li> <li>Profiling and bottlenecks</li> </ul>"},{"location":"Python/Calculating_pi/#0-introduction","title":"0. Introduction\u00b6","text":"<p>In this notebook, we will learn how to calculate $\\pi$ using Monte Carlo methods.</p> <p>We'll be simulating raindrops on a tile to determine the value of $\\pi$. In a Monte Carlo simulation, we use random numbers within our simulation to try and understand some underlying relationship or phenomenon. Let's start discussing the problem:</p>"},{"location":"Python/Calculating_pi/#01-the-setup","title":"0.1 The Setup\u00b6","text":"<p>Let's imagine a flat square tile outside on a rainy day. For simplicity, let's call the width and length of the square to be 2. Now, let's say we draw a circle within the square such that the radius of the circle is 1 (i.e., its diameter is equal to 2, or the length of the square). The areas of the two shapes are:</p> <p>$$ A_{square} = l \\times w = 2 \\times 2 = 4 $$ $$ A_{circle} = \\pi r^2 = \\pi (1)^2 = \\pi $$</p> <p>If we were to count the number of raindrops that fall in the square and in the circle, we can use the ratio of the two to estimate $\\pi$:</p> <p>$$ \\frac{A_{circle}}{A_{square}} = \\frac{\\pi}{4}, $$</p> <p>or</p> <p>$$ \\pi = 4 \\times \\frac{A_{circle}}{A_{square}}. $$</p> <p>This holds true as the number of raindrops increases to infinity. In general, we can say:</p> <p>$$ \\pi \\approx 4 \\times \\frac{A_{circle}}{A_{square}}. $$</p> <p>We'll aim to reproduce the above equation using a Monte Carlo simulation.</p> <p>Let's visualize the setup:</p>"},{"location":"Python/Calculating_pi/#1-visualizing-the-problem","title":"1. Visualizing the problem\u00b6","text":"<p>We'll use <code>matplotlib</code> to visualize the problem.</p>"},{"location":"Python/Calculating_pi/#2-generating-random-numbers","title":"2. Generating Random Numbers\u00b6","text":"<p>We can generate random numbers using <code>numpy</code>. <code>numpy</code> has numerous methods for generating random numbers.</p> <p>Typically, we'll want our random numbers to mimic some sort of underlying distribution. Let's look at a few of <code>numpy</code>'s built-in random number generators.</p>"},{"location":"Python/Calculating_pi/#21-lets-visualize-the-randomness","title":"2.1 Let's Visualize the Randomness!\u00b6","text":"<p>Let's use <code>matplotlib</code> to create a multi-panel plot.</p>"},{"location":"Python/Calculating_pi/#22-generate-random-numbers-following-an-arbitrary-distribution","title":"2.2 Generate Random Numbers Following an Arbitrary Distribution\u00b6","text":"<p>It's often the case that the distribution we want to simulate doesn't follow a predefined distribution. In these cases, we can still generate random numbers following that distribution using the Cumulative Distribution Function (CDF).</p> <p>Let's start off by looking at an arbitrary distribution:</p>"},{"location":"Python/Calculating_pi/#3-getting-back-to-pi","title":"3. Getting back to $\\pi$\u00b6","text":"<p>Now that we know how to generate random numbers, how might we use this to calculate $\\pi$?</p> <p>Hint:</p> <ul> <li>Let's assume that the rain falling can be modeled using a Uniform distribution</li> </ul>"},{"location":"Python/Calculating_pi/#31-getting-an-accurate-result","title":"3.1 Getting an accurate result\u00b6","text":"<p>We can start by increasing the number of rain drops. Let's look at how long it takes to run the simulation for different numbers of rain drops. To do this we'll use the <code>time</code> module and some \"python magic\"</p>"},{"location":"Python/Calculating_pi/#4-performance-and-benchmarking","title":"4. Performance and Benchmarking\u00b6","text":"<p>When we're trying to improve our code it's important to set metrics that we want to improve on. Such metrics might be:</p> <ul> <li>The overall runtime</li> <li>The resource usage (e.g. memory, CPU)</li> <li>The accuracy of the results</li> </ul> <p>In reality improving one of the above might worsen the others. For example, we've already looked at the accuracy of the result. To improve the accuracy we needed to increase the number of rain drops. This means that we need to increase the runtime, but also to increase the memory requirements (as we've need to store more random numbers!).</p> <p>Optimizing our code is often a balancing act between competing interests.</p> <p>In this section we'll look at ways to examine the performance of our function.</p>"},{"location":"Python/Calculating_pi/#41-line-profiling","title":"4.1 Line Profiling\u00b6","text":"<p>Now that we have an idea of the runtime let's see where in the code most of the time is spent.</p> <p>Line profiling allows us to estimate the runtime of each line of code. This helps identify bottlenecks and where to focus our efforts when optimizing our code.</p> <p>For this we'll use <code>line_profiler</code> but there are others:</p> <ul> <li>Line Profiler</li> <li>PProfile</li> <li>CProfile</li> </ul> <p>As well as some awesome visulaiztion tools:</p> <ul> <li>Snakeviz</li> <li>Flamegraph</li> </ul>"},{"location":"Python/Calculating_pi/#41-removing-the-for-loop","title":"4.1 Removing the for loop\u00b6","text":"<p>We can remove the for loop using <code>numpy</code> array's masking. Let's look at the following:</p>"},{"location":"Python/Calculating_pi/#42-pre-compiling-our-code-with-numba","title":"4.2 Pre-compiling our code with Numba\u00b6","text":"<p>We can use <code>numba</code> to \"compile\" the code. When compiling the code numba will perform some optimization and remove the step of converting the python script to machine code. This is done using the <code>@jit</code> decorator.</p> <p><code>jit</code> stands for \"just-in-time\" compiling. This means that the first time this function is called, it will be compiled. So the first time it runs, it will take longer than usual, but after that, the runtime should be improved:</p>"},{"location":"Python/Calculating_pi/#5-creating-an-nice-visualization","title":"5. Creating an nice visualization\u00b6","text":"<p>Now that we have well-performing code, let's wrap everything together to make a nice visualization to communicate our findings.</p>"},{"location":"Python/GaiaTutorial-Empty/","title":"Table of contents","text":"<p> GaiaTutorial-Empty.ipynb</p> <p>The data for this tutorial can be downloaded from: here or here</p> <p>Top of the page </p> In\u00a0[1]: Copied! <pre>import astroquery\n</pre> import astroquery <p>Top of the page</p> In\u00a0[2]: Copied! <pre>from astroquery.gaia import Gaia\n</pre> from astroquery.gaia import Gaia <p>In astronomy, there are often vast databases filled with valuable information, and sometimes, we might not know exactly what we're looking for. That's where the ability to explore and discover available database tables becomes essential.</p> In\u00a0[\u00a0]: Copied! <pre>tables = Gaia.load_tables()\n</pre> tables = Gaia.load_tables() <pre>INFO: Retrieving tables... [astroquery.utils.tap.core]\nINFO: Parsing tables... [astroquery.utils.tap.core]\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>We're interested in the \"gaiadr3.gaia_source\" table, this is entry 88. We can also see what columns are included in this table:</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>Top of the page</p> <p></p> In\u00a0[\u00a0]: Copied! <pre># import pandas as pd\n# query_size = 1000000  # Number of stars we want to get\n# distance = 200  # Distance (in pc) out to which we will query\n# job = Gaia.launch_job_async(\"select top {}\".format(query_size)+\n#                 \" ra, dec, parallax, parallax_over_error, \"   # Getting source location and parallax\n#                 \" bp_rp, phot_g_mean_mag \"                    # Getting color and magnitude measurements\n#                 \" from gaiadr3.gaia_source\"                   # Selecting the data source\n#                 # All of these are data quality checks\n#                 \" where parallax_over_error &gt; 10\"\n#                 \" and visibility_periods_used &gt; 8\"\n#                 \" and phot_g_mean_flux_over_error &gt; 50\"\n#                 \" and phot_bp_mean_flux_over_error &gt; 20\"\n#                 \" and phot_rp_mean_flux_over_error &gt; 20\"\n#                 \" and phot_bp_rp_excess_factor &lt;\"\n#                 \" 1.3+0.06*power(phot_bp_mean_mag-phot_rp_mean_mag,2)\"\n#                 \" and phot_bp_rp_excess_factor &gt;\"\n#                 \" 1.0+0.015*power(phot_bp_mean_mag-phot_rp_mean_mag,2)\"\n#                 \" and astrometric_chi2_al/(astrometric_n_good_obs_al-5)&lt;\"\n#                 \"1.44*greatest(1,exp(-0.4*(phot_g_mean_mag-19.5)))\"\n#                 # Filtering on distance\n#                 +\" and 1000/parallax &lt;= {}\".format(distance))\n\n\n# r = job.get_results()\n# # Convert to pandas\n# df = r.to_pandas()\n# # Save to a csv\n# df.to_csv(\"gaia3.csv\")\n</pre> # import pandas as pd # query_size = 1000000  # Number of stars we want to get # distance = 200  # Distance (in pc) out to which we will query # job = Gaia.launch_job_async(\"select top {}\".format(query_size)+ #                 \" ra, dec, parallax, parallax_over_error, \"   # Getting source location and parallax #                 \" bp_rp, phot_g_mean_mag \"                    # Getting color and magnitude measurements #                 \" from gaiadr3.gaia_source\"                   # Selecting the data source #                 # All of these are data quality checks #                 \" where parallax_over_error &gt; 10\" #                 \" and visibility_periods_used &gt; 8\" #                 \" and phot_g_mean_flux_over_error &gt; 50\" #                 \" and phot_bp_mean_flux_over_error &gt; 20\" #                 \" and phot_rp_mean_flux_over_error &gt; 20\" #                 \" and phot_bp_rp_excess_factor &lt;\" #                 \" 1.3+0.06*power(phot_bp_mean_mag-phot_rp_mean_mag,2)\" #                 \" and phot_bp_rp_excess_factor &gt;\" #                 \" 1.0+0.015*power(phot_bp_mean_mag-phot_rp_mean_mag,2)\" #                 \" and astrometric_chi2_al/(astrometric_n_good_obs_al-5)&lt;\" #                 \"1.44*greatest(1,exp(-0.4*(phot_g_mean_mag-19.5)))\" #                 # Filtering on distance #                 +\" and 1000/parallax &lt;= {}\".format(distance))   # r = job.get_results() # # Convert to pandas # df = r.to_pandas() # # Save to a csv # df.to_csv(\"gaia3.csv\") <p>Top of the page</p> <p></p> In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n# Read in a csv file with pandas\n</pre> import pandas as pd # Read in a csv file with pandas  In\u00a0[\u00a0]: Copied! <pre># use head() to see the first few entries\n</pre> # use head() to see the first few entries In\u00a0[\u00a0]: Copied! <pre># use describe to see some useful stats\n</pre> # use describe to see some useful stats <p>Top of the page</p> <p></p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nfrom astropy import units as u\nfrom astropy.coordinates import Distance\n</pre> import numpy as np from astropy import units as u from astropy.coordinates import Distance In\u00a0[\u00a0]: Copied! <pre># Use astropy to get the distance to stars\ndef get_distance(parallax):\n</pre> # Use astropy to get the distance to stars def get_distance(parallax):  In\u00a0[\u00a0]: Copied! <pre># Calculate the distances to the stars\nstar_distances =\n</pre> # Calculate the distances to the stars star_distances =   In\u00a0[\u00a0]: Copied! <pre># Get the minimum and maximum of the distances\nmin_distance = \nmax_distance = \nprint (min_distance, max_distance)\n</pre> # Get the minimum and maximum of the distances min_distance =  max_distance =  print (min_distance, max_distance) <p>So, let's recap our progress:</p> <ul> <li>We utilized Astroquery to retrieve data from the Gaia mission, obtaining essential information about stars.</li> <li>With the help of Astropy, we handled the conversion of parallax angles from milliarcseconds to arcseconds and subsequently calculated the distances to these stars in parsecs.</li> </ul> <p>The outputted distances are indicated with <code>pc</code> after the numeric value, signifying the unit of measurement. Astropy makes it easy to work with these units and perform conversions as needed.</p> In\u00a0[\u00a0]: Copied! <pre># Convert the distances to light years, meters and inches\n</pre> # Convert the distances to light years, meters and inches  <p>Converting between units with Astropy is as simple as using <code>.to(\"unit\")</code>. This not only saves time but also minimizes the risk of human errors when performing unit conversions.</p> <p>Top of the page</p> <p></p> In\u00a0[\u00a0]: Copied! <pre># Calculate the absolute magnitude of the star\ndef calculate_absolute_magnitude(apparent_magnitude, distance):\n</pre> # Calculate the absolute magnitude of the star def calculate_absolute_magnitude(apparent_magnitude, distance):  In\u00a0[\u00a0]: Copied! <pre>AbsM =\n</pre> AbsM =  <p>Let's add the values we've determinded back into out data frame</p> In\u00a0[\u00a0]: Copied! <pre># Add the values to our dataset\ndf[\"AbsM\"] = AbsM\ndf[\"Distance\"] = star_distances.value\n</pre> # Add the values to our dataset df[\"AbsM\"] = AbsM df[\"Distance\"] = star_distances.value In\u00a0[\u00a0]: Copied! <pre>df.head()\n</pre> df.head() <p>Top of the page</p> <p></p> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n</pre> import matplotlib.pyplot as plt In\u00a0[\u00a0]: Copied! <pre># Get the columns to plot (excluding the first column, possibly an index or label)\ncols = \n\n# Calculate the number of rows and columns for subplots\nnrows = 2\nncols = \n\n# Create a figure with subplots\nfig, axs = plt.subplots(nrows, ncols, figsize=(24, 6))\n\n# Flatten the axs array to loop through the subplots\nfor i, ax in enumerate(axs.ravel()):\n    # Check if there are more columns to plot\n    if i &lt; len(cols):\n\n\n# Adjust subplot layout for better spacing\nplt.tight_layout()\nplt.show()\n</pre>  # Get the columns to plot (excluding the first column, possibly an index or label) cols =   # Calculate the number of rows and columns for subplots nrows = 2 ncols =   # Create a figure with subplots fig, axs = plt.subplots(nrows, ncols, figsize=(24, 6))  # Flatten the axs array to loop through the subplots for i, ax in enumerate(axs.ravel()):     # Check if there are more columns to plot     if i &lt; len(cols):   # Adjust subplot layout for better spacing plt.tight_layout() plt.show() <p>seaborn also has a lof of useful plotting capbabilities</p> In\u00a0[\u00a0]: Copied! <pre>import seaborn as sns\n</pre> import seaborn as sns In\u00a0[\u00a0]: Copied! <pre># Create a figure with subplots using Seaborn\nfig, axs = plt.subplots(nrows, ncols, figsize=(24, 6))\n\n# number of points to plot\nnpoints = 100000\n\n# Flatten the axs array to loop through the subplots\nfor i, ax in enumerate(axs.ravel()):\n    # Check if there are more columns to plot\n    if i &lt; len(cols):\n\n\n# Adjust subplot layout for better spacing\nplt.tight_layout()\nplt.show()\n</pre> # Create a figure with subplots using Seaborn fig, axs = plt.subplots(nrows, ncols, figsize=(24, 6))  # number of points to plot npoints = 100000  # Flatten the axs array to loop through the subplots for i, ax in enumerate(axs.ravel()):     # Check if there are more columns to plot     if i &lt; len(cols):   # Adjust subplot layout for better spacing plt.tight_layout() plt.show()  <p>Top of the page</p> <p></p> In\u00a0[\u00a0]: Copied! <pre># Calculate the correlation matrix\ncormat = \n\n# Set the colormap and color range\ncmap = sns.color_palette(\"coolwarm\", as_cmap=True)  # You can change the colormap as needed\nvmin, vmax = -1, 1  # Set the range for the color scale\n\n# Create the correlation heatmap\nplt.figure(figsize=(10, 8))  # Set the figure size\n\n\n# Set plot title\nplt.title(\"Correlation Heatmap\")\n\n# Show the plot\nplt.show()\n</pre>  # Calculate the correlation matrix cormat =   # Set the colormap and color range cmap = sns.color_palette(\"coolwarm\", as_cmap=True)  # You can change the colormap as needed vmin, vmax = -1, 1  # Set the range for the color scale  # Create the correlation heatmap plt.figure(figsize=(10, 8))  # Set the figure size   # Set plot title plt.title(\"Correlation Heatmap\")  # Show the plot plt.show()  <p>With just a few lines of code, we can quickly discern significant insights from a correlation plot. We observe a clear correlation between the distance (<code>Distance</code>) and the brightness, as indicated by the <code>phot_g_mean_mag</code> value. However, this correlation disappears when we convert the brightness to Absolute Magnitude (<code>AbsM</code>).</p> <p>Top of the page</p> <p></p> In\u00a0[\u00a0]: Copied! <pre># Create a scatter plot of bp_rp vs AbsM\nax = plt.subplot()\n\n\n# Invert the y axis, smaller magnitude -&gt; Brighter star\nax.invert_yaxis()\n</pre> # Create a scatter plot of bp_rp vs AbsM ax = plt.subplot()   # Invert the y axis, smaller magnitude -&gt; Brighter star ax.invert_yaxis()   <p>While the scatter plot does reveal some underlying structure, it may not provide a highly informative representation of the data. The plot becomes saturated in high-density regions, making it challenging to discern details. To address this, we can use a histogram to visualize the density distribution more effectively.</p> In\u00a0[\u00a0]: Copied! <pre>ax = plt.subplot()\n\n# Create a 2D histogram\n# bins = 100, require 100 bins in the x and y directions\n# cmin = 10, require at least 10 entries in a bin to be shown\n# cmap = \"jet\", use the \"jet\" color map\n\nax.invert_yaxis()\n</pre> ax = plt.subplot()  # Create a 2D histogram # bins = 100, require 100 bins in the x and y directions # cmin = 10, require at least 10 entries in a bin to be shown # cmap = \"jet\", use the \"jet\" color map  ax.invert_yaxis() <p>This looks better, but we've made a critical error... Top of the page</p> <p></p> <p></p> <p>What information do you get from the above plot?</p> <p></p> <p>CMasheris a valuable Python package offering accessible scientific colormaps. All the color maps included in CMasher are designed to be colorblind-friendly and exhibit a linear increase in brightness.\"</p> In\u00a0[\u00a0]: Copied! <pre>import cmasher as cms\n</pre>  import cmasher as cms In\u00a0[\u00a0]: Copied! <pre>ax = plt.subplot()\n\n# Create a 2D histogram\n# bins = 100, require 100 bins in the x and y directions\n# cmin = 10, require at least 10 entries in a bin to be shown\n# cmap = \"jet\", use the \"jet\" color map\n\n\nax.invert_yaxis()\n</pre> ax = plt.subplot()  # Create a 2D histogram # bins = 100, require 100 bins in the x and y directions # cmin = 10, require at least 10 entries in a bin to be shown # cmap = \"jet\", use the \"jet\" color map   ax.invert_yaxis() <p>This looks better, but we still need to clean things up. The plot looks good, but a reader has no idea what is been plotted.</p> In\u00a0[\u00a0]: Copied! <pre># use colors to get a log color scale\nfrom matplotlib import colors\n\n\n# Create a subplot\nax = plt.subplot()\n\n# Scatter plot of data points with customizations\nax.scatter(df[\"bp_rp\"], df[\"AbsM\"], alpha=0.05, s=1, color='k', zorder=0)\n\n# Create a 2D histogram (density plot) with a log-normal color scale\n# - bins: Use 100 bins in both the x and y directions\n# - cmin: Require at least 10 entries in a bin to be shown\n# - cmap: Use the \"toxic\" color map from the cms module\n# - norm: Utilize a logarithmic color scale to better visualize data distribution\nh = ax.hist2d(df[\"bp_rp\"], df[\"AbsM\"], bins=100, cmin=50, norm=colors.LogNorm(), zorder=0.5, cmap=cms.toxic)\n\n\n# Invert the y-axis to match the typical appearance of a color-magnitude diagram\nax.invert_yaxis()\n\n# Add a color bar for the density plot\ncb = fig.colorbar(h[3], ax=ax, pad=0.02)\n\n# Set labels for the x and y axes\nax.set_xlabel(r'$G_{BP} - G_{RP}$')\nax.set_ylabel(r'$M_G$')\n\n# Set a label for the color bar\ncb.set_label(r\"$\\mathrm{Stellar~density}$\")\n\n# Add grid lines to the plot\nax.grid()\n</pre> # use colors to get a log color scale from matplotlib import colors   # Create a subplot ax = plt.subplot()  # Scatter plot of data points with customizations ax.scatter(df[\"bp_rp\"], df[\"AbsM\"], alpha=0.05, s=1, color='k', zorder=0)  # Create a 2D histogram (density plot) with a log-normal color scale # - bins: Use 100 bins in both the x and y directions # - cmin: Require at least 10 entries in a bin to be shown # - cmap: Use the \"toxic\" color map from the cms module # - norm: Utilize a logarithmic color scale to better visualize data distribution h = ax.hist2d(df[\"bp_rp\"], df[\"AbsM\"], bins=100, cmin=50, norm=colors.LogNorm(), zorder=0.5, cmap=cms.toxic)   # Invert the y-axis to match the typical appearance of a color-magnitude diagram ax.invert_yaxis()  # Add a color bar for the density plot cb = fig.colorbar(h[3], ax=ax, pad=0.02)  # Set labels for the x and y axes ax.set_xlabel(r'$G_{BP} - G_{RP}$') ax.set_ylabel(r'$M_G$')  # Set a label for the color bar cb.set_label(r\"$\\mathrm{Stellar~density}$\")  # Add grid lines to the plot ax.grid() <p>So, what insights can we gather from this plot?</p> <ul> <li>The plot's left side corresponds to 'blue,' while the right side corresponds to 'red.'</li> <li>The lower section of the plot represents 'dim' stars, whereas the upper section showcases 'bright' stars.</li> <li>The majority of stars form a discernible diagonal pattern, with dimmer stars appearing 'redder' and brighter stars appearing 'bluer.'</li> <li>Notably, we observe a distinct grouping of dim, blue stars in the bottom-left quadrant of the plot.</li> <li>There is a noticeable 'turn-off' branch where we encounter both bright blue and bright red stars.</li> <li>As we progress along this branch, the bright red stars appear to level off in brightness and become more 'red.'</li> <li>Stars residing in the bright red and dim blue regions are relatively scarce.</li> </ul> <p>Top of the page</p> <p></p> <p>In the realm of astronomy, we've uncovered one of the most profound tools: the Hertzsprung-Russell (HR) Diagram.</p> <p></p> <p>The HR Diagram serves as a stellar narrative, shedding light on the intricate lives of stars. It all commences with a star's birth, and its initial position on the diagram is determined by its mass. Stars embark on their cosmic journey along the 'Main Sequence,' where they sustain themselves by converting hydrogen into helium\u2014a process known as hydrogen burning. This phase constitutes the majority of a star's existence.</p> <p>However, as time advances, stars deplete their hydrogen fuel. When this occurs, they transition to the 'Giant' branch. Giants display a cooler surface temperature, giving them a 'reddish' appearance. Concurrently, they experience a considerable increase in luminosity, shining much brighter. This phenomenon is accompanied by a significant expansion in their physical size, leading to their classification as 'red giants.'</p> <p>Now, here's the fascinating twist: Stars like our Sun, after the exhaustion of helium, evolve into 'White Dwarfs.' This transformation arises from the fact that they lack the necessary mass to facilitate the fusion of elements heavier than helium. These stars have meticulously fused all their available helium into carbon. Unlike most stars that maintain a delicate equilibrium between gravitational forces and nuclear fusion, White Dwarfs are held in place by a phenomenon known as 'electron degeneracy.'</p> <p>In essence, the HR Diagram serves as a window into the diverse chapters of stellar existence, offering us valuable insights into the captivating stories of these cosmic marvels.</p> <p>Top of the page</p> <p></p> In\u00a0[\u00a0]: Copied! <pre>from astropy.coordinates import SkyCoord\n</pre> from astropy.coordinates import SkyCoord In\u00a0[\u00a0]: Copied! <pre># Use SkyCoord to convert coordinates\ncoord = SkyCoord(, unit=\"deg\")\n</pre> # Use SkyCoord to convert coordinates coord = SkyCoord(, unit=\"deg\")  In\u00a0[\u00a0]: Copied! <pre># Convert to galactic coordiates\ngal_coords = coord.galactic\n</pre> # Convert to galactic coordiates gal_coords = coord.galactic In\u00a0[\u00a0]: Copied! <pre>plt.subplot(111, projection='aitoff')\nplt.grid(True)\n# Create a scatter plot of all the stars\nplt.scatter(, alpha = 0.002)\n</pre> plt.subplot(111, projection='aitoff') plt.grid(True) # Create a scatter plot of all the stars plt.scatter(, alpha = 0.002)  In\u00a0[\u00a0]: Copied! <pre>def get_bin_centers(binning):\n    \"\"\"\n    Calculate the bin centers given a set of bin edges.\n\n    Parameters:\n    binning (numpy.ndarray): An array containing the bin edges.\n\n    Returns:\n    numpy.ndarray: An array of bin centers.\n\n    Example:\n    &gt;&gt;&gt; bin_edges = np.array([0, 1, 2, 3, 4, 5])\n    &gt;&gt;&gt; centers = get_bin_centers(bin_edges)\n    &gt;&gt;&gt; print(centers)\n    [0.5 1.5 2.5 3.5 4.5]\n    \"\"\"\n\n    # Calculate the width of each bin by subtracting the previous bin's edge from the current bin's edge\n    width = binning[1:] - binning[:-1]\n\n    # Calculate the bin centers by adding half of the bin width to the left bin edge\n    bin_centers = binning[:-1] + 0.5 * width\n\n    return bin_centers\n</pre> def get_bin_centers(binning):     \"\"\"     Calculate the bin centers given a set of bin edges.      Parameters:     binning (numpy.ndarray): An array containing the bin edges.      Returns:     numpy.ndarray: An array of bin centers.      Example:     &gt;&gt;&gt; bin_edges = np.array([0, 1, 2, 3, 4, 5])     &gt;&gt;&gt; centers = get_bin_centers(bin_edges)     &gt;&gt;&gt; print(centers)     [0.5 1.5 2.5 3.5 4.5]     \"\"\"      # Calculate the width of each bin by subtracting the previous bin's edge from the current bin's edge     width = binning[1:] - binning[:-1]      # Calculate the bin centers by adding half of the bin width to the left bin edge     bin_centers = binning[:-1] + 0.5 * width      return bin_centers  In\u00a0[\u00a0]: Copied! <pre># Define the number of bins and the bin edges for galactic longitude (l) and latitude (b)\nnbins = 100\nl_bins = np.linspace(-180, 180, nbins)\nb_bins = np.linspace(-90, 90, nbins)\n\n# Calculate the bin centers for galactic longitude (l) and latitude (b)\nl_center = get_bin_centers(l_bins)\nb_center = get_bin_centers(b_bins)\n\n# Calculate the 2D histogram (counts) of galactic coordinates l and b\n# Use np.histogram2d to create a 2D histogram from galactic coordinates\n# Wrap galactic longitude (l) at 180 degrees to ensure it covers the full range\ncounts, _, _ = np.histogram2d(\n    gal_coords.l.wrap_at('180d').radian,  # Convert and wrap l in radians\n    gal_coords.b.radian,                # Convert b in radians\n    bins=[np.deg2rad(l_bins), np.deg2rad(b_bins)]  # Convert bin edges to radians\n)\n</pre> # Define the number of bins and the bin edges for galactic longitude (l) and latitude (b) nbins = 100 l_bins = np.linspace(-180, 180, nbins) b_bins = np.linspace(-90, 90, nbins)  # Calculate the bin centers for galactic longitude (l) and latitude (b) l_center = get_bin_centers(l_bins) b_center = get_bin_centers(b_bins)  # Calculate the 2D histogram (counts) of galactic coordinates l and b # Use np.histogram2d to create a 2D histogram from galactic coordinates # Wrap galactic longitude (l) at 180 degrees to ensure it covers the full range counts, _, _ = np.histogram2d(     gal_coords.l.wrap_at('180d').radian,  # Convert and wrap l in radians     gal_coords.b.radian,                # Convert b in radians     bins=[np.deg2rad(l_bins), np.deg2rad(b_bins)]  # Convert bin edges to radians )  In\u00a0[\u00a0]: Copied! <pre># Create a subplot with an Aitoff projection for a celestial map\nax = plt.subplot(111, projection='aitoff')\n\n# Create a pseudo-color mesh plot (pcolormesh) for the stellar density map\n# Use np.deg2rad to convert bin centers from degrees to radians for the x and y axes\n# Use 'counts.T' to transpose the counts array for proper orientation\n# Apply a PowerNorm to enhance visualization (power=0.7)\n# Set zorder to control the layering of the plot\n# Use the 'savanna' color map from cms module\nh = ax.pcolormesh(np.deg2rad(l_center), np.deg2rad(b_center), counts.T, norm=colors.PowerNorm(0.7), zorder=0.5, cmap=cms.savanna)\n\n# Add a color bar for the density map\ncb = fig.colorbar(h, ax=ax, pad=0.02)\n\n# Set labels for the x and y axes\nax.set_xlabel(r'Galactic Longitude')\nax.set_ylabel(r'Galactic Latitude')\n\n# Set a label for the color bar\ncb.set_label(r\"$\\mathrm{Stellar~density}$\")\n\n# Add grid lines to the celestial map\nax.grid()\n</pre> # Create a subplot with an Aitoff projection for a celestial map ax = plt.subplot(111, projection='aitoff')  # Create a pseudo-color mesh plot (pcolormesh) for the stellar density map # Use np.deg2rad to convert bin centers from degrees to radians for the x and y axes # Use 'counts.T' to transpose the counts array for proper orientation # Apply a PowerNorm to enhance visualization (power=0.7) # Set zorder to control the layering of the plot # Use the 'savanna' color map from cms module h = ax.pcolormesh(np.deg2rad(l_center), np.deg2rad(b_center), counts.T, norm=colors.PowerNorm(0.7), zorder=0.5, cmap=cms.savanna)  # Add a color bar for the density map cb = fig.colorbar(h, ax=ax, pad=0.02)  # Set labels for the x and y axes ax.set_xlabel(r'Galactic Longitude') ax.set_ylabel(r'Galactic Latitude')  # Set a label for the color bar cb.set_label(r\"$\\mathrm{Stellar~density}$\")  # Add grid lines to the celestial map ax.grid()  <p>We observer that most stars fall alone a latitude of 0. This is the galactic plane. We also notice some spots of high stellar density. These are likely stellar clusters. Clusters of gravationally bound groups of stars.</p> <p>Top of the page</p> <p></p> In\u00a0[\u00a0]: Copied! <pre># Load in open cluster data\ndf_open_clusters =\n</pre> # Load in open cluster data df_open_clusters =  In\u00a0[\u00a0]: Copied! <pre>df_open_clusters.head()\n</pre> df_open_clusters.head() In\u00a0[\u00a0]: Copied! <pre># Filter the clusters within 200 pc\ndf_open_clusters_near =\n</pre> # Filter the clusters within 200 pc df_open_clusters_near =  In\u00a0[\u00a0]: Copied! <pre>df_open_clusters_near.head()\n</pre> df_open_clusters_near.head() In\u00a0[\u00a0]: Copied! <pre># Convert to Galactic Coordintes\ncluster_ra = df_open_clusters_near[\"Unnamed: 1_level_0\"]\ncluster_dec = df_open_clusters_near[\"Unnamed: 2_level_0\"]\ncluster_coords = \n\ndf_open_clusters_near[\"l\"] = \ndf_open_clusters_near[\"b\"] =\n</pre> # Convert to Galactic Coordintes cluster_ra = df_open_clusters_near[\"Unnamed: 1_level_0\"] cluster_dec = df_open_clusters_near[\"Unnamed: 2_level_0\"] cluster_coords =   df_open_clusters_near[\"l\"] =  df_open_clusters_near[\"b\"] =  In\u00a0[\u00a0]: Copied! <pre>df_open_clusters_near.head()\n</pre> df_open_clusters_near.head() In\u00a0[\u00a0]: Copied! <pre>sns.set_palette(\"colorblind\")\n\n# Create a subplot with an Aitoff projection for a celestial map\nax = plt.subplot(111, projection='aitoff')\n\n# Create a pseudo-color mesh plot (pcolormesh) for the stellar density map\n# Use np.deg2rad to convert bin centers from degrees to radians for the x and y axes\n# Use 'counts.T' to transpose the counts array for proper orientation\n# Apply a PowerNorm to enhance visualization (power=0.7)\n# Set zorder to control the layering of the plot\n# Use the 'savanna' color map from cms module\nh = ax.pcolormesh(np.deg2rad(l_center), np.deg2rad(b_center), counts.T, norm=colors.PowerNorm(0.7), zorder=0.5, cmap=cms.sepia)\n\n# Add a color bar for the density map\ncb = fig.colorbar(h, ax=ax, pad=0.02)\n\n# Set labels for the x and y axes\nax.set_xlabel(r'Galactic Longitude')\nax.set_ylabel(r'Galactic Latitude')\n\n# Set a label for the color bar\ncb.set_label(r\"$\\mathrm{Stellar~density}$\")\n\n\n# Set the number of brightest objects to overlay\nN = 3 # Adjust this to select the top N brightest objects\n\n# Sort the DataFrame by apparent magnitude (brightness)\nsorted_data = \n\n# Select the top N brightest objects\nbrightest_objects = \n\n# Create a scatter plot of galactic coordinates (l, b)\ncircle_size = 140\nfor i in range(len(brightest_objects)):\n    ax.scatter(\n        brightest_objects['l'].iloc[i], \n        brightest_objects['b'].iloc[i], \n        facecolor='none', edgecolor=f'C{i}',\n        s=circle_size,\n        label=brightest_objects['Cluster identifier'].iloc[i])\n\n\nax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2)  # Adjust the bbox_to_anchor to control the legend position\n# Add grid lines to the celestial map\nax.grid()\n</pre> sns.set_palette(\"colorblind\")  # Create a subplot with an Aitoff projection for a celestial map ax = plt.subplot(111, projection='aitoff')  # Create a pseudo-color mesh plot (pcolormesh) for the stellar density map # Use np.deg2rad to convert bin centers from degrees to radians for the x and y axes # Use 'counts.T' to transpose the counts array for proper orientation # Apply a PowerNorm to enhance visualization (power=0.7) # Set zorder to control the layering of the plot # Use the 'savanna' color map from cms module h = ax.pcolormesh(np.deg2rad(l_center), np.deg2rad(b_center), counts.T, norm=colors.PowerNorm(0.7), zorder=0.5, cmap=cms.sepia)  # Add a color bar for the density map cb = fig.colorbar(h, ax=ax, pad=0.02)  # Set labels for the x and y axes ax.set_xlabel(r'Galactic Longitude') ax.set_ylabel(r'Galactic Latitude')  # Set a label for the color bar cb.set_label(r\"$\\mathrm{Stellar~density}$\")   # Set the number of brightest objects to overlay N = 3 # Adjust this to select the top N brightest objects  # Sort the DataFrame by apparent magnitude (brightness) sorted_data =   # Select the top N brightest objects brightest_objects =   # Create a scatter plot of galactic coordinates (l, b) circle_size = 140 for i in range(len(brightest_objects)):     ax.scatter(         brightest_objects['l'].iloc[i],          brightest_objects['b'].iloc[i],          facecolor='none', edgecolor=f'C{i}',         s=circle_size,         label=brightest_objects['Cluster identifier'].iloc[i])   ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2)  # Adjust the bbox_to_anchor to control the legend position # Add grid lines to the celestial map ax.grid()  <p>Top of the page</p> <p></p> In\u00a0[\u00a0]: Copied! <pre>from sklearn.cluster import DBSCAN\n</pre> from sklearn.cluster import DBSCAN In\u00a0[\u00a0]: Copied! <pre>x = gal_coords.l.wrap_at('180d').radian\ny = gal_coords.b.radian\nz = np.log10(df[\"Distance\"])\ndata = np.array([x,y,z]).T\n\nnormalized_data = (data - data.mean(axis=0)) / data.std(axis=0)\n</pre> x = gal_coords.l.wrap_at('180d').radian y = gal_coords.b.radian z = np.log10(df[\"Distance\"]) data = np.array([x,y,z]).T  normalized_data = (data - data.mean(axis=0)) / data.std(axis=0)  In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Python/GaiaTutorial-Empty/#table-of-contents","title":"Table of contents\u00b6","text":"<ol> <li>Introduction of Gaia</li> <li>What is Astroquery</li> <li>Querying databases with Astroquery</li> <li>Working with Pandas Dataframes</li> <li>Calculating the Distance to Stars</li> <li>Converting units with Astropy</li> <li>Converting from apparent to absolute magnitude</li> <li>Exploratory Data Analysis with Pandas</li> <li>Understanding Correlations: Unraveling Relationships in Data</li> <li>Making Publication Quality Plots</li> <li>Understanding Color Maps and Accessibility</li> <li>The Hertzsprung-Russell Diagram</li> <li>Mapping Stellar Density in Galactic Coordinates</li> <li>Open Clusters</li> <li>Clustering</li> </ol>"},{"location":"Python/GaiaTutorial-Empty/#accessing-and-manipulating-astronomical-data-using-python","title":"Accessing and Manipulating Astronomical Data Using Python\u00b6","text":"<p>In this session, we will learn how to use the Python package Astroquery to access astronomical data. We will then utilize Astropy to simplify various calculations and unit conversions.</p> <p>Our primary focus will be on data obtained by the Gaia mission. Gaia is a European Space Agency mission with the goal of creating a 3D map of the Milky Way. Gaia's mission is to accurately measure the positions of over 1 billion stars.</p> <p> Image: Gaia - Exploring the Multi-dimensional Milky Way</p> <p>We will be working with data collected using Gaia's Blue and Red Photometers (BP &amp; RP) and making use of parallax measurements to estimate the distances to celestial objects.</p>"},{"location":"Python/GaiaTutorial-Empty/#what-is-astroquery","title":"What is Astroquery? \u00b6","text":"<p>Astroquery is a Python package that streamlines the process of querying astronomical databases and catalogs. It offers a wide range of options for accessing astronomical data. Some of the available options include:</p> <ul> <li>Querying databases of celestial objects.</li> <li>Retrieving data from sky surveys and telescopes.</li> <li>Accessing information from observatories and space missions.</li> </ul> <p> Image: A selection of options provided by Astroquery</p> <p>For our tutorial, we will focus on using Astroquery to work with data from the Gaia mission. To get started, let's import the Gaia module:</p>"},{"location":"Python/GaiaTutorial-Empty/#what-data-are-we-interseted-in","title":"What Data are we interseted in?\u00b6","text":"<p>We're interested in extracting specific data fields from the database, which will help us gain insights into celestial objects. Here's what we want to retrieve:</p> <ol> <li><p>Source Location:</p> <ul> <li><code>ra</code> (Right Ascension)</li> <li><code>dec</code> (Declination)</li> </ul> </li> <li><p>Color-Magnitude Diagram:</p> <ul> <li><code>bp_rp</code> (Blue-Red Color), which represents the difference between the Blue and Red band magnitudes.</li> </ul> </li> <li><p>Brightness of the Source:</p> <ul> <li><code>phot_g_mean_mag</code>, which is the average apparent G-band magnitude.</li> </ul> </li> <li><p>Distance Estimation:</p> <ul> <li><code>parallax</code>, which measures the apparent motion of the star due to the Earth's motion.</li> <li><code>parallax_over_error</code>, calculated as the ratio of parallax to the uncertainty on the parallax.</li> </ul> </li> </ol> <p>| Image: Measuring Stellar Distances by Parallax</p> <p>By combining these fields, we will construct a comprehensive query that will help us obtain the data we need. Additionally, it's a good practice to save the query results to a CSV file for future reference, so we won't need to rerun the query.</p>"},{"location":"Python/GaiaTutorial-Empty/#what-is-pandas","title":"What is pandas\u00b6","text":"<p>Pandas is a popular open-source data manipulation and analysis library for Python, providing versatile data structures and data analysis tools. A pandas <code>dataframe</code> is a two-dimensional, labeled data structure that resembles a table and is used for data manipulation, analysis, and exploration in Python.</p>"},{"location":"Python/GaiaTutorial-Empty/#how-do-we-calculate-the-distance-to-stars","title":"How Do We Calculate the Distance to Stars?\u00b6","text":"<p>Determining the distance to stars in astronomy involves trigonometry. We can calculate the distance ($d$) to a star using the formula:</p> <p>$$d = \\frac{1}{p},$$</p> <p>Here, $d$ is the distance in a unit called \"parsecs,\" and \"$p$\" is the parallax angle in arcseconds. It's important to note that the Gaia data records the parallax angle in units of milliarcseconds, so we need to perform a unit conversion. Top of the page</p> <p></p>"},{"location":"Python/GaiaTutorial-Empty/#how-do-we-convert-units","title":"How Do We Convert Units?\u00b6","text":"<p>To handle this unit conversion, we'll utilize <code>astropy.units</code>. To further simplify the process, we'll leverage Astropy not only for unit conversion but also for distance calculation. In addition, we'll use NumPy arrays to perform mathematical operations on large lists or arrays of numbers.</p>"},{"location":"Python/GaiaTutorial-Empty/#converting-from-apparent-magnitude-to-absolute-magnitude","title":"Converting from Apparent Magnitude to Absolute Magnitude\u00b6","text":"<p>When assessing the brightness of an astronomical object, it's crucial to consider the impact of distance on our measurements.</p> <p>The brightness of an object appears different depending on its proximity to us; it will appear brighter if it's closer and dimmer if it's farther away. In astronomy, we commonly use \"Apparent Magnitude\" to describe how bright an object appears from our vantage point and \"Absolute Magnitude\" to standardize brightness by accounting for the object's distance. Absolute Magnitude quantifies how bright an object would appear if it were located precisely 10 parsecs away.</p> <p> Image: Absolute Magnitude Concept</p> <p>By knowing the distance in parsecs, we can convert from apparent to absolute magnitude using the formula:</p> <p>$$M = m - 5 \\log_{10}(d_{pc}) + 5,$$</p> <p>Where:</p> <ul> <li>$M$ is the Absolute Magnitude.</li> <li>$m$ is the Apparent Magnitude.</li> <li>$d_{pc}$ is the distance in parsecs.</li> </ul> <p>This formula enables us to correct for the influence of distance when comparing the brightness of celestial objects.</p>"},{"location":"Python/GaiaTutorial-Empty/#exploratory-data-analysis-with-pandas","title":"Exploratory Data Analysis with Pandas\u00b6","text":"<p>Explore your dataset through the lens of Exploratory Data Analysis (EDA). EDA is all about unveiling hidden patterns and stories within your data. It's your detective work to discover insights, detect anomalies, and gain a deeper understanding of your dataset.</p> <p>We'll utilize pandas for data manipulation and Matplotlib/Seaborn for visualization. In this section, we'll delve into techniques for exploring your dataset, from basic properties to captivating visualizations. Welcome to the world of EDA!</p> <p>Some methods of EDA:</p> <ol> <li><p>Descriptive Statistics:</p> <ul> <li>Summary statistics</li> <li>Count of missing values</li> </ul> </li> <li><p>Data Distribution:</p> <ul> <li>Histograms</li> <li>Box plots</li> <li>Probability density functions (PDFs)</li> <li>Skewness and kurtosis</li> </ul> </li> <li><p>Data Relationships:</p> <ul> <li>Correlation matrix</li> <li>Scatter plots</li> <li>Heatmaps</li> <li>Pair plots</li> </ul> </li> <li><p>Categorical Data:</p> <ul> <li>Frequency counts</li> <li>Bar plots</li> <li>Cross-tabulations</li> </ul> </li> <li><p>Data Quality:</p> <ul> <li>Outliers</li> <li>Missing data</li> <li>Data consistency</li> </ul> </li> <li><p>Dimensionality Reduction:</p> <ul> <li>Principal Component Analysis (PCA)</li> <li>t-Distributed Stochastic Neighbor Embedding (t-SNE)</li> </ul> </li> <li><p>Data Visualization:</p> <ul> <li>Various plots and charts</li> </ul> </li> </ol>"},{"location":"Python/GaiaTutorial-Empty/#understanding-correlations-unraveling-relationships-in-data","title":"Understanding Correlations: Unraveling Relationships in Data\u00b6","text":"<p>In the realm of data analysis, uncovering the relationships and dependencies between different variables is a fundamental task. Correlations are the keys that unlock the doors to understanding the hidden patterns, associations, and causations within datasets. These intriguing statistical relationships offer a wealth of insights, guiding our understanding of how changes in one variable can impact another.</p> <p>Correlations enable us to answer questions such as:</p> <ul> <li>Does an increase in one variable lead to a corresponding increase or decrease in another?</li> <li>Are two variables completely independent, or is there a subtle connection waiting to be discovered?</li> <li>Which factors are most influential in affecting a specific outcome?</li> </ul> <p>With the correlation between two parameters (A and B) we can say:</p> <ul> <li>1, when  A increases so does B. These are said to be \"correalated\"</li> <li>0, when A increases we see no change in B. These are said to be \"uncorrelated\"</li> <li>-1, when A increase B decreases. These are said to be \"anit-correalted\"</li> </ul> <p>In the case of 1 and -1, we have 100% correlation and anti-correlation. We can have any value between these two numbers</p>"},{"location":"Python/GaiaTutorial-Empty/#making-publication-quality-plots","title":"Making Publication Quality Plots\u00b6","text":"<p>Now, let's proceed with building the color-magnitude diagram by plotting <code>bp_rp</code> against <code>AbsM</code>.</p> <p>Creating a high-quality scientific plot is essential for effectively conveying your data. A well-crafted plot should be designed to clearly communicate the key information to the viewer. It should include the following elements to enhance data understanding:</p> <ul> <li><p>Labelled Axes: Clearly annotated x and y axes provide context and meaning to the data.</p> </li> <li><p>Descriptive Title: A meaningful title summarizing the plot's content is crucial for quick comprehension.</p> </li> <li><p>Grid Lines: Grid lines assist in reading data values accurately and help viewers align data points.</p> </li> <li><p>Sensible Scaling: Appropriate axis scaling ensures that data is presented in a visually understandable manner without distortion.</p> </li> <li><p>Legend (if applicable): If multiple data series are displayed, a legend should explain their meaning.</p> </li> <li><p>Colorbar (if applicable): When using color to represent data values, a colorbar helps viewers interpret the color-coding.</p> </li> </ul> <p>By incorporating these elements, your scientific plot becomes a powerful tool for sharing insights and findings with your audience.</p>"},{"location":"Python/GaiaTutorial-Empty/#understanding-color-maps-and-accessibility","title":"Understanding Color Maps and Accessibility\u00b6","text":"<p>Color maps play a crucial role in data visualization, helping us represent data through a spectrum of colors. However, not all color maps are created equal, and some, like the \"jet\" color map, have raised concerns due to their perceptual issues.</p> <p>The \"jet\" color map, characterized by its vibrant and rainbow-like appearance, is notorious for its drawbacks, including:</p> <ul> <li><p>Perceptual Uniformity: It fails to maintain a perceptual uniformity, which means that equal steps in data values may not appear equally spaced in the color map, leading to misleading visualizations.</p> </li> <li><p>Color Saturation: The \"jet\" color map can saturate at high and low ends, making it challenging to distinguish different data values in these regions.</p> </li> <li><p>Colorblind Accessibility: Accessibility is a vital concern. Many individuals with color vision deficiencies, commonly referred to as colorblindness, may find it difficult to interpret data represented using color maps like \"jet.\"</p> </li> </ul> <p>To create effective and accessible visualizations, it's essential to consider the choice of color map and its impact on how data is perceived, especially in cases where colorblind individuals may interact with the visualizations.</p>"},{"location":"Python/GaiaTutorial-Empty/#the-hertzsprung-russell-diagram","title":"The Hertzsprung-Russell Diagram\u00b6","text":""},{"location":"Python/GaiaTutorial-Empty/#mapping-stellar-density-in-galactic-coordinates","title":"Mapping Stellar Density in Galactic Coordinates\u00b6","text":"<p>Our celestial journey continues as we leverage the rich data from the Gaia mission that we previously dissected. In this leg of our astronomical exploration, we're about to embark on an exciting mission: mapping the stellar density within our Milky Way galaxy.</p> <p>Armed with Gaia's invaluable data, we'll now venture into the realm of galactic coordinates. These coordinates provide us with a unique perspective, allowing us to chart the positions and densities of stars across the vast expanse of our galactic home.</p> <p>By harnessing the power of data analysis and visualization, we'll unveil a stellar density map that offers an intricate view of the distribution and concentration of stars within the Milky Way. Our map will not only guide us through the celestial neighborhoods but also shed light on the interplay between stars, clusters, and the galactic structures that shape our night sky.</p> <p>So, prepare to join us in this thrilling endeavor as we decode the secrets of the stars and craft a stellar density map in the wondrous realm of galactic coordinates.</p>"},{"location":"Python/GaiaTutorial-Empty/#open-clusters","title":"Open Clusters\u00b6","text":"<p>Open cluster: NGC 2164</p> <p>Open clusters are remarkable gatherings of young stars that form from the same cloud of interstellar gas and dust. These stellar congregations are a captivating feature of our galaxy, the Milky Way, and can be found in various regions of its spiral arms. Open clusters typically contain hundreds to thousands of stars.</p> <p>Open clusters provide insights into stellar evolution and the early stages of star formation. The young stars within open clusters often share similar ages, compositions, and distances, making them ideal laboratories for studying stellar properties and the effects of their environment on their development.</p> <p>Over time, open clusters gradually disperse due to gravitational interactions and tidal forces from the Milky Way. As a result, the stars within these clusters eventually go their separate ways, becoming part of the general stellar population of our galaxy.</p> <p>Studying open clusters offers valuable clues about the dynamics of our galaxy, the formation of stars, and the life cycles of stellar systems. Astronomers continue to explore and catalog these captivating celestial gatherings to deepen our understanding of the cosmos.</p>"},{"location":"Python/GaiaTutorial-Empty/#clustering","title":"Clustering\u00b6","text":"<p>It's noteworthy that three of the brightest nearby open clusters align with three of the most prominent hotspots on our stellar density map. This alignment isn't surprising, as a density map naturally highlights concentrations of stars, including known star clusters.</p> <p>Now, let's embark on the journey of exploring additional potential star clusters within our dataset. We'll employ a clustering algorithm known as DBSCAN. DBSCAN is a versatile algorithm capable of identifying clusters within data without prior knowledge of the number of clusters. Furthermore, it's designed to handle noise, such as random stars, ensuring that our focus remains on identifying genuine clusters without being perturbed by nearby stars.</p>"},{"location":"Python/GaiaTutorial-Empty/#how-did-you-find-this-tutorial","title":"How did you find this tutorial?\u00b6","text":"<p>We'd love to hear your thoughts! Please fill out this feedback form: https://docs.google.com/forms/d/e/1FAIpQLScoFM4j3HbkeKSkS2OVKwgvzhLLzTlxEJe9dyJN5ZgEQ3bXKA/viewform</p>"},{"location":"Python/GaiaTutorial/","title":"Table of contents","text":"<p>Top of the page </p> In\u00a0[1]: Copied! <pre>import astroquery\n</pre> import astroquery <p>Top of the page</p> In\u00a0[2]: Copied! <pre>from astroquery.gaia import Gaia\n</pre> from astroquery.gaia import Gaia <p>In astronomy, there are often vast databases filled with valuable information, and sometimes, we might not know exactly what we're looking for. That's where the ability to explore and discover available database tables becomes essential.</p> In\u00a0[3]: Copied! <pre>tables = Gaia.load_tables()\n</pre> tables = Gaia.load_tables() <pre>INFO: Retrieving tables... [astroquery.utils.tap.core]\nINFO: Parsing tables... [astroquery.utils.tap.core]\nINFO: Done. [astroquery.utils.tap.core]\n</pre> In\u00a0[4]: Copied! <pre>for i, tab in enumerate(tables):\n    print (i, tab.name)\n</pre> for i, tab in enumerate(tables):     print (i, tab.name) <pre>0 external.apassdr9\n1 external.gaiadr2_astrophysical_parameters\n2 external.gaiadr2_geometric_distance\n3 external.gaiaedr3_distance\n4 external.gaiaedr3_gcns_main_1\n5 external.gaiaedr3_gcns_rejected_1\n6 external.gaiaedr3_spurious\n7 external.galex_ais\n8 external.ravedr5_com\n9 external.ravedr5_dr5\n10 external.ravedr5_gra\n11 external.ravedr5_on\n12 external.ravedr6\n13 external.sdssdr13_photoprimary\n14 external.skymapperdr1_master\n15 external.skymapperdr2_master\n16 external.tmass_xsc\n17 gaiadr1.aux_qso_icrf2_match\n18 gaiadr1.ext_phot_zero_point\n19 gaiadr1.allwise_best_neighbour\n20 gaiadr1.allwise_neighbourhood\n21 gaiadr1.gsc23_best_neighbour\n22 gaiadr1.gsc23_neighbourhood\n23 gaiadr1.ppmxl_best_neighbour\n24 gaiadr1.ppmxl_neighbourhood\n25 gaiadr1.sdss_dr9_best_neighbour\n26 gaiadr1.sdss_dr9_neighbourhood\n27 gaiadr1.tmass_best_neighbour\n28 gaiadr1.tmass_neighbourhood\n29 gaiadr1.ucac4_best_neighbour\n30 gaiadr1.ucac4_neighbourhood\n31 gaiadr1.urat1_best_neighbour\n32 gaiadr1.urat1_neighbourhood\n33 gaiadr1.cepheid\n34 gaiadr1.phot_variable_time_series_gfov\n35 gaiadr1.phot_variable_time_series_gfov_statistical_parameters\n36 gaiadr1.rrlyrae\n37 gaiadr1.variable_summary\n38 gaiadr1.allwise_original_valid\n39 gaiadr1.gsc23_original_valid\n40 gaiadr1.ppmxl_original_valid\n41 gaiadr1.sdssdr9_original_valid\n42 gaiadr1.tmass_original_valid\n43 gaiadr1.ucac4_original_valid\n44 gaiadr1.urat1_original_valid\n45 gaiadr1.gaia_source\n46 gaiadr1.tgas_source\n47 gaiadr2.aux_allwise_agn_gdr2_cross_id\n48 gaiadr2.aux_iers_gdr2_cross_id\n49 gaiadr2.aux_sso_orbit_residuals\n50 gaiadr2.aux_sso_orbits\n51 gaiadr2.dr1_neighbourhood\n52 gaiadr2.allwise_best_neighbour\n53 gaiadr2.allwise_neighbourhood\n54 gaiadr2.apassdr9_best_neighbour\n55 gaiadr2.apassdr9_neighbourhood\n56 gaiadr2.gsc23_best_neighbour\n57 gaiadr2.gsc23_neighbourhood\n58 gaiadr2.hipparcos2_best_neighbour\n59 gaiadr2.hipparcos2_neighbourhood\n60 gaiadr2.panstarrs1_best_neighbour\n61 gaiadr2.panstarrs1_neighbourhood\n62 gaiadr2.ppmxl_best_neighbour\n63 gaiadr2.ppmxl_neighbourhood\n64 gaiadr2.ravedr5_best_neighbour\n65 gaiadr2.ravedr5_neighbourhood\n66 gaiadr2.sdssdr9_best_neighbour\n67 gaiadr2.sdssdr9_neighbourhood\n68 gaiadr2.tmass_best_neighbour\n69 gaiadr2.tmass_neighbourhood\n70 gaiadr2.tycho2_best_neighbour\n71 gaiadr2.tycho2_neighbourhood\n72 gaiadr2.urat1_best_neighbour\n73 gaiadr2.urat1_neighbourhood\n74 gaiadr2.sso_observation\n75 gaiadr2.sso_source\n76 gaiadr2.vari_cepheid\n77 gaiadr2.vari_classifier_class_definition\n78 gaiadr2.vari_classifier_definition\n79 gaiadr2.vari_classifier_result\n80 gaiadr2.vari_long_period_variable\n81 gaiadr2.vari_rotation_modulation\n82 gaiadr2.vari_rrlyrae\n83 gaiadr2.vari_short_timescale\n84 gaiadr2.vari_time_series_statistics\n85 gaiadr2.panstarrs1_original_valid\n86 gaiadr2.gaia_source\n87 gaiadr2.ruwe\n88 gaiadr3.gaia_source\n89 gaiadr3.gaia_source_lite\n90 gaiadr3.astrophysical_parameters\n91 gaiadr3.astrophysical_parameters_supp\n92 gaiadr3.oa_neuron_information\n93 gaiadr3.oa_neuron_xp_spectra\n94 gaiadr3.total_galactic_extinction_map\n95 gaiadr3.total_galactic_extinction_map_opt\n96 gaiadr3.commanded_scan_law\n97 gaiadr3.allwise_best_neighbour\n98 gaiadr3.allwise_neighbourhood\n99 gaiadr3.apassdr9_best_neighbour\n100 gaiadr3.apassdr9_join\n101 gaiadr3.apassdr9_neighbourhood\n102 gaiadr3.dr2_neighbourhood\n103 gaiadr3.gsc23_best_neighbour\n104 gaiadr3.gsc23_join\n105 gaiadr3.gsc23_neighbourhood\n106 gaiadr3.hipparcos2_best_neighbour\n107 gaiadr3.hipparcos2_neighbourhood\n108 gaiadr3.panstarrs1_best_neighbour\n109 gaiadr3.panstarrs1_join\n110 gaiadr3.panstarrs1_neighbourhood\n111 gaiadr3.ravedr5_best_neighbour\n112 gaiadr3.ravedr5_join\n113 gaiadr3.ravedr5_neighbourhood\n114 gaiadr3.ravedr6_best_neighbour\n115 gaiadr3.ravedr6_join\n116 gaiadr3.ravedr6_neighbourhood\n117 gaiadr3.sdssdr13_best_neighbour\n118 gaiadr3.sdssdr13_join\n119 gaiadr3.sdssdr13_neighbourhood\n120 gaiadr3.skymapperdr2_best_neighbour\n121 gaiadr3.skymapperdr2_join\n122 gaiadr3.skymapperdr2_neighbourhood\n123 gaiadr3.tmass_psc_xsc_best_neighbour\n124 gaiadr3.tmass_psc_xsc_join\n125 gaiadr3.tmass_psc_xsc_neighbourhood\n126 gaiadr3.tycho2tdsc_merge_best_neighbour\n127 gaiadr3.tycho2tdsc_merge_neighbourhood\n128 gaiadr3.urat1_best_neighbour\n129 gaiadr3.urat1_neighbourhood\n130 gaiadr3.galaxy_candidates\n131 gaiadr3.galaxy_catalogue_name\n132 gaiadr3.qso_candidates\n133 gaiadr3.qso_catalogue_name\n134 gaiadr3.nss_acceleration_astro\n135 gaiadr3.nss_non_linear_spectro\n136 gaiadr3.nss_two_body_orbit\n137 gaiadr3.nss_vim_fl\n138 gaiadr3.binary_masses\n139 gaiadr3.chemical_cartography\n140 gaiadr3.gold_sample_carbon_stars\n141 gaiadr3.gold_sample_fgkm_stars\n142 gaiadr3.gold_sample_oba_stars\n143 gaiadr3.gold_sample_solar_analogues\n144 gaiadr3.gold_sample_spss\n145 gaiadr3.gold_sample_ucd\n146 gaiadr3.sso_orbits\n147 gaiadr3.synthetic_photometry_gspc\n148 gaiadr3.vari_spurious_signals\n149 gaiadr3.agn_cross_id\n150 gaiadr3.frame_rotator_source\n151 gaiadr3.gaia_crf3_xm\n152 gaiadr3.alerts_mixedin_sourceids\n153 gaiadr3.science_alerts\n154 gaiadr3.gaia_source_simulation\n155 gaiadr3.gaia_universe_model\n156 gaiadr3.sso_observation\n157 gaiadr3.sso_reflectance_spectrum\n158 gaiadr3.sso_source\n159 gaiadr3.xp_summary\n160 gaiadr3.vari_agn\n161 gaiadr3.vari_cepheid\n162 gaiadr3.vari_classifier_class_definition\n163 gaiadr3.vari_classifier_definition\n164 gaiadr3.vari_classifier_result\n165 gaiadr3.vari_compact_companion\n166 gaiadr3.vari_eclipsing_binary\n167 gaiadr3.vari_epoch_radial_velocity\n168 gaiadr3.vari_long_period_variable\n169 gaiadr3.vari_microlensing\n170 gaiadr3.vari_ms_oscillator\n171 gaiadr3.vari_planetary_transit\n172 gaiadr3.vari_planetary_transit_13june2022\n173 gaiadr3.vari_rad_vel_statistics\n174 gaiadr3.vari_rotation_modulation\n175 gaiadr3.vari_rrlyrae\n176 gaiadr3.vari_short_timescale\n177 gaiadr3.vari_summary\n178 gaiadr3.tycho2tdsc_merge\n179 gaiaedr3.gaia_source\n180 gaiaedr3.agn_cross_id\n181 gaiaedr3.commanded_scan_law\n182 gaiaedr3.dr2_neighbourhood\n183 gaiaedr3.frame_rotator_source\n184 gaiaedr3.allwise_best_neighbour\n185 gaiaedr3.allwise_neighbourhood\n186 gaiaedr3.apassdr9_best_neighbour\n187 gaiaedr3.apassdr9_join\n188 gaiaedr3.apassdr9_neighbourhood\n189 gaiaedr3.gsc23_best_neighbour\n190 gaiaedr3.gsc23_join\n191 gaiaedr3.gsc23_neighbourhood\n192 gaiaedr3.hipparcos2_best_neighbour\n193 gaiaedr3.hipparcos2_neighbourhood\n194 gaiaedr3.panstarrs1_best_neighbour\n195 gaiaedr3.panstarrs1_join\n196 gaiaedr3.panstarrs1_neighbourhood\n197 gaiaedr3.ravedr5_best_neighbour\n198 gaiaedr3.ravedr5_join\n199 gaiaedr3.ravedr5_neighbourhood\n200 gaiaedr3.sdssdr13_best_neighbour\n201 gaiaedr3.sdssdr13_join\n202 gaiaedr3.sdssdr13_neighbourhood\n203 gaiaedr3.skymapperdr2_best_neighbour\n204 gaiaedr3.skymapperdr2_join\n205 gaiaedr3.skymapperdr2_neighbourhood\n206 gaiaedr3.tmass_psc_xsc_best_neighbour\n207 gaiaedr3.tmass_psc_xsc_join\n208 gaiaedr3.tmass_psc_xsc_neighbourhood\n209 gaiaedr3.tycho2tdsc_merge_best_neighbour\n210 gaiaedr3.tycho2tdsc_merge_neighbourhood\n211 gaiaedr3.urat1_best_neighbour\n212 gaiaedr3.urat1_neighbourhood\n213 gaiaedr3.gaia_source_simulation\n214 gaiaedr3.gaia_universe_model\n215 gaiaedr3.tycho2tdsc_merge\n216 gaiafpr.crowded_field_source\n217 gaiafpr.lens_candidates\n218 gaiafpr.lens_catalogue_name\n219 gaiafpr.lens_observation\n220 gaiafpr.lens_outlier\n221 gaiafpr.sso_observation\n222 gaiafpr.sso_source\n223 gaiafpr.interstellar_medium_params\n224 gaiafpr.interstellar_medium_spectra\n225 gaiafpr.vari_epoch_radial_velocity\n226 gaiafpr.vari_long_period_variable\n227 gaiafpr.vari_rad_vel_statistics\n228 public.hipparcos\n229 public.hipparcos_newreduction\n230 public.hubble_sc\n231 public.igsl_source\n232 public.igsl_source_catalog_ids\n233 public.tycho2\n234 public.dual\n235 tap_config.coord_sys\n236 tap_config.properties\n237 tap_schema.columns\n238 tap_schema.key_columns\n239 tap_schema.keys\n240 tap_schema.schemas\n241 tap_schema.tables\n</pre> <p>We're interested in the \"gaiadr3.gaia_source\" table, this is entry 88. We can also see what columns are included in this table:</p> In\u00a0[5]: Copied! <pre>cols = tables[88].columns\nfor col in cols:\n    print (col.name)\n</pre> cols = tables[88].columns for col in cols:     print (col.name) <pre>solution_id\ndesignation\nsource_id\nrandom_index\nref_epoch\nra\nra_error\ndec\ndec_error\nparallax\nparallax_error\nparallax_over_error\npm\npmra\npmra_error\npmdec\npmdec_error\nra_dec_corr\nra_parallax_corr\nra_pmra_corr\nra_pmdec_corr\ndec_parallax_corr\ndec_pmra_corr\ndec_pmdec_corr\nparallax_pmra_corr\nparallax_pmdec_corr\npmra_pmdec_corr\nastrometric_n_obs_al\nastrometric_n_obs_ac\nastrometric_n_good_obs_al\nastrometric_n_bad_obs_al\nastrometric_gof_al\nastrometric_chi2_al\nastrometric_excess_noise\nastrometric_excess_noise_sig\nastrometric_params_solved\nastrometric_primary_flag\nnu_eff_used_in_astrometry\npseudocolour\npseudocolour_error\nra_pseudocolour_corr\ndec_pseudocolour_corr\nparallax_pseudocolour_corr\npmra_pseudocolour_corr\npmdec_pseudocolour_corr\nastrometric_matched_transits\nvisibility_periods_used\nastrometric_sigma5d_max\nmatched_transits\nnew_matched_transits\nmatched_transits_removed\nipd_gof_harmonic_amplitude\nipd_gof_harmonic_phase\nipd_frac_multi_peak\nipd_frac_odd_win\nruwe\nscan_direction_strength_k1\nscan_direction_strength_k2\nscan_direction_strength_k3\nscan_direction_strength_k4\nscan_direction_mean_k1\nscan_direction_mean_k2\nscan_direction_mean_k3\nscan_direction_mean_k4\nduplicated_source\nphot_g_n_obs\nphot_g_mean_flux\nphot_g_mean_flux_error\nphot_g_mean_flux_over_error\nphot_g_mean_mag\nphot_bp_n_obs\nphot_bp_mean_flux\nphot_bp_mean_flux_error\nphot_bp_mean_flux_over_error\nphot_bp_mean_mag\nphot_rp_n_obs\nphot_rp_mean_flux\nphot_rp_mean_flux_error\nphot_rp_mean_flux_over_error\nphot_rp_mean_mag\nphot_bp_rp_excess_factor\nphot_bp_n_contaminated_transits\nphot_bp_n_blended_transits\nphot_rp_n_contaminated_transits\nphot_rp_n_blended_transits\nphot_proc_mode\nbp_rp\nbp_g\ng_rp\nradial_velocity\nradial_velocity_error\nrv_method_used\nrv_nb_transits\nrv_nb_deblended_transits\nrv_visibility_periods_used\nrv_expected_sig_to_noise\nrv_renormalised_gof\nrv_chisq_pvalue\nrv_time_duration\nrv_amplitude_robust\nrv_template_teff\nrv_template_logg\nrv_template_fe_h\nrv_atm_param_origin\nvbroad\nvbroad_error\nvbroad_nb_transits\ngrvs_mag\ngrvs_mag_error\ngrvs_mag_nb_transits\nrvs_spec_sig_to_noise\nphot_variable_flag\nl\nb\necl_lon\necl_lat\nin_qso_candidates\nin_galaxy_candidates\nnon_single_star\nhas_xp_continuous\nhas_xp_sampled\nhas_rvs\nhas_epoch_photometry\nhas_epoch_rv\nhas_mcmc_gspphot\nhas_mcmc_msc\nin_andromeda_survey\nclassprob_dsc_combmod_quasar\nclassprob_dsc_combmod_galaxy\nclassprob_dsc_combmod_star\nteff_gspphot\nteff_gspphot_lower\nteff_gspphot_upper\nlogg_gspphot\nlogg_gspphot_lower\nlogg_gspphot_upper\nmh_gspphot\nmh_gspphot_lower\nmh_gspphot_upper\ndistance_gspphot\ndistance_gspphot_lower\ndistance_gspphot_upper\nazero_gspphot\nazero_gspphot_lower\nazero_gspphot_upper\nag_gspphot\nag_gspphot_lower\nag_gspphot_upper\nebpminrp_gspphot\nebpminrp_gspphot_lower\nebpminrp_gspphot_upper\nlibname_gspphot\n</pre> <p>Top of the page</p> <p></p> In\u00a0[6]: Copied! <pre># import pandas as pd\n# query_size = 1000000  # Number of stars we want to get\n# distance = 200  # Distance (in pc) out to which we will query\n# job = Gaia.launch_job_async(\"select top {}\".format(query_size)+\n#                 \" ra, dec, parallax, parallax_over_error, \"   # Getting source location and parallax\n#                 \" bp_rp, phot_g_mean_mag \"                    # Getting color and magnitude measurements\n#                 \" from gaiadr3.gaia_source\"                   # Selecting the data source\n#                 # All of these are data quality checks\n#                 \" where parallax_over_error &gt; 10\"\n#                 \" and visibility_periods_used &gt; 8\"\n#                 \" and phot_g_mean_flux_over_error &gt; 50\"\n#                 \" and phot_bp_mean_flux_over_error &gt; 20\"\n#                 \" and phot_rp_mean_flux_over_error &gt; 20\"\n#                 \" and phot_bp_rp_excess_factor &lt;\"\n#                 \" 1.3+0.06*power(phot_bp_mean_mag-phot_rp_mean_mag,2)\"\n#                 \" and phot_bp_rp_excess_factor &gt;\"\n#                 \" 1.0+0.015*power(phot_bp_mean_mag-phot_rp_mean_mag,2)\"\n#                 \" and astrometric_chi2_al/(astrometric_n_good_obs_al-5)&lt;\"\n#                 \"1.44*greatest(1,exp(-0.4*(phot_g_mean_mag-19.5)))\"\n#                 # Filtering on distance\n#                 +\" and 1000/parallax &lt;= {}\".format(distance))\n\n\n# r = job.get_results()\n# # Convert to pandas\n# df = r.to_pandas()\n# # Save to a csv\n# df.to_csv(\"gaia3.csv\")\n</pre> # import pandas as pd # query_size = 1000000  # Number of stars we want to get # distance = 200  # Distance (in pc) out to which we will query # job = Gaia.launch_job_async(\"select top {}\".format(query_size)+ #                 \" ra, dec, parallax, parallax_over_error, \"   # Getting source location and parallax #                 \" bp_rp, phot_g_mean_mag \"                    # Getting color and magnitude measurements #                 \" from gaiadr3.gaia_source\"                   # Selecting the data source #                 # All of these are data quality checks #                 \" where parallax_over_error &gt; 10\" #                 \" and visibility_periods_used &gt; 8\" #                 \" and phot_g_mean_flux_over_error &gt; 50\" #                 \" and phot_bp_mean_flux_over_error &gt; 20\" #                 \" and phot_rp_mean_flux_over_error &gt; 20\" #                 \" and phot_bp_rp_excess_factor &lt;\" #                 \" 1.3+0.06*power(phot_bp_mean_mag-phot_rp_mean_mag,2)\" #                 \" and phot_bp_rp_excess_factor &gt;\" #                 \" 1.0+0.015*power(phot_bp_mean_mag-phot_rp_mean_mag,2)\" #                 \" and astrometric_chi2_al/(astrometric_n_good_obs_al-5)&lt;\" #                 \"1.44*greatest(1,exp(-0.4*(phot_g_mean_mag-19.5)))\" #                 # Filtering on distance #                 +\" and 1000/parallax &lt;= {}\".format(distance))   # r = job.get_results() # # Convert to pandas # df = r.to_pandas() # # Save to a csv # df.to_csv(\"gaia3.csv\") <p>Top of the page</p> <p></p> In\u00a0[7]: Copied! <pre>import pandas as pd\ndf = pd.read_csv(\"gaia3.csv\")\n</pre> import pandas as pd df = pd.read_csv(\"gaia3.csv\") In\u00a0[8]: Copied! <pre>df.head()\n</pre> df.head() Out[8]: Unnamed: 0 ra dec parallax parallax_over_error bp_rp phot_g_mean_mag pm pmra pmra_error pmdec pmdec_error 0 0 251.492554 -50.749533 6.383756 103.153015 0.795505 10.546730 32.093870 31.380714 0.059630 -6.728077 0.045104 1 1 251.589977 -50.638635 13.398257 750.623540 2.165551 13.470652 98.008840 -49.358441 0.017024 -84.672770 0.012676 2 2 251.093139 -51.007072 6.106724 448.880800 1.269135 12.529914 9.986012 5.650984 0.013578 8.233275 0.009894 3 3 251.746461 -51.201058 7.481942 87.461440 2.588948 16.969418 147.466350 10.335827 0.085701 -147.103700 0.056509 4 4 250.188154 -51.273050 8.606533 163.012250 2.755577 16.339110 5.618203 -1.305274 0.064197 5.464473 0.043635 In\u00a0[9]: Copied! <pre>df.describe()\n</pre> df.describe() Out[9]: Unnamed: 0 ra dec parallax parallax_over_error bp_rp phot_g_mean_mag pm pmra pmra_error pmdec pmdec_error count 1.433622e+06 1.433622e+06 1.433622e+06 1.433622e+06 1.433622e+06 1.433622e+06 1.433622e+06 1.433622e+06 1.433622e+06 1.433622e+06 1.433622e+06 1.433622e+06 mean 7.168105e+05 1.826020e+02 -5.932620e-01 7.951749e+00 2.770547e+02 2.157298e+00 1.482734e+01 6.438331e+01 -1.552588e+00 5.269976e-02 -2.148792e+01 4.799410e-02 std 4.138512e+05 1.039087e+02 3.972358e+01 4.958763e+00 2.882675e+02 8.357065e-01 2.650869e+00 6.995515e+01 6.803307e+01 4.709206e-02 6.281956e+01 4.192370e-02 min 0.000000e+00 1.786275e-04 -8.991512e+01 5.000001e+00 1.001268e+01 -6.166201e-01 2.777715e+00 1.365079e-02 -4.406469e+03 4.317274e-03 -5.817800e+03 5.807723e-03 25% 3.584052e+05 9.176268e+01 -3.176953e+01 5.610729e+00 9.643445e+01 1.533804e+00 1.329365e+01 2.768513e+01 -2.899033e+01 2.056515e-02 -4.296366e+01 1.857064e-02 50% 7.168105e+05 1.847296e+02 -6.636379e-01 6.571446e+00 1.894774e+02 2.433110e+00 1.555917e+01 4.784237e+01 -1.827989e+00 3.864415e-02 -1.609957e+01 3.580896e-02 75% 1.075216e+06 2.738694e+02 3.016465e+01 8.494259e+00 3.672869e+02 2.789234e+00 1.682265e+01 7.966884e+01 2.575548e+01 7.004569e-02 6.412341e+00 6.389684e-02 max 1.433621e+06 3.599994e+02 8.994934e+01 7.680665e+02 1.540048e+04 5.435446e+00 2.058442e+01 1.039335e+04 6.765995e+03 2.072775e+00 1.036239e+04 2.114847e+00 <p>Top of the page</p> <p></p> In\u00a0[10]: Copied! <pre>import numpy as np\nfrom astropy import units as u\nfrom astropy.coordinates import Distance\n</pre> import numpy as np from astropy import units as u from astropy.coordinates import Distance In\u00a0[11]: Copied! <pre>def get_distance(parallax):\n    \"\"\"\n    Calculate the distance to a star using its parallax angle.\n\n    Parameters:\n    -----------\n    parallax : float\n        Parallax angle in milliarcseconds (mas).\n\n    Returns:\n    --------\n    astropy.coordinates.Distance\n        The distance to the star in parsecs (pc).\n    \n    Example:\n    --------\n    &gt;&gt;&gt; parallax = 12.34  # Parallax angle in mas\n    &gt;&gt;&gt; distance = get_distance(parallax)\n    &gt;&gt;&gt; print(distance)\n    81.15 pc\n    \"\"\"\n    # Convert parallax angle to milliarcseconds (mas)\n    p = parallax * u.mas\n    \n    # Calculate the distance using astropy's Distance module\n    distance = Distance(parallax=p)\n    \n    return distance\n</pre>  def get_distance(parallax):     \"\"\"     Calculate the distance to a star using its parallax angle.      Parameters:     -----------     parallax : float         Parallax angle in milliarcseconds (mas).      Returns:     --------     astropy.coordinates.Distance         The distance to the star in parsecs (pc).          Example:     --------     &gt;&gt;&gt; parallax = 12.34  # Parallax angle in mas     &gt;&gt;&gt; distance = get_distance(parallax)     &gt;&gt;&gt; print(distance)     81.15 pc     \"\"\"     # Convert parallax angle to milliarcseconds (mas)     p = parallax * u.mas          # Calculate the distance using astropy's Distance module     distance = Distance(parallax=p)          return distance  In\u00a0[12]: Copied! <pre>star_distances = get_distance( np.array(df[\"parallax\"]) )\n</pre> star_distances = get_distance( np.array(df[\"parallax\"]) )  In\u00a0[13]: Copied! <pre>min_distance = star_distances.min()\nmax_distance = star_distances.max()\nprint (min_distance, max_distance)\n</pre>  min_distance = star_distances.min() max_distance = star_distances.max() print (min_distance, max_distance) <pre>1.3019705311704333 pc 199.9999448214332 pc\n</pre> <p>So, let's recap our progress:</p> <ul> <li>We utilized Astroquery to retrieve data from the Gaia mission, obtaining essential information about stars.</li> <li>With the help of Astropy, we handled the conversion of parallax angles from milliarcseconds to arcseconds and subsequently calculated the distances to these stars in parsecs.</li> </ul> <p>The outputted distances are indicated with <code>pc</code> after the numeric value, signifying the unit of measurement. Astropy makes it easy to work with these units and perform conversions as needed.</p> In\u00a0[14]: Copied! <pre>print (f'Distance in light years {min_distance.to(\"lightyear\"):0.2f}')\nprint (f'Distance in meters {min_distance.to(\"m\"):0.2e}')\n# If we want to use imperial units we must enable it\n# (because we should always use metric!)\nu.imperial.enable()  \nprint (f'Distance in inches {min_distance.to(\"inch\"):0.2e}')\n</pre> print (f'Distance in light years {min_distance.to(\"lightyear\"):0.2f}') print (f'Distance in meters {min_distance.to(\"m\"):0.2e}') # If we want to use imperial units we must enable it # (because we should always use metric!) u.imperial.enable()   print (f'Distance in inches {min_distance.to(\"inch\"):0.2e}') <pre>Distance in light years 4.25 lyr\nDistance in meters 4.02e+16 m\nDistance in inches 1.58e+18 inch\n</pre> <p>Converting between units with Astropy is as simple as using <code>.to(\"unit\")</code>. This not only saves time but also minimizes the risk of human errors when performing unit conversions.</p> <p>Top of the page</p> <p></p> In\u00a0[15]: Copied! <pre>def calculate_absolute_magnitude(apparent_magnitude, distance):\n    \"\"\"\n    Calculate the Absolute Magnitude of an astronomical object given its Apparent Magnitude and distance.\n\n    Parameters:\n    -----------\n    apparent_magnitude : float\n        The Apparent Magnitude of the object.\n    \n    distance : astropy.units.Quantity\n        The distance to the object in parsecs (pc).\n\n    Returns:\n    --------\n    float\n        The Absolute Magnitude of the object.\n\n    Example:\n    --------\n    &gt;&gt;&gt; apparent_mag = 5.0  # Apparent Magnitude\n    &gt;&gt;&gt; distance = 10 * u.pc  # Distance in parsecs\n    &gt;&gt;&gt; absolute_mag = calculate_absolute_magnitude(apparent_mag, distance)\n    &gt;&gt;&gt; print(absolute_mag)\n    0.0\n    \"\"\"\n    # Convert distance to parsecs\n    dist_pc = distance.to(u.pc)\n    \n    # Calculate the Absolute Magnitude using the formula\n    M = apparent_magnitude - 5 * np.log10(dist_pc.value) + 5\n\n    return M\n</pre> def calculate_absolute_magnitude(apparent_magnitude, distance):     \"\"\"     Calculate the Absolute Magnitude of an astronomical object given its Apparent Magnitude and distance.      Parameters:     -----------     apparent_magnitude : float         The Apparent Magnitude of the object.          distance : astropy.units.Quantity         The distance to the object in parsecs (pc).      Returns:     --------     float         The Absolute Magnitude of the object.      Example:     --------     &gt;&gt;&gt; apparent_mag = 5.0  # Apparent Magnitude     &gt;&gt;&gt; distance = 10 * u.pc  # Distance in parsecs     &gt;&gt;&gt; absolute_mag = calculate_absolute_magnitude(apparent_mag, distance)     &gt;&gt;&gt; print(absolute_mag)     0.0     \"\"\"     # Convert distance to parsecs     dist_pc = distance.to(u.pc)          # Calculate the Absolute Magnitude using the formula     M = apparent_magnitude - 5 * np.log10(dist_pc.value) + 5      return M  In\u00a0[16]: Copied! <pre>AbsM = calculate_absolute_magnitude(np.array(df[\"phot_g_mean_mag\"]), star_distances)\n</pre> AbsM = calculate_absolute_magnitude(np.array(df[\"phot_g_mean_mag\"]), star_distances) <p>Let's add the values we've determinded back into out data frame</p> In\u00a0[17]: Copied! <pre>df[\"AbsM\"] = AbsM\ndf[\"Distance\"] = star_distances.value\n</pre> df[\"AbsM\"] = AbsM df[\"Distance\"] = star_distances.value In\u00a0[18]: Copied! <pre>df.head()\n</pre> df.head() Out[18]: Unnamed: 0 ra dec parallax parallax_over_error bp_rp phot_g_mean_mag pm pmra pmra_error pmdec pmdec_error AbsM Distance 0 0 251.492554 -50.749533 6.383756 103.153015 0.795505 10.546730 32.093870 31.380714 0.059630 -6.728077 0.045104 4.572111 156.647594 1 1 251.589977 -50.638635 13.398257 750.623540 2.165551 13.470652 98.008840 -49.358441 0.017024 -84.672770 0.012676 9.105894 74.636572 2 2 251.093139 -51.007072 6.106724 448.880800 1.269135 12.529914 9.986012 5.650984 0.013578 8.233275 0.009894 6.458955 163.753924 3 3 251.746461 -51.201058 7.481942 87.461440 2.588948 16.969418 147.466350 10.335827 0.085701 -147.103700 0.056509 11.339490 133.655144 4 4 250.188154 -51.273050 8.606533 163.012250 2.755577 16.339110 5.618203 -1.305274 0.064197 5.464473 0.043635 11.013251 116.190808 <p>Top of the page</p> <p></p> In\u00a0[19]: Copied! <pre>import matplotlib.pyplot as plt\n</pre> import matplotlib.pyplot as plt In\u00a0[20]: Copied! <pre># Get the columns to plot (excluding the first column, possibly an index or label)\ncols = df.columns[1:]\n\n# Calculate the number of rows and columns for subplots\nnrows = 2\nncols = (len(cols) + 1) // 2\n\n# Create a figure with subplots\nfig, axs = plt.subplots(nrows, ncols, figsize=(24, 6))\n\n# Flatten the axs array to loop through the subplots\nfor i, ax in enumerate(axs.ravel()):\n    # Check if there are more columns to plot\n    if i &lt; len(cols):\n        # Plot a histogram for the current column\n        df[cols[i]].plot(kind='hist', ax=ax)\n        ax.set_title(cols[i])\n\n# Adjust subplot layout for better spacing\nplt.tight_layout()\nplt.show()\n</pre>  # Get the columns to plot (excluding the first column, possibly an index or label) cols = df.columns[1:]  # Calculate the number of rows and columns for subplots nrows = 2 ncols = (len(cols) + 1) // 2  # Create a figure with subplots fig, axs = plt.subplots(nrows, ncols, figsize=(24, 6))  # Flatten the axs array to loop through the subplots for i, ax in enumerate(axs.ravel()):     # Check if there are more columns to plot     if i &lt; len(cols):         # Plot a histogram for the current column         df[cols[i]].plot(kind='hist', ax=ax)         ax.set_title(cols[i])  # Adjust subplot layout for better spacing plt.tight_layout() plt.show() <p>seaborn also has a lof of useful plotting capbabilities</p> In\u00a0[21]: Copied! <pre>import seaborn as sns\n</pre> import seaborn as sns In\u00a0[22]: Copied! <pre># Create a figure with subplots using Seaborn\nfig, axs = plt.subplots(nrows, ncols, figsize=(24, 6))\n\n# Flatten the axs array to loop through the subplots\nfor i, ax in enumerate(axs.ravel()):\n    # Check if there are more columns to plot\n    if i &lt; len(cols):\n        # Use Seaborn to create a histogram for the current column\n        sns.histplot(data=df[:100000], x=cols[i], ax=ax, kde=True)  # Set kde=True for a kernel density estimate\n        ax.set_title(cols[i])\n\n# Adjust subplot layout for better spacing\nplt.tight_layout()\nplt.show()\n</pre> # Create a figure with subplots using Seaborn fig, axs = plt.subplots(nrows, ncols, figsize=(24, 6))  # Flatten the axs array to loop through the subplots for i, ax in enumerate(axs.ravel()):     # Check if there are more columns to plot     if i &lt; len(cols):         # Use Seaborn to create a histogram for the current column         sns.histplot(data=df[:100000], x=cols[i], ax=ax, kde=True)  # Set kde=True for a kernel density estimate         ax.set_title(cols[i])  # Adjust subplot layout for better spacing plt.tight_layout() plt.show()  <p>Top of the page</p> <p></p> In\u00a0[23]: Copied! <pre># Calculate the correlation matrix\ncormat = df[cols].corr()\n\n# Set the colormap and color range\ncmap = sns.color_palette(\"coolwarm\", as_cmap=True)  # You can change the colormap as needed\nvmin, vmax = -1, 1  # Set the range for the color scale\n\n# Create the correlation heatmap\nplt.figure(figsize=(10, 8))  # Set the figure size\nsns.heatmap(cormat, cmap=cmap, vmin=vmin, vmax=vmax, annot=True, fmt=\".2f\")  # You can add annotations with fmt=\".2f\"\n\n# Set plot title\nplt.title(\"Correlation Heatmap\")\n\n# Show the plot\nplt.show()\n</pre>  # Calculate the correlation matrix cormat = df[cols].corr()  # Set the colormap and color range cmap = sns.color_palette(\"coolwarm\", as_cmap=True)  # You can change the colormap as needed vmin, vmax = -1, 1  # Set the range for the color scale  # Create the correlation heatmap plt.figure(figsize=(10, 8))  # Set the figure size sns.heatmap(cormat, cmap=cmap, vmin=vmin, vmax=vmax, annot=True, fmt=\".2f\")  # You can add annotations with fmt=\".2f\"  # Set plot title plt.title(\"Correlation Heatmap\")  # Show the plot plt.show()  <p>With just a few lines of code, we can quickly discern significant insights from a correlation plot. We observe a clear correlation between the distance (<code>Distance</code>) and the brightness, as indicated by the <code>phot_g_mean_mag</code> value. However, this correlation disappears when we convert the brightness to Absolute Magnitude (<code>AbsM</code>).</p> <p>Top of the page</p> <p></p> In\u00a0[24]: Copied! <pre>ax = plt.subplot()\n# Lets plot each point\n# alpha = 0.05, make the points transparanet (5%)\n# s = 1, make the points small\n# color = \"k\" make the points black\nax.scatter(df[\"bp_rp\"], df[\"AbsM\"], alpha=0.05, s=1, color='k')\n\n# Invert the y axis, smaller magnitude -&gt; Brighter star\nax.invert_yaxis()\n</pre> ax = plt.subplot() # Lets plot each point # alpha = 0.05, make the points transparanet (5%) # s = 1, make the points small # color = \"k\" make the points black ax.scatter(df[\"bp_rp\"], df[\"AbsM\"], alpha=0.05, s=1, color='k')  # Invert the y axis, smaller magnitude -&gt; Brighter star ax.invert_yaxis()   <p>While the scatter plot does reveal some underlying structure, it may not provide a highly informative representation of the data. The plot becomes saturated in high-density regions, making it challenging to discern details. To address this, we can use a histogram to visualize the density distribution more effectively.</p> In\u00a0[25]: Copied! <pre>ax = plt.subplot()\nax.scatter(df[\"bp_rp\"], df[\"AbsM\"], alpha=0.05, s=1, color='k', zorder=0)\n\n# Create a 2D histogram\n# bins = 100, require 100 bins in the x and y directions\n# cmin = 10, require at least 10 entries in a bin to be shown\n# cmap = \"jet\", use the \"jet\" color map\nh = ax.hist2d(df[\"bp_rp\"], df[\"AbsM\"], bins=100, cmin=100,  cmap=\"jet\")\nax.invert_yaxis()\n</pre> ax = plt.subplot() ax.scatter(df[\"bp_rp\"], df[\"AbsM\"], alpha=0.05, s=1, color='k', zorder=0)  # Create a 2D histogram # bins = 100, require 100 bins in the x and y directions # cmin = 10, require at least 10 entries in a bin to be shown # cmap = \"jet\", use the \"jet\" color map h = ax.hist2d(df[\"bp_rp\"], df[\"AbsM\"], bins=100, cmin=100,  cmap=\"jet\") ax.invert_yaxis() <p>This looks better, but we've made a critical error... Top of the page</p> <p></p> <p></p> <p>What information do you get from the above plot?</p> <p></p> <p>CMasheris a valuable Python package offering accessible scientific colormaps. All the color maps included in CMasher are designed to be colorblind-friendly and exhibit a linear increase in brightness.\"</p> In\u00a0[26]: Copied! <pre>import cmasher as cms\n</pre>  import cmasher as cms In\u00a0[27]: Copied! <pre>ax = plt.subplot()\nax.scatter(df[\"bp_rp\"], df[\"AbsM\"], alpha=0.05, s=1, color='k', zorder=0)\n\n# Create a 2D histogram\n# bins = 100, require 100 bins in the x and y directions\n# cmin = 10, require at least 10 entries in a bin to be shown\n# cmap = \"jet\", use the \"jet\" color map\nh = ax.hist2d(df[\"bp_rp\"], df[\"AbsM\"], bins=100, cmin=50,  cmap=cms.toxic)\nax.invert_yaxis()\n</pre> ax = plt.subplot() ax.scatter(df[\"bp_rp\"], df[\"AbsM\"], alpha=0.05, s=1, color='k', zorder=0)  # Create a 2D histogram # bins = 100, require 100 bins in the x and y directions # cmin = 10, require at least 10 entries in a bin to be shown # cmap = \"jet\", use the \"jet\" color map h = ax.hist2d(df[\"bp_rp\"], df[\"AbsM\"], bins=100, cmin=50,  cmap=cms.toxic) ax.invert_yaxis() <p>This looks better, but we still need to clean things up. The plot looks good, but a reader has no idea what is been plotted.</p> In\u00a0[28]: Copied! <pre># use colors to get a log color scale\nfrom matplotlib import colors\n\n\n# Create a subplot\nax = plt.subplot()\n\n# Scatter plot of data points with customizations\nax.scatter(df[\"bp_rp\"], df[\"AbsM\"], alpha=0.05, s=1, color='k', zorder=0)\n\n# Create a 2D histogram (density plot) with a log-normal color scale\n# - bins: Use 100 bins in both the x and y directions\n# - cmin: Require at least 10 entries in a bin to be shown\n# - cmap: Use the \"toxic\" color map from the cms module\n# - norm: Utilize a logarithmic color scale to better visualize data distribution\nh = ax.hist2d(df[\"bp_rp\"], df[\"AbsM\"], bins=100, cmin=50, norm=colors.LogNorm(), zorder=0.5, cmap=cms.toxic)\n\n\n# Invert the y-axis to match the typical appearance of a color-magnitude diagram\nax.invert_yaxis()\n\n# Add a color bar for the density plot\ncb = fig.colorbar(h[3], ax=ax, pad=0.02)\n\n# Set labels for the x and y axes\nax.set_xlabel(r'$G_{BP} - G_{RP}$')\nax.set_ylabel(r'$M_G$')\n\n# Set a label for the color bar\ncb.set_label(r\"$\\mathrm{Stellar~density}$\")\n\n# Add grid lines to the plot\nax.grid()\n</pre> # use colors to get a log color scale from matplotlib import colors   # Create a subplot ax = plt.subplot()  # Scatter plot of data points with customizations ax.scatter(df[\"bp_rp\"], df[\"AbsM\"], alpha=0.05, s=1, color='k', zorder=0)  # Create a 2D histogram (density plot) with a log-normal color scale # - bins: Use 100 bins in both the x and y directions # - cmin: Require at least 10 entries in a bin to be shown # - cmap: Use the \"toxic\" color map from the cms module # - norm: Utilize a logarithmic color scale to better visualize data distribution h = ax.hist2d(df[\"bp_rp\"], df[\"AbsM\"], bins=100, cmin=50, norm=colors.LogNorm(), zorder=0.5, cmap=cms.toxic)   # Invert the y-axis to match the typical appearance of a color-magnitude diagram ax.invert_yaxis()  # Add a color bar for the density plot cb = fig.colorbar(h[3], ax=ax, pad=0.02)  # Set labels for the x and y axes ax.set_xlabel(r'$G_{BP} - G_{RP}$') ax.set_ylabel(r'$M_G$')  # Set a label for the color bar cb.set_label(r\"$\\mathrm{Stellar~density}$\")  # Add grid lines to the plot ax.grid() <p>So, what insights can we gather from this plot?</p> <ul> <li>The plot's left side corresponds to 'blue,' while the right side corresponds to 'red.'</li> <li>The lower section of the plot represents 'dim' stars, whereas the upper section showcases 'bright' stars.</li> <li>The majority of stars form a discernible diagonal pattern, with dimmer stars appearing 'redder' and brighter stars appearing 'bluer.'</li> <li>Notably, we observe a distinct grouping of dim, blue stars in the bottom-left quadrant of the plot.</li> <li>There is a noticeable 'turn-off' branch where we encounter both bright blue and bright red stars.</li> <li>As we progress along this branch, the bright red stars appear to level off in brightness and become more 'red.'</li> <li>Stars residing in the bright red and dim blue regions are relatively scarce.</li> </ul> <p>Top of the page</p> <p></p> <p>In the realm of astronomy, we've uncovered one of the most profound tools: the Hertzsprung-Russell (HR) Diagram.</p> <p></p> <p>The HR Diagram serves as a stellar narrative, shedding light on the intricate lives of stars. It all commences with a star's birth, and its initial position on the diagram is determined by its mass. Stars embark on their cosmic journey along the 'Main Sequence,' where they sustain themselves by converting hydrogen into helium\u2014a process known as hydrogen burning. This phase constitutes the majority of a star's existence.</p> <p>However, as time advances, stars deplete their hydrogen fuel. When this occurs, they transition to the 'Giant' branch. Giants display a cooler surface temperature, giving them a 'reddish' appearance. Concurrently, they experience a considerable increase in luminosity, shining much brighter. This phenomenon is accompanied by a significant expansion in their physical size, leading to their classification as 'red giants.'</p> <p>Now, here's the fascinating twist: Stars like our Sun, after the exhaustion of helium, evolve into 'White Dwarfs.' This transformation arises from the fact that they lack the necessary mass to facilitate the fusion of elements heavier than helium. These stars have meticulously fused all their available helium into carbon. Unlike most stars that maintain a delicate equilibrium between gravitational forces and nuclear fusion, White Dwarfs are held in place by a phenomenon known as 'electron degeneracy.'</p> <p>In essence, the HR Diagram serves as a window into the diverse chapters of stellar existence, offering us valuable insights into the captivating stories of these cosmic marvels.</p> <p>Top of the page</p> <p></p> In\u00a0[29]: Copied! <pre>from astropy.coordinates import SkyCoord\n</pre> from astropy.coordinates import SkyCoord In\u00a0[30]: Copied! <pre>coord = SkyCoord(df[\"ra\"], df[\"dec\"], unit=\"deg\")\n</pre> coord = SkyCoord(df[\"ra\"], df[\"dec\"], unit=\"deg\")  In\u00a0[31]: Copied! <pre>gal_coords = coord.galactic\n</pre> gal_coords = coord.galactic In\u00a0[32]: Copied! <pre>plt.subplot(111, projection='aitoff')\nplt.grid(True)\nplt.scatter(gal_coords.l.wrap_at('180d').radian, gal_coords.b.radian, alpha = 0.002)\n</pre> plt.subplot(111, projection='aitoff') plt.grid(True) plt.scatter(gal_coords.l.wrap_at('180d').radian, gal_coords.b.radian, alpha = 0.002)  Out[32]: <pre>&lt;matplotlib.collections.PathCollection at 0x7f3339ae9850&gt;</pre> In\u00a0[33]: Copied! <pre>def get_bin_centers(binning):\n    \"\"\"\n    Calculate the bin centers given a set of bin edges.\n\n    Parameters:\n    binning (numpy.ndarray): An array containing the bin edges.\n\n    Returns:\n    numpy.ndarray: An array of bin centers.\n\n    Example:\n    &gt;&gt;&gt; bin_edges = np.array([0, 1, 2, 3, 4, 5])\n    &gt;&gt;&gt; centers = get_bin_centers(bin_edges)\n    &gt;&gt;&gt; print(centers)\n    [0.5 1.5 2.5 3.5 4.5]\n    \"\"\"\n\n    # Calculate the width of each bin by subtracting the previous bin's edge from the current bin's edge\n    width = binning[1:] - binning[:-1]\n\n    # Calculate the bin centers by adding half of the bin width to the left bin edge\n    bin_centers = binning[:-1] + 0.5 * width\n\n    return bin_centers\n</pre> def get_bin_centers(binning):     \"\"\"     Calculate the bin centers given a set of bin edges.      Parameters:     binning (numpy.ndarray): An array containing the bin edges.      Returns:     numpy.ndarray: An array of bin centers.      Example:     &gt;&gt;&gt; bin_edges = np.array([0, 1, 2, 3, 4, 5])     &gt;&gt;&gt; centers = get_bin_centers(bin_edges)     &gt;&gt;&gt; print(centers)     [0.5 1.5 2.5 3.5 4.5]     \"\"\"      # Calculate the width of each bin by subtracting the previous bin's edge from the current bin's edge     width = binning[1:] - binning[:-1]      # Calculate the bin centers by adding half of the bin width to the left bin edge     bin_centers = binning[:-1] + 0.5 * width      return bin_centers  In\u00a0[34]: Copied! <pre># Define the number of bins and the bin edges for galactic longitude (l) and latitude (b)\nnbins = 100\nl_bins = np.linspace(-180, 180, nbins)\nb_bins = np.linspace(-90, 90, nbins)\n\n# Calculate the bin centers for galactic longitude (l) and latitude (b)\nl_center = get_bin_centers(l_bins)\nb_center = get_bin_centers(b_bins)\n\n# Calculate the 2D histogram (counts) of galactic coordinates l and b\n# Use np.histogram2d to create a 2D histogram from galactic coordinates\n# Wrap galactic longitude (l) at 180 degrees to ensure it covers the full range\ncounts, _, _ = np.histogram2d(\n    gal_coords.l.wrap_at('180d').radian,  # Convert and wrap l in radians\n    gal_coords.b.radian,                # Convert b in radians\n    bins=[np.deg2rad(l_bins), np.deg2rad(b_bins)]  # Convert bin edges to radians\n)\n</pre> # Define the number of bins and the bin edges for galactic longitude (l) and latitude (b) nbins = 100 l_bins = np.linspace(-180, 180, nbins) b_bins = np.linspace(-90, 90, nbins)  # Calculate the bin centers for galactic longitude (l) and latitude (b) l_center = get_bin_centers(l_bins) b_center = get_bin_centers(b_bins)  # Calculate the 2D histogram (counts) of galactic coordinates l and b # Use np.histogram2d to create a 2D histogram from galactic coordinates # Wrap galactic longitude (l) at 180 degrees to ensure it covers the full range counts, _, _ = np.histogram2d(     gal_coords.l.wrap_at('180d').radian,  # Convert and wrap l in radians     gal_coords.b.radian,                # Convert b in radians     bins=[np.deg2rad(l_bins), np.deg2rad(b_bins)]  # Convert bin edges to radians )  In\u00a0[35]: Copied! <pre># Create a subplot with an Aitoff projection for a celestial map\nax = plt.subplot(111, projection='aitoff')\n\n# Create a pseudo-color mesh plot (pcolormesh) for the stellar density map\n# Use np.deg2rad to convert bin centers from degrees to radians for the x and y axes\n# Use 'counts.T' to transpose the counts array for proper orientation\n# Apply a PowerNorm to enhance visualization (power=0.7)\n# Set zorder to control the layering of the plot\n# Use the 'savanna' color map from cms module\nh = ax.pcolormesh(np.deg2rad(l_center), np.deg2rad(b_center), counts.T, norm=colors.PowerNorm(0.7), zorder=0.5, cmap=cms.savanna)\n\n# Add a color bar for the density map\ncb = fig.colorbar(h, ax=ax, pad=0.02)\n\n# Set labels for the x and y axes\nax.set_xlabel(r'Galactic Longitude')\nax.set_ylabel(r'Galactic Latitude')\n\n# Set a label for the color bar\ncb.set_label(r\"$\\mathrm{Stellar~density}$\")\n\n# Add grid lines to the celestial map\nax.grid()\n</pre> # Create a subplot with an Aitoff projection for a celestial map ax = plt.subplot(111, projection='aitoff')  # Create a pseudo-color mesh plot (pcolormesh) for the stellar density map # Use np.deg2rad to convert bin centers from degrees to radians for the x and y axes # Use 'counts.T' to transpose the counts array for proper orientation # Apply a PowerNorm to enhance visualization (power=0.7) # Set zorder to control the layering of the plot # Use the 'savanna' color map from cms module h = ax.pcolormesh(np.deg2rad(l_center), np.deg2rad(b_center), counts.T, norm=colors.PowerNorm(0.7), zorder=0.5, cmap=cms.savanna)  # Add a color bar for the density map cb = fig.colorbar(h, ax=ax, pad=0.02)  # Set labels for the x and y axes ax.set_xlabel(r'Galactic Longitude') ax.set_ylabel(r'Galactic Latitude')  # Set a label for the color bar cb.set_label(r\"$\\mathrm{Stellar~density}$\")  # Add grid lines to the celestial map ax.grid()  <p>We observer that most stars fall alone a latitude of 0. This is the galactic plane. We also notice some spots of high stellar density. These are likely stellar clusters. Clusters of gravationally bound groups of stars.</p> <p>Top of the page</p> <p></p> In\u00a0[36]: Copied! <pre>df_open_clusters = pd.read_csv(\"./OpenClusters.csv\")\n</pre> df_open_clusters = pd.read_csv(\"./OpenClusters.csv\") In\u00a0[37]: Copied! <pre>df_open_clusters_near = df_open_clusters[df_open_clusters[\"Distance (parsecs)\"] &lt; 200]\n</pre> df_open_clusters_near = df_open_clusters[df_open_clusters[\"Distance (parsecs)\"] &lt; 200] In\u00a0[38]: Copied! <pre>df_open_clusters_near.head()\n</pre> df_open_clusters_near.head() Out[38]: Unnamed: 0 Cluster identifier Unnamed: 1_level_0 Unnamed: 2_level_0 Constellation Distance (parsecs) Age (Myr) Diameter Apparent magnitude Notes 0 0 Hyades 04h 26.9m +15d 52m Taurus 47.0 625 330.0 0.5 [3] 1 1 Coma Star Cluster 12h 22.5m +25d 51m Coma Berenices 86.0 400-500 120.0 1.8 [4] 20 20 Messier 44, Beehive Cluster 08h 40.4m +19d 41m Cancer 187.0 830 70.0 3.1 [10][11] 21 21 Messier 45, Pleiades 03h 47.4m +24d 07m Taurus 136.0 125 120.0 1.2 [12] 30 30 IC 2602, Southern Pleiades 10h 43.2m -64d 24m Carina 167.0 30 100.0 1.9 [14] In\u00a0[39]: Copied! <pre>cluster_ra = df_open_clusters_near[\"Unnamed: 1_level_0\"]\ncluster_dec = df_open_clusters_near[\"Unnamed: 2_level_0\"]\ncluster_coords = SkyCoord(cluster_ra,cluster_dec)\n\ndf_open_clusters_near[\"l\"] = cluster_coords.galactic.l.rad\ndf_open_clusters_near[\"b\"] = cluster_coords.galactic.b.rad\n</pre> cluster_ra = df_open_clusters_near[\"Unnamed: 1_level_0\"] cluster_dec = df_open_clusters_near[\"Unnamed: 2_level_0\"] cluster_coords = SkyCoord(cluster_ra,cluster_dec)  df_open_clusters_near[\"l\"] = cluster_coords.galactic.l.rad df_open_clusters_near[\"b\"] = cluster_coords.galactic.b.rad <pre>/tmp/ipykernel_105960/3746201907.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_open_clusters_near[\"l\"] = cluster_coords.galactic.l.rad\n/tmp/ipykernel_105960/3746201907.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_open_clusters_near[\"b\"] = cluster_coords.galactic.b.rad\n</pre> In\u00a0[40]: Copied! <pre>df_open_clusters_near.head()\n</pre> df_open_clusters_near.head() Out[40]: Unnamed: 0 Cluster identifier Unnamed: 1_level_0 Unnamed: 2_level_0 Constellation Distance (parsecs) Age (Myr) Diameter Apparent magnitude Notes l b 0 0 Hyades 04h 26.9m +15d 52m Taurus 47.0 625 330.0 0.5 [3] 3.142706 -0.389958 1 1 Coma Star Cluster 12h 22.5m +25d 51m Coma Berenices 86.0 400-500 120.0 1.8 [4] 3.882823 1.455624 20 20 Messier 44, Beehive Cluster 08h 40.4m +19d 41m Cancer 187.0 830 70.0 3.1 [10][11] 3.593651 0.567058 21 21 Messier 45, Pleiades 03h 47.4m +24d 07m Taurus 136.0 125 120.0 1.2 [12] 2.908491 -0.409449 30 30 IC 2602, Southern Pleiades 10h 43.2m -64d 24m Carina 167.0 30 100.0 1.9 [14] 5.054886 -0.085419 In\u00a0[41]: Copied! <pre>sns.set_palette(\"colorblind\")\n\n# Create a subplot with an Aitoff projection for a celestial map\nax = plt.subplot(111, projection='aitoff')\n\n# Create a pseudo-color mesh plot (pcolormesh) for the stellar density map\n# Use np.deg2rad to convert bin centers from degrees to radians for the x and y axes\n# Use 'counts.T' to transpose the counts array for proper orientation\n# Apply a PowerNorm to enhance visualization (power=0.7)\n# Set zorder to control the layering of the plot\n# Use the 'savanna' color map from cms module\nh = ax.pcolormesh(np.deg2rad(l_center), np.deg2rad(b_center), counts.T, norm=colors.PowerNorm(0.7), zorder=0.5, cmap=cms.sepia)\n\n# Add a color bar for the density map\ncb = fig.colorbar(h, ax=ax, pad=0.02)\n\n# Set labels for the x and y axes\nax.set_xlabel(r'Galactic Longitude')\nax.set_ylabel(r'Galactic Latitude')\n\n# Set a label for the color bar\ncb.set_label(r\"$\\mathrm{Stellar~density}$\")\n\n\n# Set the number of brightest objects to overlay\nN = 3 # Adjust this to select the top N brightest objects\n\n# Sort the DataFrame by apparent magnitude (brightness)\nsorted_data = df_open_clusters_near.sort_values(by='Apparent magnitude')\n\n# Select the top N brightest objects\nbrightest_objects = sorted_data.head(N)\n\n# Create a scatter plot of galactic coordinates (l, b)\ncircle_size = 140\nfor i in range(len(brightest_objects)):\n    ax.scatter(\n        brightest_objects['l'].iloc[i], \n        brightest_objects['b'].iloc[i], \n        facecolor='none', edgecolor=f'C{i}',\n        s=circle_size,\n        label=brightest_objects['Cluster identifier'].iloc[i])\n\n\nax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2)  # Adjust the bbox_to_anchor to control the legend position\n# Add grid lines to the celestial map\nax.grid()\n</pre> sns.set_palette(\"colorblind\")  # Create a subplot with an Aitoff projection for a celestial map ax = plt.subplot(111, projection='aitoff')  # Create a pseudo-color mesh plot (pcolormesh) for the stellar density map # Use np.deg2rad to convert bin centers from degrees to radians for the x and y axes # Use 'counts.T' to transpose the counts array for proper orientation # Apply a PowerNorm to enhance visualization (power=0.7) # Set zorder to control the layering of the plot # Use the 'savanna' color map from cms module h = ax.pcolormesh(np.deg2rad(l_center), np.deg2rad(b_center), counts.T, norm=colors.PowerNorm(0.7), zorder=0.5, cmap=cms.sepia)  # Add a color bar for the density map cb = fig.colorbar(h, ax=ax, pad=0.02)  # Set labels for the x and y axes ax.set_xlabel(r'Galactic Longitude') ax.set_ylabel(r'Galactic Latitude')  # Set a label for the color bar cb.set_label(r\"$\\mathrm{Stellar~density}$\")   # Set the number of brightest objects to overlay N = 3 # Adjust this to select the top N brightest objects  # Sort the DataFrame by apparent magnitude (brightness) sorted_data = df_open_clusters_near.sort_values(by='Apparent magnitude')  # Select the top N brightest objects brightest_objects = sorted_data.head(N)  # Create a scatter plot of galactic coordinates (l, b) circle_size = 140 for i in range(len(brightest_objects)):     ax.scatter(         brightest_objects['l'].iloc[i],          brightest_objects['b'].iloc[i],          facecolor='none', edgecolor=f'C{i}',         s=circle_size,         label=brightest_objects['Cluster identifier'].iloc[i])   ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2)  # Adjust the bbox_to_anchor to control the legend position # Add grid lines to the celestial map ax.grid()  <p>Top of the page</p> <p></p> In\u00a0[42]: Copied! <pre>from sklearn.cluster import DBSCAN\n</pre> from sklearn.cluster import DBSCAN In\u00a0[43]: Copied! <pre>x = gal_coords.l.wrap_at('180d').radian\ny = gal_coords.b.radian\nz = np.log10(df[\"Distance\"])\ndata = np.array([x,y,z]).T\n\nnormalized_data = (data - data.mean(axis=0)) / data.std(axis=0)\n</pre> x = gal_coords.l.wrap_at('180d').radian y = gal_coords.b.radian z = np.log10(df[\"Distance\"]) data = np.array([x,y,z]).T  normalized_data = (data - data.mean(axis=0)) / data.std(axis=0)  In\u00a0[44]: Copied! <pre># # Define DBSCAN parameters\n# eps_values = np.linspace(0.1,1, 5)  # Epsilon neighborhood size\n# min_samples = 1000  # Minimum number of samples in a cluster\n\n# for eps in eps_values:\n#     # Apply DBSCAN\n#     dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n#     labels = dbscan.fit_predict(data)\n\n#     mean_l = []\n#     mean_b = []\n#     mean_dist = []\n    \n#     for lab in set(labels):\n#         if lab == -1:\n#             continue\n#         print (lab)\n#         lab_mask = labels == lab\n#         sub_l = x[lab_mask]\n#         sub_b = y[lab_mask]\n#         sub_d = z[lab_mask]\n#         mean_l.append(np.mean(sub_l))\n#         mean_b.append(np.mean(sub_b))\n#         mean_dist.append(np.mean(sub_d))\n\n#     fig = plt.figure(figsize=(11,6))\n#     # Create a subplot with an Aitoff projection for a celestial map\n#     ax = plt.subplot(111, projection='aitoff')\n    \n#     # Create a pseudo-color mesh plot (pcolormesh) for the stellar density map\n#     # Use np.deg2rad to convert bin centers from degrees to radians for the x and y axes\n#     # Use 'counts.T' to transpose the counts array for proper orientation\n#     # Apply a PowerNorm to enhance visualization (power=0.7)\n#     # Set zorder to control the layering of the plot\n#     # Use the 'savanna' color map from cms module\n#     h = ax.pcolormesh(np.deg2rad(l_center), np.deg2rad(b_center), counts.T, norm=colors.PowerNorm(0.7), zorder=0.5, cmap=cms.sepia)\n    \n#     # Add a color bar for the density map\n#     cb = fig.colorbar(h, ax=ax, pad=0.02)\n    \n#     # Set labels for the x and y axes\n#     ax.set_xlabel(r'Galactic Longitude')\n#     ax.set_ylabel(r'Galactic Latitude')\n    \n#     # Set a label for the color bar\n#     cb.set_label(r\"$\\mathrm{Stellar~density}$\")\n    \n    \n#     # # Set the number of brightest objects to overlay\n#     # N = 3 # Adjust this to select the top N brightest objects\n    \n#     # # Sort the DataFrame by apparent magnitude (brightness)\n#     # sorted_data = df_open_clusters_near.sort_values(by='Apparent magnitude')\n    \n#     # # Select the top N brightest objects\n#     # brightest_objects = sorted_data.head(N)\n    \n#     # # Create a scatter plot of galactic coordinates (l, b)\n#     # circle_size = 140\n#     # for i in range(len(brightest_objects)):\n#     #     ax.scatter(\n#     #         brightest_objects['l'].iloc[i], \n#     #         brightest_objects['b'].iloc[i], \n#     #         facecolor='none', edgecolor=f'C{i}',\n#     #         s=circle_size,\n#     #         label=brightest_objects['Cluster identifier'].iloc[i])\n    \n    \n#     for i in range(len(mean_l)):\n#         ax.scatter(\n#             mean_l[i], \n#             mean_b[i], \n#             facecolor='none', edgecolor=f'C{i}',\n#             s=circle_size,\n#             label=f\"Cluster {i+1}\")\n    \n#     ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2)  # Adjust the bbox_to_anchor to control the legend position\n#     # Add grid lines to the celestial map\n#     ax.grid()\n#     fig.savefig(f\"eps_{eps}.png\")\n</pre> # # Define DBSCAN parameters # eps_values = np.linspace(0.1,1, 5)  # Epsilon neighborhood size # min_samples = 1000  # Minimum number of samples in a cluster  # for eps in eps_values: #     # Apply DBSCAN #     dbscan = DBSCAN(eps=eps, min_samples=min_samples) #     labels = dbscan.fit_predict(data)  #     mean_l = [] #     mean_b = [] #     mean_dist = []      #     for lab in set(labels): #         if lab == -1: #             continue #         print (lab) #         lab_mask = labels == lab #         sub_l = x[lab_mask] #         sub_b = y[lab_mask] #         sub_d = z[lab_mask] #         mean_l.append(np.mean(sub_l)) #         mean_b.append(np.mean(sub_b)) #         mean_dist.append(np.mean(sub_d))  #     fig = plt.figure(figsize=(11,6)) #     # Create a subplot with an Aitoff projection for a celestial map #     ax = plt.subplot(111, projection='aitoff')      #     # Create a pseudo-color mesh plot (pcolormesh) for the stellar density map #     # Use np.deg2rad to convert bin centers from degrees to radians for the x and y axes #     # Use 'counts.T' to transpose the counts array for proper orientation #     # Apply a PowerNorm to enhance visualization (power=0.7) #     # Set zorder to control the layering of the plot #     # Use the 'savanna' color map from cms module #     h = ax.pcolormesh(np.deg2rad(l_center), np.deg2rad(b_center), counts.T, norm=colors.PowerNorm(0.7), zorder=0.5, cmap=cms.sepia)      #     # Add a color bar for the density map #     cb = fig.colorbar(h, ax=ax, pad=0.02)      #     # Set labels for the x and y axes #     ax.set_xlabel(r'Galactic Longitude') #     ax.set_ylabel(r'Galactic Latitude')      #     # Set a label for the color bar #     cb.set_label(r\"$\\mathrm{Stellar~density}$\")           #     # # Set the number of brightest objects to overlay #     # N = 3 # Adjust this to select the top N brightest objects      #     # # Sort the DataFrame by apparent magnitude (brightness) #     # sorted_data = df_open_clusters_near.sort_values(by='Apparent magnitude')      #     # # Select the top N brightest objects #     # brightest_objects = sorted_data.head(N)      #     # # Create a scatter plot of galactic coordinates (l, b) #     # circle_size = 140 #     # for i in range(len(brightest_objects)): #     #     ax.scatter( #     #         brightest_objects['l'].iloc[i],  #     #         brightest_objects['b'].iloc[i],  #     #         facecolor='none', edgecolor=f'C{i}', #     #         s=circle_size, #     #         label=brightest_objects['Cluster identifier'].iloc[i])           #     for i in range(len(mean_l)): #         ax.scatter( #             mean_l[i],  #             mean_b[i],  #             facecolor='none', edgecolor=f'C{i}', #             s=circle_size, #             label=f\"Cluster {i+1}\")      #     ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2)  # Adjust the bbox_to_anchor to control the legend position #     # Add grid lines to the celestial map #     ax.grid() #     fig.savefig(f\"eps_{eps}.png\") In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Python/GaiaTutorial/#table-of-contents","title":"Table of contents\u00b6","text":"<ol> <li>Introduction of Gaia</li> <li>What is Astroquery</li> <li>Querying databases with Astroquery</li> <li>Working with Pandas Dataframes</li> <li>Calculating the Distance to Stars</li> <li>Converting units with Astropy</li> <li>Converting from apparent to absolute magnitude</li> <li>Exploratory Data Analysis with Pandas</li> <li>Understanding Correlations: Unraveling Relationships in Data</li> <li>Making Publication Quality Plots</li> <li>Understanding Color Maps and Accessibility</li> <li>The Hertzsprung-Russell Diagram</li> <li>Mapping Stellar Density in Galactic Coordinates</li> <li>Open Clusters</li> <li>Clustering</li> </ol>"},{"location":"Python/GaiaTutorial/#accessing-and-manipulating-astronomical-data-using-python","title":"Accessing and Manipulating Astronomical Data Using Python\u00b6","text":"<p>In this session, we will learn how to use the Python package Astroquery to access astronomical data. We will then utilize Astropy to simplify various calculations and unit conversions.</p> <p>Our primary focus will be on data obtained by the Gaia mission. Gaia is a European Space Agency mission with the goal of creating a 3D map of the Milky Way. Gaia's mission is to accurately measure the positions of over 1 billion stars.</p> <p> Image: Gaia - Exploring the Multi-dimensional Milky Way</p> <p>We will be working with data collected using Gaia's Blue and Red Photometers (BP &amp; RP) and making use of parallax measurements to estimate the distances to celestial objects.</p> <p>The use of Gaia data for a HR diagram is based off of a blog post by Vlas Sokolov.</p>"},{"location":"Python/GaiaTutorial/#what-is-astroquery","title":"What is Astroquery? \u00b6","text":"<p>Astroquery is a Python package that streamlines the process of querying astronomical databases and catalogs. It offers a wide range of options for accessing astronomical data. Some of the available options include:</p> <ul> <li>Querying databases of celestial objects.</li> <li>Retrieving data from sky surveys and telescopes.</li> <li>Accessing information from observatories and space missions.</li> </ul> <p></p> <p>Image: A selection of options provided by Astroquery</p> <p>For our tutorial, we will focus on using Astroquery to work with data from the Gaia mission. To get started, let's import the Gaia module:</p>"},{"location":"Python/GaiaTutorial/#what-data-are-we-interseted-in","title":"What Data are we interseted in?\u00b6","text":"<p>We're interested in extracting specific data fields from the database, which will help us gain insights into celestial objects. Here's what we want to retrieve:</p> <ol> <li><p>Source Location:</p> <ul> <li><code>ra</code> (Right Ascension)</li> <li><code>dec</code> (Declination)</li> </ul> </li> <li><p>Color-Magnitude Diagram:</p> <ul> <li><code>bp_rp</code> (Blue-Red Color), which represents the difference between the Blue and Red band magnitudes.</li> </ul> </li> <li><p>Brightness of the Source:</p> <ul> <li><code>phot_g_mean_mag</code>, which is the average apparent G-band magnitude.</li> </ul> </li> <li><p>Distance Estimation:</p> <ul> <li><code>parallax</code>, which measures the apparent motion of the star due to the Earth's motion.</li> <li><code>parallax_over_error</code>, calculated as the ratio of parallax to the uncertainty on the parallax.</li> </ul> </li> </ol> <p> Image: Measuring Stellar Distances by Parallax</p> <p>By combining these fields, we will construct a comprehensive query that will help us obtain the data we need. Additionally, it's a good practice to save the query results to a CSV file for future reference, so we won't need to rerun the query.</p>"},{"location":"Python/GaiaTutorial/#what-is-pandas","title":"What is pandas\u00b6","text":"<p>Pandas is a popular open-source data manipulation and analysis library for Python, providing versatile data structures and data analysis tools. A pandas <code>dataframe</code> is a two-dimensional, labeled data structure that resembles a table and is used for data manipulation, analysis, and exploration in Python.</p>"},{"location":"Python/GaiaTutorial/#how-do-we-calculate-the-distance-to-stars","title":"How Do We Calculate the Distance to Stars?\u00b6","text":"<p>Determining the distance to stars in astronomy involves trigonometry. We can calculate the distance ($d$) to a star using the formula:</p> <p>$$d = \\frac{1}{p},$$</p> <p>Here, $d$ is the distance in a unit called \"parsecs,\" and \"$p$\" is the parallax angle in arcseconds. It's important to note that the Gaia data records the parallax angle in units of milliarcseconds, so we need to perform a unit conversion. Top of the page</p> <p></p>"},{"location":"Python/GaiaTutorial/#how-do-we-convert-units","title":"How Do We Convert Units?\u00b6","text":"<p>To handle this unit conversion, we'll utilize <code>astropy.units</code>. To further simplify the process, we'll leverage Astropy not only for unit conversion but also for distance calculation. In addition, we'll use NumPy arrays to perform mathematical operations on large lists or arrays of numbers.</p>"},{"location":"Python/GaiaTutorial/#converting-from-apparent-magnitude-to-absolute-magnitude","title":"Converting from Apparent Magnitude to Absolute Magnitude\u00b6","text":"<p>When assessing the brightness of an astronomical object, it's crucial to consider the impact of distance on our measurements.</p> <p>The brightness of an object appears different depending on its proximity to us; it will appear brighter if it's closer and dimmer if it's farther away. In astronomy, we commonly use \"Apparent Magnitude\" to describe how bright an object appears from our vantage point and \"Absolute Magnitude\" to standardize brightness by accounting for the object's distance. Absolute Magnitude quantifies how bright an object would appear if it were located precisely 10 parsecs away.</p> <p> Image: Absolute Magnitude Concept</p> <p>By knowing the distance in parsecs, we can convert from apparent to absolute magnitude using the formula:</p> <p>$$M = m - 5 \\log_{10}(d_{pc}) + 5,$$</p> <p>Where:</p> <ul> <li>$M$ is the Absolute Magnitude.</li> <li>$m$ is the Apparent Magnitude.</li> <li>$d_{pc}$ is the distance in parsecs.</li> </ul> <p>This formula enables us to correct for the influence of distance when comparing the brightness of celestial objects.</p>"},{"location":"Python/GaiaTutorial/#exploratory-data-analysis-with-pandas","title":"Exploratory Data Analysis with Pandas\u00b6","text":"<p>Explore your dataset through the lens of Exploratory Data Analysis (EDA). EDA is all about unveiling hidden patterns and stories within your data. It's your detective work to discover insights, detect anomalies, and gain a deeper understanding of your dataset.</p> <p>We'll utilize pandas for data manipulation and Matplotlib/Seaborn for visualization. In this section, we'll delve into techniques for exploring your dataset, from basic properties to captivating visualizations. Welcome to the world of EDA!</p> <p>Some methods of EDA:</p> <ol> <li><p>Descriptive Statistics:</p> <ul> <li>Summary statistics</li> <li>Count of missing values</li> </ul> </li> <li><p>Data Distribution:</p> <ul> <li>Histograms</li> <li>Box plots</li> <li>Probability density functions (PDFs)</li> <li>Skewness and kurtosis</li> </ul> </li> <li><p>Data Relationships:</p> <ul> <li>Correlation matrix</li> <li>Scatter plots</li> <li>Heatmaps</li> <li>Pair plots</li> </ul> </li> <li><p>Categorical Data:</p> <ul> <li>Frequency counts</li> <li>Bar plots</li> <li>Cross-tabulations</li> </ul> </li> <li><p>Data Quality:</p> <ul> <li>Outliers</li> <li>Missing data</li> <li>Data consistency</li> </ul> </li> <li><p>Dimensionality Reduction:</p> <ul> <li>Principal Component Analysis (PCA)</li> <li>t-Distributed Stochastic Neighbor Embedding (t-SNE)</li> </ul> </li> <li><p>Data Visualization:</p> <ul> <li>Various plots and charts</li> </ul> </li> </ol>"},{"location":"Python/GaiaTutorial/#understanding-correlations-unraveling-relationships-in-data","title":"Understanding Correlations: Unraveling Relationships in Data\u00b6","text":"<p>In the realm of data analysis, uncovering the relationships and dependencies between different variables is a fundamental task. Correlations are the keys that unlock the doors to understanding the hidden patterns, associations, and causations within datasets. These intriguing statistical relationships offer a wealth of insights, guiding our understanding of how changes in one variable can impact another.</p> <p>Correlations enable us to answer questions such as:</p> <ul> <li>Does an increase in one variable lead to a corresponding increase or decrease in another?</li> <li>Are two variables completely independent, or is there a subtle connection waiting to be discovered?</li> <li>Which factors are most influential in affecting a specific outcome?</li> </ul> <p>With the correlation between two parameters (A and B) we can say:</p> <ul> <li>1, when  A increases so does B. These are said to be \"correalated\"</li> <li>0, when A increases we see no change in B. These are said to be \"uncorrelated\"</li> <li>-1, when A increase B decreases. These are said to be \"anit-correalted\"</li> </ul> <p>In the case of 1 and -1, we have 100% correlation and anti-correlation. We can have any value between these two numbers</p>"},{"location":"Python/GaiaTutorial/#making-publication-quality-plots","title":"Making Publication Quality Plots\u00b6","text":"<p>Now, let's proceed with building the color-magnitude diagram by plotting <code>bp_rp</code> against <code>AbsM</code>.</p> <p>Creating a high-quality scientific plot is essential for effectively conveying your data. A well-crafted plot should be designed to clearly communicate the key information to the viewer. It should include the following elements to enhance data understanding:</p> <ul> <li><p>Labelled Axes: Clearly annotated x and y axes provide context and meaning to the data.</p> </li> <li><p>Descriptive Title: A meaningful title summarizing the plot's content is crucial for quick comprehension.</p> </li> <li><p>Grid Lines: Grid lines assist in reading data values accurately and help viewers align data points.</p> </li> <li><p>Sensible Scaling: Appropriate axis scaling ensures that data is presented in a visually understandable manner without distortion.</p> </li> <li><p>Legend (if applicable): If multiple data series are displayed, a legend should explain their meaning.</p> </li> <li><p>Colorbar (if applicable): When using color to represent data values, a colorbar helps viewers interpret the color-coding.</p> </li> </ul> <p>By incorporating these elements, your scientific plot becomes a powerful tool for sharing insights and findings with your audience.</p>"},{"location":"Python/GaiaTutorial/#understanding-color-maps-and-accessibility","title":"Understanding Color Maps and Accessibility\u00b6","text":"<p>Color maps play a crucial role in data visualization, helping us represent data through a spectrum of colors. However, not all color maps are created equal, and some, like the \"jet\" color map, have raised concerns due to their perceptual issues.</p> <p>The \"jet\" color map, characterized by its vibrant and rainbow-like appearance, is notorious for its drawbacks, including:</p> <ul> <li><p>Perceptual Uniformity: It fails to maintain a perceptual uniformity, which means that equal steps in data values may not appear equally spaced in the color map, leading to misleading visualizations.</p> </li> <li><p>Color Saturation: The \"jet\" color map can saturate at high and low ends, making it challenging to distinguish different data values in these regions.</p> </li> <li><p>Colorblind Accessibility: Accessibility is a vital concern. Many individuals with color vision deficiencies, commonly referred to as colorblindness, may find it difficult to interpret data represented using color maps like \"jet.\"</p> </li> </ul> <p>To create effective and accessible visualizations, it's essential to consider the choice of color map and its impact on how data is perceived, especially in cases where colorblind individuals may interact with the visualizations.</p>"},{"location":"Python/GaiaTutorial/#the-hertzsprung-russell-diagram","title":"The Hertzsprung-Russell Diagram\u00b6","text":""},{"location":"Python/GaiaTutorial/#mapping-stellar-density-in-galactic-coordinates","title":"Mapping Stellar Density in Galactic Coordinates\u00b6","text":"<p>Our celestial journey continues as we leverage the rich data from the Gaia mission that we previously dissected. In this leg of our astronomical exploration, we're about to embark on an exciting mission: mapping the stellar density within our Milky Way galaxy.</p> <p>Armed with Gaia's invaluable data, we'll now venture into the realm of galactic coordinates. These coordinates provide us with a unique perspective, allowing us to chart the positions and densities of stars across the vast expanse of our galactic home.</p> <p>By harnessing the power of data analysis and visualization, we'll unveil a stellar density map that offers an intricate view of the distribution and concentration of stars within the Milky Way. Our map will not only guide us through the celestial neighborhoods but also shed light on the interplay between stars, clusters, and the galactic structures that shape our night sky.</p> <p>So, prepare to join us in this thrilling endeavor as we decode the secrets of the stars and craft a stellar density map in the wondrous realm of galactic coordinates.</p>"},{"location":"Python/GaiaTutorial/#open-clusters","title":"Open Clusters\u00b6","text":"<p>Open cluster: NGC 2164</p> <p>Open clusters are remarkable gatherings of young stars that form from the same cloud of interstellar gas and dust. These stellar congregations are a captivating feature of our galaxy, the Milky Way, and can be found in various regions of its spiral arms. Open clusters typically contain hundreds to thousands of stars.</p> <p>Open clusters provide insights into stellar evolution and the early stages of star formation. The young stars within open clusters often share similar ages, compositions, and distances, making them ideal laboratories for studying stellar properties and the effects of their environment on their development.</p> <p>Over time, open clusters gradually disperse due to gravitational interactions and tidal forces from the Milky Way. As a result, the stars within these clusters eventually go their separate ways, becoming part of the general stellar population of our galaxy.</p> <p>Studying open clusters offers valuable clues about the dynamics of our galaxy, the formation of stars, and the life cycles of stellar systems. Astronomers continue to explore and catalog these captivating celestial gatherings to deepen our understanding of the cosmos.</p>"},{"location":"Python/GaiaTutorial/#clustering","title":"Clustering\u00b6","text":"<p>It's noteworthy that three of the brightest nearby open clusters align with three of the most prominent hotspots on our stellar density map. This alignment isn't surprising, as a density map naturally highlights concentrations of stars, including known star clusters.</p> <p>Now, let's embark on the journey of exploring additional potential star clusters within our dataset. We'll employ a clustering algorithm known as DBSCAN. DBSCAN is a versatile algorithm capable of identifying clusters within data without prior knowledge of the number of clusters. Furthermore, it's designed to handle noise, such as random stars, ensuring that our focus remains on identifying genuine clusters without being perturbed by nearby stars.</p>"},{"location":"Python/MachineLearningFermi_solutions/","title":"Machine Learning in Astrophysics","text":"In\u00a0[\u00a0]: Copied! <pre>from astropy.io import fits\nfrom astropy.table import Table\nimport pandas as pd\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n</pre> from astropy.io import fits from astropy.table import Table import pandas as pd   import matplotlib.pyplot as plt import seaborn as sns import numpy as np  In\u00a0[\u00a0]: Copied! <pre>!wget https://fermi.gsfc.nasa.gov/ssc/data/access/lat/14yr_catalog/gll_psc_v35.fit\n</pre> !wget https://fermi.gsfc.nasa.gov/ssc/data/access/lat/14yr_catalog/gll_psc_v35.fit <pre>--2024-12-09 15:52:57--  https://fermi.gsfc.nasa.gov/ssc/data/access/lat/14yr_catalog/gll_psc_v35.fit\nResolving fermi.gsfc.nasa.gov (fermi.gsfc.nasa.gov)... 129.164.179.26, 2001:4d0:2310:150::26\nConnecting to fermi.gsfc.nasa.gov (fermi.gsfc.nasa.gov)|129.164.179.26|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 7819200 (7.5M) [application/fits]\nSaving to: \u2018gll_psc_v35.fit\u2019\n\ngll_psc_v35.fit     100%[===================&gt;]   7.46M  3.89MB/s    in 1.9s    \n\n2024-12-09 15:52:59 (3.89 MB/s) - \u2018gll_psc_v35.fit\u2019 saved [7819200/7819200]\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>hdul = fits.open(\"./gll_psc_v35.fit\")\nhdul.info()\n</pre> hdul = fits.open(\"./gll_psc_v35.fit\") hdul.info() <pre>Filename: ./gll_psc_v35.fit\nNo.    Name      Ver    Type      Cards   Dimensions   Format\n  0  PRIMARY       1 PrimaryHDU      24   ()      \n  1  LAT_Point_Source_Catalog    1 BinTableHDU    456   7195R x 79C   [18A, I, E, E, E, E, E, E, E, E, E, E, I, 18A, E, E, E, E, E, E, 17A, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, 8E, 16E, 8E, 8E, E, E, E, E, E, E, D, E, 14E, 28E, 14E, 18A, 18A, 18A, 18A, 18A, 18A, A, 30A, 5A, 10A, 28A, 30A, E, E, D, D, E, I]   \n  2  ExtendedSources    1 BinTableHDU     74   82R x 11C   [17A, E, E, E, E, 11A, E, E, E, 11A, 24A]   \n  3  ROIs          1 BinTableHDU    121   1991R x 10C   [I, E, E, E, E, E, E, E, E, E]   \n  4  Components    1 BinTableHDU     53   19R x 9C   [E, E, I, I, E, E, E, I, I]   \n  5  EnergyBounds    1 BinTableHDU     73   20R x 10C   [E, E, E, I, I, E, E, E, I, I]   \n  6  Hist_Start    1 BinTableHDU     40   15R x 1C   [D]   \n  7  GTI           1 BinTableHDU     37   80251R x 2C   [D, D]   \n</pre> In\u00a0[\u00a0]: Copied! <pre>lat_psc = Table.read(hdul[1])\nlat_psc\n</pre> lat_psc = Table.read(hdul[1]) lat_psc Out[\u00a0]: Table length=7195 Source_NameDataReleaseRAJ2000DEJ2000GLONGLATConf_68_SemiMajorConf_68_SemiMinorConf_68_PosAngConf_95_SemiMajorConf_95_SemiMinorConf_95_PosAngROI_numExtended_Source_NameSignif_AvgPivot_EnergyFlux1000Unc_Flux1000Energy_Flux100Unc_Energy_Flux100SpectrumTypePL_Flux_DensityUnc_PL_Flux_DensityPL_IndexUnc_PL_IndexLP_Flux_DensityUnc_LP_Flux_DensityLP_IndexUnc_LP_IndexLP_betaUnc_LP_betaLP_SigCurvLP_EPeakUnc_LP_EPeakPLEC_Flux_DensityUnc_PLEC_Flux_DensityPLEC_IndexSUnc_PLEC_IndexSPLEC_ExpfactorSUnc_PLEC_ExpfactorSPLEC_Exp_IndexUnc_PLEC_Exp_IndexPLEC_SigCurvPLEC_EPeakUnc_PLEC_EPeakNpredFlux_BandUnc_Flux_BandnuFnu_BandSqrt_TS_BandVariability_IndexFrac_VariabilityUnc_Frac_VariabilitySignif_PeakFlux_PeakUnc_Flux_PeakTime_PeakPeak_IntervalFlux_HistoryUnc_Flux_HistorySqrt_TS_HistoryASSOC_4FGLASSOC_FGLASSOC_FHLASSOC_GAM1ASSOC_GAM2ASSOC_GAM3TEVCAT_FLAGASSOC_TEVCLASS1CLASS2ASSOC1ASSOC2ASSOC_PROB_BAYASSOC_PROB_LRRA_CounterpartDEC_CounterpartUnc_CounterpartFlags degdegdegdegdegdegdegdegdegdegMeVph / (s cm2)ph / (s cm2)erg / (s cm2)erg / (s cm2)ph / (MeV s cm2)ph / (MeV s cm2)ph / (MeV s cm2)ph / (MeV s cm2)MeVMeVph / (MeV s cm2)ph / (MeV s cm2)MeVMeVph / (s cm2)ph / (s cm2)erg / (s cm2)ph / (s cm2)ph / (s cm2)ssph / (s cm2)ph / (s cm2)degdegdeg str18int16float32float32float32float32float32float32float32float32float32float32int16str18float32float32float32float32float32float32str17float32float32float32float32float32float32float32float32float32float32float32float32float32float32float32float32float32float32float32float32float32float32float32float32float32float32[8]float32[8,2]float32[8]float32[8]float32float32float32float32float32float32float64float32float32[14]float32[14,2]float32[14]str18str18str18str18str18str18str1str30str5str10str28str30float32float32float64float64float32int16 4FGL J0000.3-735510.0983-73.9220307.7090-42.72950.03240.0315-62.7000.05250.0510-62.70017268.4931917.721.4796e-102.1770e-111.7352e-122.6915e-13PowerLaw4.2844e-146.2686e-152.24740.11744.8417e-148.4813e-152.12800.19690.11000.11191.1181071.711443.504.8620e-148.0372e-152.04890.20700.182320.151020.6667--1.3291427.562230.54411.912.0565643e-08 .. 5.1613483e-17-1.6676287e-08 .. 2.0635616e-123.2624888e-12 .. 8.333941e-181.1720686 .. 0.012.83500.000010.0000-inf-inf-inf-inf-inf6.084946e-12 .. 3.1096483e-09-- .. 1.5917939e-090.0 .. 2.41333724FGL J0000.3-7355N0.00000.0000------0 4FGL J0000.5+074320.13757.7273101.6565-53.02950.09450.0700-10.2200.15330.1135-10.2201065.6811379.881.5541e-103.0373e-111.9306e-123.7501e-13PowerLaw9.7825e-141.8354e-142.33080.13101.1449e-132.4667e-142.24700.19130.10970.10651.194447.47737.201.0522e-132.1944e-142.24730.17890.065670.103800.6667--0.000----384.923.750181e-12 .. 9.930329e-16-- .. 2.7929685e-125.929706e-16 .. 1.554265e-160.0 .. 0.025.56900.56170.2739-inf-inf-inf-inf-inf1.5356434e-09 .. 2.4428466e-09-1.1904873e-09 .. 2.4088174e-091.4323782 .. 1.20263874FGL J0000.5+0743N0.00000.0000------0 4FGL J0000.7+253030.187825.5153108.7751-35.95920.05140.041082.4000.08330.066582.40013444.1975989.266.7792e-112.2785e-118.0505e-132.4709e-13PowerLaw2.1343e-156.5433e-161.85620.19343.0265e-151.0188e-151.80190.32990.25110.18141.6898885.666262.713.3732e-151.1067e-151.62500.38540.688000.409560.6667--1.7919534.834121.6595.185.159399e-14 .. 9.303994e-14-- .. 3.4502763e-128.312768e-18 .. 1.7575507e-140.0 .. 0.01672322713.14930.000010.0000-inf-inf-inf-inf-inf1.3094809e-09 .. 5.0288596e-10-6.474343e-10 .. 7.7705853e-103.7604396 .. 0.9907554FGL J0000.7+2530N0.00000.0000------0 4FGL J0001.2+474110.312647.6859114.2502-14.33810.03690.0332-45.9000.05980.0538-45.9006685.5243218.101.2155e-102.6822e-111.3691e-123.2222e-13PowerLaw1.1363e-142.4424e-152.15510.15391.2383e-143.1681e-152.11420.19070.04980.09660.3941022.073515.221.1934e-142.8185e-152.09400.18560.042270.084830.6667--0.000----271.441.964253e-11 .. 1.6018216e-18-- .. 2.118955e-123.1274286e-15 .. 2.6799535e-190.0 .. 0.028.91560.66390.33314.7975.7389635e-091.7132418e-09286670016.031560000.02.7438956e-09 .. 3.4820546e-15-1.5440191e-09 .. 9.97245e-101.9989107 .. 0.04FGL J0001.2+4741NbcuB3 2358+4740.99610.93860.329347.70020.000000 4FGL J0001.2-074710.3151-7.797189.0327-67.30500.01840.017664.1000.02990.028564.10067124.4972065.157.0245e-104.6535e-117.8076e-125.1475e-13PowerLaw1.6904e-131.1110e-142.08160.05011.7784e-131.3428e-142.05250.06510.03790.03101.2831033.511244.781.7597e-131.2692e-142.02880.06760.040950.033880.6667--1.171797.883014.201204.731.6641469e-12 .. 1.2048088e-12-- .. 3.695932e-122.657329e-16 .. 2.0750411e-130.0 .. 0.8472246551.74240.42910.110910.2271.632524e-082.33745e-09349790016.031560000.08.462245e-09 .. 3.470081e-09-1.5965307e-09 .. 1.688124e-097.926063 .. 3.11380534FGL J0001.2-07473FGL J0001.2-07483FHL J0001.2-0748NbllPMN J0001-07460.99700.93290.3251-7.77410.000000 4FGL J0001.4-001030.3717-0.169996.8920-60.49130.03650.0363-52.7400.05920.0589-52.74015065.2522869.261.1773e-102.7161e-111.3124e-123.1021e-13PowerLaw1.4221e-143.2475e-152.10900.18371.3973e-143.6007e-152.11800.1871-0.01020.06920.000----1.4730e-143.5671e-152.04460.25810.032800.086140.6667--0.00081.309030.00207.881.7131245e-13 .. 3.4016991e-16-- .. 2.8853061e-122.732577e-17 .. 5.7954914e-170.0 .. 0.09.37420.000010.0000-inf-inf-inf-inf-inf1.2336494e-09 .. 7.619416e-10-9.843029e-10 .. 1.7601309e-091.3531672 .. 0.474803784FGL J0001.4-0010NbllFBQS J0001-00110.99160.81770.3395-0.19440.004500 4FGL J0001.5+211310.381521.2183107.6494-40.16770.02600.0240-60.5200.04220.0389-60.520134446.426340.921.1855e-095.4453e-112.2820e-116.3708e-13LogParabola3.8179e-111.0140e-122.67040.02174.3688e-111.3399e-122.54550.03320.14890.02157.54954.6018.404.2284e-111.2519e-122.50600.03380.184540.028200.6667--7.787----5749.196.6762965e-08 .. 6.1398297e-15-2.8545768e-08 .. 2.5565743e-121.0662745e-11 .. 5.91162e-162.3333735 .. 0.01861.60341.08820.214638.1402.0028273e-076.6710975e-09476030016.031560000.05.10925e-09 .. 1.1947686e-08-2.8717098e-09 .. 3.7160157e-091.829908 .. 3.55508924FGL J0001.5+21133FGL J0001.4+21203EG J2359+2041NfsrqTXS 2358+2090.99800.95820.384921.22670.000000 4FGL J0001.6+350330.409835.0573111.5381-26.71030.06290.0501-49.1000.10200.0813-49.1007806.4751091.061.8919e-103.4430e-111.3373e-123.6359e-13LogParabola1.7218e-132.8462e-142.44930.11932.3079e-134.2560e-142.15490.29680.32420.18412.418859.26482.522.3266e-134.2877e-141.95270.34020.654480.314350.6667--2.8661170.92554.37335.831.1733832e-08 .. 1.0613527e-15-8.083276e-09 .. 2.386919e-122.0044877e-12 .. 9.833707e-171.4500567 .. 0.011.19870.000010.0000-inf-inf-inf-inf-inf3.207077e-09 .. 2.2588191e-09-1.012114e-09 .. 1.0322188e-093.726184 .. 2.62584FGL J0001.6+3503N0.00000.0000------0 4FGL J0001.6-415610.4165-41.9425334.2263-72.02850.04270.032444.0900.06920.052544.090134917.9822729.533.1268e-103.1619e-113.2608e-124.2668e-13LogParabola3.9823e-143.9064e-151.78110.06404.5783e-145.2111e-151.58100.12480.12090.05672.63015441.658987.764.3217e-144.7601e-151.58660.11940.098210.060490.6667--2.33020271.2910459.36294.071.8588997e-10 .. 1.5726335e-12-- .. 2.2862857e-123.1364227e-14 .. 2.1498715e-130.037754785 .. 1.582419925.60950.36890.1471-inf-inf-inf-inf-inf1.2599416e-09 .. 1.3015212e-09-3.4846484e-10 .. 3.8189454e-105.4899836 .. 6.5241674FGL J0001.6-41563FGL J0002.2-41523FHL J0001.9-4155Nbcu2MASS J00013275-41552520.99630.85460.3865-41.92370.000070 ............................................................................................................................................................................................................................................. 4FGL J2359.1+17191359.775617.3225105.5174-43.77250.03340.0313-18.9900.05420.0507-18.9902387.1433016.571.4012e-102.6541e-111.5609e-122.9398e-13PowerLaw1.5503e-142.9193e-152.01960.12551.8969e-144.2185e-151.90660.18210.12190.09811.3764424.302999.351.7827e-143.7740e-151.86980.18730.115080.112320.6667--0.9117009.595601.37229.351.8924191e-10 .. 7.098173e-17-- .. 2.6471787e-123.0292853e-14 .. 1.2534039e-170.0 .. 0.09.73120.000010.0000-inf-inf-inf-inf-inf1.0522296e-09 .. 2.5952473e-11-7.9103996e-10 .. 1.2582255e-091.3821346 .. 0.04FGL J2359.1+1719NbcuNVSS J235901+171926--0.0000359.754917.32380.000000 4FGL J2359.2-31342359.8167-31.58328.4116-77.80100.06090.04831.9300.09870.07831.93017639.180571.071.4652e-102.3966e-112.9069e-123.6328e-13PowerLaw1.1519e-121.3677e-132.71650.09871.1765e-121.6588e-132.71370.10610.02150.06100.234----1.1584e-121.4892e-132.70650.11070.014390.058860.6667--0.000----900.229.0763175e-09 .. 1.1232852e-17-- .. 3.1554514e-121.4137116e-12 .. 1.5420619e-180.5659317 .. 0.095.40961.06900.241710.4472.889726e-083.3430512e-09507590016.031560000.04.6589457e-09 .. 6.420198e-10-2.7989315e-09 .. 2.8244502e-091.7039856 .. 0.263492354FGL J2359.2-3134NfsrqPKS 2357-3180.98170.9533359.8979-31.56220.0000016 4FGL J2359.3+02152359.83292.260397.7490-58.04090.03740.033449.5200.06060.054149.52015068.5785153.451.4273e-102.9032e-111.3542e-122.8231e-13LogParabola5.6610e-151.0589e-151.75250.10669.8337e-152.0377e-151.57790.20990.36690.13113.7879160.702833.189.8472e-152.0597e-151.35690.23290.694270.262010.6667--3.93410602.072587.0983.816.12997e-12 .. 5.640288e-17-- .. 2.8466034e-121.0625808e-15 .. 5.4204736e-180.0 .. 0.018.01040.25970.5599-inf-inf-inf-inf-inf2.5990912e-10 .. 3.5572528e-10-1.0145343e-10 .. 1.8114238e-104.1475573 .. 3.69242844FGL J2359.3+0215Nbcu1RXS J235916.9+0215050.99760.8404359.82102.25560.004500 4FGL J2359.3+25023359.834225.0416108.2474-36.34010.04320.036067.5400.07000.058367.54013445.5593147.201.1876e-102.6855e-111.3272e-123.1223e-13PowerLaw1.1758e-142.6006e-152.12310.16751.3738e-143.5773e-152.01500.28070.13520.16241.0032977.493197.771.3975e-143.5652e-151.90330.31880.273940.283340.6667--1.0634320.853543.40235.131.2702883e-13 .. 5.382678e-16-- .. 2.9702519e-122.0250846e-17 .. 9.1195385e-170.0 .. 0.022.05150.47160.3875-inf-inf-inf-inf-inf1.4257835e-09 .. 2.2204516e-09-1.298539e-09 .. 1.5770656e-091.1429439 .. 2.36923124FGL J2359.3+2502N0.00000.0000------0 4FGL J2359.3-20491359.8357-20.818958.0901-76.54290.02460.0215-73.9000.03990.0348-73.900169318.2892930.843.5826e-103.2927e-114.0940e-123.8947e-13PowerLaw4.2403e-143.8934e-151.92850.06934.2834e-144.6351e-151.92430.08070.00440.03360.00015503608.00950889600.004.3602e-144.2689e-151.89010.08960.019180.027780.6667--0.00031028.6244514.93533.914.9757958e-09 .. 1.948438e-12-- .. 2.6153853e-127.9938936e-13 .. 3.5714053e-130.22761072 .. 2.100163514.49820.06830.3772-inf-inf-inf-inf-inf2.975037e-09 .. 3.769433e-09-1.0253463e-09 .. 1.0628519e-094.0335164 .. 7.02983954FGL J2359.3-20493FGL J2359.5-20523FHL J2359.3-2049NbllTXS 2356-2100.99450.9715359.8314-20.79890.000002 4FGL J2359.3+14441359.839014.7498104.5647-46.25630.13670.0884-65.4000.22170.1433-65.4003457.791581.711.7027e-103.1031e-112.3558e-124.3196e-13LogParabola1.0647e-121.4439e-132.63850.10261.3274e-122.0172e-132.46650.18910.22300.11742.510204.39174.471.3466e-122.0285e-132.30270.22840.463390.231630.6667--2.765246.73321.64601.491.0087045e-09 .. 6.621411e-15-- .. 2.7959755e-121.6467336e-13 .. 6.134908e-160.0 .. 0.013.56200.16930.3996-inf-inf-inf-inf-inf8.7565954e-09 .. 3.6501813e-09-2.3720672e-09 .. 2.5192155e-094.02097 .. 1.61157294FGL J2359.3+1444N0.24000.0000------2 4FGL J2359.7-50413359.9365-50.6853322.1281-64.47280.13070.103984.4900.21190.168584.4904675.150717.057.9821e-112.0142e-111.1160e-122.7356e-13LogParabola2.9541e-136.1210e-142.71440.14474.6061e-139.2680e-142.63990.25800.44660.18363.037350.25150.834.6789e-139.3302e-142.47240.27980.938980.386150.6667--3.199388.47219.94357.772.112895e-08 .. 3.74138e-15-1.4760097e-08 .. 2.332284e-123.5827396e-12 .. 3.4664855e-161.4511985 .. 0.06.93430.000010.0000-inf-inf-inf-inf-inf2.6759004e-09 .. 1.3225839e-09-1.5685266e-09 .. 1.4912929e-091.7924763 .. 0.9006634FGL J2359.7-5041NbcuAT20G J235947-5042330.96210.8819359.9484-50.70930.004500 4FGL J2359.9-37361359.9816-37.6160345.6628-74.91960.05080.0435-58.7700.08230.0706-58.77025711.4761490.852.4481e-103.0261e-111.7523e-122.8096e-13LogParabola1.0050e-131.1950e-142.09490.08331.2366e-131.6873e-141.90440.15050.18590.08682.7741927.75685.901.1698e-131.6267e-141.86860.15220.202600.138070.6667--2.3852556.011083.52326.328.510176e-14 .. 1.5588352e-19-- .. 3.1501399e-121.4317436e-17 .. 1.637076e-200.0 .. 0.08.08280.000010.0000-inf-inf-inf-inf-inf1.535316e-09 .. 1.7728115e-09-5.864724e-10 .. 6.528685e-103.3132436 .. 4.2129424FGL J2359.9-37363FGL J0000.2-3738NbcuNVSS J000008-3738190.99130.00000.0351-37.63910.000000 4FGL J2359.9+31453359.990831.7601110.3210-29.84800.01890.0179-74.1000.03070.0291-74.10025411.3073988.552.0863e-102.9220e-112.4250e-123.3327e-13PowerLaw1.3804e-141.8803e-151.89280.09611.6659e-142.7553e-151.75450.15880.13970.09151.8779603.135398.041.6304e-142.6227e-151.68440.16880.194690.133070.6667--1.86911969.665392.77317.271.8725167e-10 .. 7.6339624e-13-- .. 3.1282615e-123.0125733e-14 .. 1.4201306e-130.0 .. 0.535576919.20510.35690.2332-inf-inf-inf-inf-inf3.0808625e-10 .. 1.5011231e-09-- .. 8.147619e-100.54794914 .. 3.5265514FGL J2359.9+3145NbcuNVSS J235955+3145580.99940.8929359.980431.76670.004500 In\u00a0[\u00a0]: Copied! <pre>names = [name for name in lat_psc.colnames if len(lat_psc[name].shape) &lt;= 1]\ndf_psc = lat_psc[names].to_pandas()\n</pre> names = [name for name in lat_psc.colnames if len(lat_psc[name].shape) &lt;= 1] df_psc = lat_psc[names].to_pandas() In\u00a0[\u00a0]: Copied! <pre>print (f\"We have a total of {len(df_psc)} entries\")\n</pre> print (f\"We have a total of {len(df_psc)} entries\") <pre>We have a total of 7195 entries\n</pre> In\u00a0[\u00a0]: Copied! <pre>df_psc.head()\n</pre> df_psc.head() Out[\u00a0]: Source_Name DataRelease RAJ2000 DEJ2000 GLON GLAT Conf_68_SemiMajor Conf_68_SemiMinor Conf_68_PosAng Conf_95_SemiMajor ... CLASS1 CLASS2 ASSOC1 ASSOC2 ASSOC_PROB_BAY ASSOC_PROB_LR RA_Counterpart DEC_Counterpart Unc_Counterpart Flags 0 4FGL J0000.3-7355 1 0.0983 -73.921997 307.708984 -42.729538 0.032378 0.031453 -62.700001 0.0525 ... 0.000000 0.000000 NaN NaN NaN 0 1 4FGL J0000.5+0743 2 0.1375 7.727300 101.656479 -53.029457 0.094544 0.069999 -10.220000 0.1533 ... 0.000000 0.000000 NaN NaN NaN 0 2 4FGL J0000.7+2530 3 0.1878 25.515301 108.775070 -35.959175 0.051373 0.041012 82.400002 0.0833 ... 0.000000 0.000000 NaN NaN NaN 0 3 4FGL J0001.2+4741 1 0.3126 47.685902 114.250198 -14.338059 0.036880 0.033180 -45.900002 0.0598 ... bcu B3 2358+474 0.996097 0.938563 0.329341 47.700201 8.400000e-07 0 4 4FGL J0001.2-0747 1 0.3151 -7.797100 89.032722 -67.305008 0.018440 0.017577 64.099998 0.0299 ... bll PMN J0001-0746 0.997014 0.932932 0.325104 -7.774145 1.800000e-07 0 <p>5 rows \u00d7 72 columns</p> In\u00a0[\u00a0]: Copied! <pre>source_types = set(df_psc[\"CLASS1\"])\nsource_types\n</pre> source_types = set(df_psc[\"CLASS1\"]) source_types Out[\u00a0]: <pre>{'     ',\n 'AGN  ',\n 'BCU  ',\n 'BIN  ',\n 'BLL  ',\n 'FSRQ ',\n 'GAL  ',\n 'GC   ',\n 'HMB  ',\n 'LMB  ',\n 'MSP  ',\n 'NLSY1',\n 'NOV  ',\n 'PSR  ',\n 'PWN  ',\n 'RDG  ',\n 'SFR  ',\n 'SNR  ',\n 'SPP  ',\n 'UNK  ',\n 'agn  ',\n 'bcu  ',\n 'bin  ',\n 'bll  ',\n 'css  ',\n 'fsrq ',\n 'gal  ',\n 'glc  ',\n 'hmb  ',\n 'lmb  ',\n 'msp  ',\n 'nlsy1',\n 'psr  ',\n 'pwn  ',\n 'rdg  ',\n 'sbg  ',\n 'sey  ',\n 'sfr  ',\n 'snr  ',\n 'spp  ',\n 'ssrq ',\n 'unk  '}</pre> <p>Let's write a function to convert the string <code>CLASS1</code> to lowercase, remove trailing spaces, and label anything unidentified as <code>'unk'</code>.</p> In\u00a0[\u00a0]: Copied! <pre>def reclassify(row):\n    # Lower converts to lower case\n    # Strip removes trailing white space\n    cl = row.lower().strip()\n    if cl == '':\n        cl = 'unk'\n    return cl\n</pre> def reclassify(row):     # Lower converts to lower case     # Strip removes trailing white space     cl = row.lower().strip()     if cl == '':         cl = 'unk'     return cl   In\u00a0[\u00a0]: Copied! <pre>df_psc[\"CLASS1\"] = df_psc[\"CLASS1\"].map(reclassify)\ndf_psc.head()\n</pre> df_psc[\"CLASS1\"] = df_psc[\"CLASS1\"].map(reclassify) df_psc.head() Out[\u00a0]: Source_Name DataRelease RAJ2000 DEJ2000 GLON GLAT Conf_68_SemiMajor Conf_68_SemiMinor Conf_68_PosAng Conf_95_SemiMajor ... CLASS1 CLASS2 ASSOC1 ASSOC2 ASSOC_PROB_BAY ASSOC_PROB_LR RA_Counterpart DEC_Counterpart Unc_Counterpart Flags 0 4FGL J0000.3-7355 1 0.0983 -73.921997 307.708984 -42.729538 0.032378 0.031453 -62.700001 0.0525 ... unk 0.000000 0.000000 NaN NaN NaN 0 1 4FGL J0000.5+0743 2 0.1375 7.727300 101.656479 -53.029457 0.094544 0.069999 -10.220000 0.1533 ... unk 0.000000 0.000000 NaN NaN NaN 0 2 4FGL J0000.7+2530 3 0.1878 25.515301 108.775070 -35.959175 0.051373 0.041012 82.400002 0.0833 ... unk 0.000000 0.000000 NaN NaN NaN 0 3 4FGL J0001.2+4741 1 0.3126 47.685902 114.250198 -14.338059 0.036880 0.033180 -45.900002 0.0598 ... bcu B3 2358+474 0.996097 0.938563 0.329341 47.700201 8.400000e-07 0 4 4FGL J0001.2-0747 1 0.3151 -7.797100 89.032722 -67.305008 0.018440 0.017577 64.099998 0.0299 ... bll PMN J0001-0746 0.997014 0.932932 0.325104 -7.774145 1.800000e-07 0 <p>5 rows \u00d7 72 columns</p> <p>Convert the spectrum type from strings to numbers:</p> In\u00a0[\u00a0]: Copied! <pre>spectra = list(set(df_psc[\"SpectrumType\"]))\ndef class_spectrum(row):\n    for i, cl in enumerate(spectra):\n        if row == cl:\n            break\n    return i\n\ndf_psc[\"SpectrumType\"] = df_psc[\"SpectrumType\"].map(class_spectrum)\n</pre> spectra = list(set(df_psc[\"SpectrumType\"])) def class_spectrum(row):     for i, cl in enumerate(spectra):         if row == cl:             break     return i  df_psc[\"SpectrumType\"] = df_psc[\"SpectrumType\"].map(class_spectrum) <p></p> In\u00a0[\u00a0]: Copied! <pre>labels = df_psc[\"CLASS1\"]\nall_labels = set(labels)\n# -1 because we won't be classifying 'unk' sources\nprint (f\"We have {len(all_labels)-1} class types\")\nprint (all_labels)\n</pre> labels = df_psc[\"CLASS1\"] all_labels = set(labels) # -1 because we won't be classifying 'unk' sources print (f\"We have {len(all_labels)-1} class types\") print (all_labels) <pre>We have 23 class types\n{'rdg', 'unk', 'sfr', 'nov', 'psr', 'ssrq', 'gal', 'gc', 'snr', 'msp', 'css', 'sey', 'nlsy1', 'sbg', 'hmb', 'fsrq', 'pwn', 'bcu', 'bll', 'glc', 'bin', 'spp', 'agn', 'lmb'}\n</pre> In\u00a0[\u00a0]: Copied! <pre># Remove the association and counter part columns\nkeys = [k for k in df_psc.columns if (\"ASSOC\" not in k and \"Counterpart\" not in k)]\nprint (keys)\n</pre> # Remove the association and counter part columns keys = [k for k in df_psc.columns if (\"ASSOC\" not in k and \"Counterpart\" not in k)] print (keys)  <pre>['Source_Name', 'DataRelease', 'RAJ2000', 'DEJ2000', 'GLON', 'GLAT', 'Conf_68_SemiMajor', 'Conf_68_SemiMinor', 'Conf_68_PosAng', 'Conf_95_SemiMajor', 'Conf_95_SemiMinor', 'Conf_95_PosAng', 'ROI_num', 'Extended_Source_Name', 'Signif_Avg', 'Pivot_Energy', 'Flux1000', 'Unc_Flux1000', 'Energy_Flux100', 'Unc_Energy_Flux100', 'SpectrumType', 'PL_Flux_Density', 'Unc_PL_Flux_Density', 'PL_Index', 'Unc_PL_Index', 'LP_Flux_Density', 'Unc_LP_Flux_Density', 'LP_Index', 'Unc_LP_Index', 'LP_beta', 'Unc_LP_beta', 'LP_SigCurv', 'LP_EPeak', 'Unc_LP_EPeak', 'PLEC_Flux_Density', 'Unc_PLEC_Flux_Density', 'PLEC_IndexS', 'Unc_PLEC_IndexS', 'PLEC_ExpfactorS', 'Unc_PLEC_ExpfactorS', 'PLEC_Exp_Index', 'Unc_PLEC_Exp_Index', 'PLEC_SigCurv', 'PLEC_EPeak', 'Unc_PLEC_EPeak', 'Npred', 'Variability_Index', 'Frac_Variability', 'Unc_Frac_Variability', 'Signif_Peak', 'Flux_Peak', 'Unc_Flux_Peak', 'Time_Peak', 'Peak_Interval', 'TEVCAT_FLAG', 'CLASS1', 'CLASS2', 'Flags']\n</pre> In\u00a0[\u00a0]: Copied! <pre># Remove identifing information\nexclude = [\n    'DataRelease',\n    'Conf_68_PosAng', 'Conf_95_PosAng',\n    'ROI_num', 'Extended_Source_Name',\n    'RAJ2000', 'DEJ2000',  # We'll take the Galactic Lon (GLON) and Galactic Latitude (GLAT)\n    'TEVCAT_FLAG', 'CLASS2', 'Flags',\n    'Unc_Frac_Variability', # Uncertinty in how variable it it. This isn't well-defined\n    'Time_Peak', 'Peak_Interval', # If the source is variable, we don't care when it was at it's brightest\n    'Unc_PLEC_Exp_Index', 'Signif_Peak'\n          ]\n\nkeys = [k for k in keys if k not in exclude]\nprint (keys)\n</pre> # Remove identifing information exclude = [     'DataRelease',     'Conf_68_PosAng', 'Conf_95_PosAng',     'ROI_num', 'Extended_Source_Name',     'RAJ2000', 'DEJ2000',  # We'll take the Galactic Lon (GLON) and Galactic Latitude (GLAT)     'TEVCAT_FLAG', 'CLASS2', 'Flags',     'Unc_Frac_Variability', # Uncertinty in how variable it it. This isn't well-defined     'Time_Peak', 'Peak_Interval', # If the source is variable, we don't care when it was at it's brightest     'Unc_PLEC_Exp_Index', 'Signif_Peak'           ]  keys = [k for k in keys if k not in exclude] print (keys) <pre>['Source_Name', 'GLON', 'GLAT', 'Conf_68_SemiMajor', 'Conf_68_SemiMinor', 'Conf_95_SemiMajor', 'Conf_95_SemiMinor', 'Signif_Avg', 'Pivot_Energy', 'Flux1000', 'Unc_Flux1000', 'Energy_Flux100', 'Unc_Energy_Flux100', 'SpectrumType', 'PL_Flux_Density', 'Unc_PL_Flux_Density', 'PL_Index', 'Unc_PL_Index', 'LP_Flux_Density', 'Unc_LP_Flux_Density', 'LP_Index', 'Unc_LP_Index', 'LP_beta', 'Unc_LP_beta', 'LP_SigCurv', 'LP_EPeak', 'Unc_LP_EPeak', 'PLEC_Flux_Density', 'Unc_PLEC_Flux_Density', 'PLEC_IndexS', 'Unc_PLEC_IndexS', 'PLEC_ExpfactorS', 'Unc_PLEC_ExpfactorS', 'PLEC_Exp_Index', 'PLEC_SigCurv', 'PLEC_EPeak', 'Unc_PLEC_EPeak', 'Npred', 'Variability_Index', 'Frac_Variability', 'Flux_Peak', 'Unc_Flux_Peak', 'CLASS1']\n</pre> In\u00a0[\u00a0]: Copied! <pre>df_reduced = df_psc[keys]\ndf_reduced.head()\n</pre> df_reduced = df_psc[keys] df_reduced.head() Out[\u00a0]: Source_Name GLON GLAT Conf_68_SemiMajor Conf_68_SemiMinor Conf_95_SemiMajor Conf_95_SemiMinor Signif_Avg Pivot_Energy Flux1000 ... PLEC_Exp_Index PLEC_SigCurv PLEC_EPeak Unc_PLEC_EPeak Npred Variability_Index Frac_Variability Flux_Peak Unc_Flux_Peak CLASS1 0 4FGL J0000.3-7355 307.708984 -42.729538 0.032378 0.031453 0.0525 0.0510 8.492646 1917.715454 1.479606e-10 ... 0.666667 1.329241 1427.556396 2230.540283 411.909851 12.834996 0.000000 -inf -inf unk 1 4FGL J0000.5+0743 101.656479 -53.029457 0.094544 0.069999 0.1533 0.1135 5.681097 1379.882935 1.554103e-10 ... 0.666667 0.000000 NaN NaN 384.919708 25.568989 0.561723 -inf -inf unk 2 4FGL J0000.7+2530 108.775070 -35.959175 0.051373 0.041012 0.0833 0.0665 4.197268 5989.263184 6.779151e-11 ... 0.666667 1.791354 9534.827148 4121.653320 95.184700 13.149277 0.000000 -inf -inf unk 3 4FGL J0001.2+4741 114.250198 -14.338059 0.036880 0.033180 0.0598 0.0538 5.523873 3218.100098 1.215529e-10 ... 0.666667 0.000000 NaN NaN 271.438385 28.915590 0.663905 5.738964e-09 1.713242e-09 bcu 4 4FGL J0001.2-0747 89.032722 -67.305008 0.018440 0.017577 0.0299 0.0285 24.497219 2065.146729 7.024454e-10 ... 0.666667 1.171456 797.877136 3014.195312 1204.733154 51.742390 0.429135 1.632524e-08 2.337450e-09 bll <p>5 rows \u00d7 43 columns</p> In\u00a0[\u00a0]: Copied! <pre>df_reduced.describe()\n</pre> df_reduced.describe() <pre>/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py:4655: RuntimeWarning: invalid value encountered in subtract\n  diff_b_a = subtract(b, a)\n/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py:4655: RuntimeWarning: invalid value encountered in subtract\n  diff_b_a = subtract(b, a)\n</pre> Out[\u00a0]: GLON GLAT Conf_68_SemiMajor Conf_68_SemiMinor Conf_95_SemiMajor Conf_95_SemiMinor Signif_Avg Pivot_Energy Flux1000 Unc_Flux1000 ... Unc_PLEC_ExpfactorS PLEC_Exp_Index PLEC_SigCurv PLEC_EPeak Unc_PLEC_EPeak Npred Variability_Index Frac_Variability Flux_Peak Unc_Flux_Peak count 7195.000000 7195.000000 7105.000000 7105.000000 7105.000000 7105.000000 7194.000000 7195.000000 7.195000e+03 7.194000e+03 ... 7.191000e+03 7191.000000 7195.000000 4.788000e+03 4.788000e+03 7195.000000 7.195000e+03 7195.000000 7.195000e+03 7.195000e+03 mean 183.132599 0.743767 0.057264 0.044649 0.092851 0.072397 15.701175 2733.545410 1.350051e-09 6.980248e-11 ... 1.710960e-01 0.665899 2.694271 2.591914e+04 1.372819e+06 1427.819580 -inf -inf -inf -inf std 112.117607 35.936916 0.046511 0.030814 0.075415 0.049963 29.390438 3966.514160 1.840191e-08 1.087079e-10 ... 1.483610e-01 0.017649 6.128449 6.712488e+05 8.194140e+07 5640.877930 NaN NaN NaN NaN min 0.042404 -87.969360 0.004502 0.004440 0.007300 0.007200 0.024616 54.042686 1.015092e-11 7.016551e-12 ... 8.172455e-36 0.221034 0.000000 3.961106e-02 8.722919e+00 11.915721 -inf -inf -inf -inf 25% 82.476997 -21.873463 0.028184 0.025039 0.045700 0.040600 5.280337 1105.388672 1.467361e-10 2.865141e-11 ... 5.248953e-02 0.666667 0.465931 5.662150e+02 4.002390e+02 325.631378 1.143453e+01 0.000000 NaN NaN 50% 182.273224 -0.004612 0.044651 0.037189 0.072400 0.060300 7.794947 1789.375977 2.770740e-10 3.981436e-11 ... 1.334729e-01 0.666667 1.816682 1.856001e+03 1.170048e+03 595.782593 1.641460e+01 0.166052 NaN NaN 75% 287.337677 24.734880 0.072280 0.055382 0.117200 0.089800 14.657692 3136.032715 6.448548e-10 6.737097e-11 ... 2.692976e-01 0.666667 3.218210 9.392703e+03 5.534194e+03 1061.443237 2.814625e+01 0.467705 2.038295e-09 4.553143e-10 max 359.992157 88.682228 0.533161 0.380027 0.864500 0.616200 897.974976 173796.171875 1.330866e-06 3.615690e-09 ... 3.096419e+00 1.000000 257.052399 4.560933e+07 5.650350e+09 284295.468750 9.603806e+04 3.661907 5.058845e-06 3.490365e-08 <p>8 rows \u00d7 41 columns</p> In\u00a0[\u00a0]: Copied! <pre>col_names = set(df_reduced.columns)\nnum_cols = set(df_reduced._get_numeric_data())\ncat_cols = col_names - num_cols\n\nnum_cols = list(num_cols)\ncat_cols = list(cat_cols)\n\nprint (\"Numerical Columns\")\nprint (num_cols)\nprint (\"\\n-----\\n\")\n\nprint (\"Catagorical Columns\")\nprint (cat_cols)\n</pre> col_names = set(df_reduced.columns) num_cols = set(df_reduced._get_numeric_data()) cat_cols = col_names - num_cols  num_cols = list(num_cols) cat_cols = list(cat_cols)  print (\"Numerical Columns\") print (num_cols) print (\"\\n-----\\n\")  print (\"Catagorical Columns\") print (cat_cols) <pre>Numerical Columns\n['LP_Flux_Density', 'LP_EPeak', 'Npred', 'Unc_LP_Flux_Density', 'GLON', 'LP_SigCurv', 'GLAT', 'Unc_Energy_Flux100', 'Conf_95_SemiMajor', 'PLEC_Flux_Density', 'Conf_95_SemiMinor', 'Unc_PLEC_ExpfactorS', 'PLEC_ExpfactorS', 'Unc_LP_Index', 'Unc_LP_beta', 'Unc_PLEC_IndexS', 'PLEC_Exp_Index', 'Variability_Index', 'Unc_PL_Flux_Density', 'SpectrumType', 'Pivot_Energy', 'PLEC_EPeak', 'Flux1000', 'LP_Index', 'Conf_68_SemiMinor', 'Energy_Flux100', 'Signif_Avg', 'LP_beta', 'PL_Index', 'Flux_Peak', 'Unc_Flux1000', 'Frac_Variability', 'Unc_Flux_Peak', 'PL_Flux_Density', 'PLEC_SigCurv', 'PLEC_IndexS', 'Unc_PLEC_EPeak', 'Unc_PLEC_Flux_Density', 'Conf_68_SemiMajor', 'Unc_PL_Index', 'Unc_LP_EPeak']\n\n-----\n\nCatagorical Columns\n['Source_Name', 'CLASS1']\n</pre> In\u00a0[\u00a0]: Copied! <pre># Calculate the correlation matrix\ncormat = df_reduced[num_cols].corr()\n\nfig = plt.figure(figsize = (24,24))\n# Set the colormap and color range\ncmap = sns.color_palette(\"coolwarm\", as_cmap=True)  # You can change the colormap as needed\nvmin, vmax = -1, 1  # Set the range for the color scale\n\n# Create the correlation heatmap\nplt.figure(figsize=(10, 8))  # Set the figure size\nsns.heatmap(cormat, cmap=cmap, vmin=vmin, vmax=vmax, annot=False, fmt=\".2f\")  # You can add annotations with fmt=\".2f\"\n\n# Set plot title\nplt.title(\"Correlation Heatmap\")\n\n# Show the plot\nplt.show()\n</pre> # Calculate the correlation matrix cormat = df_reduced[num_cols].corr()  fig = plt.figure(figsize = (24,24)) # Set the colormap and color range cmap = sns.color_palette(\"coolwarm\", as_cmap=True)  # You can change the colormap as needed vmin, vmax = -1, 1  # Set the range for the color scale  # Create the correlation heatmap plt.figure(figsize=(10, 8))  # Set the figure size sns.heatmap(cormat, cmap=cmap, vmin=vmin, vmax=vmax, annot=False, fmt=\".2f\")  # You can add annotations with fmt=\".2f\"  # Set plot title plt.title(\"Correlation Heatmap\")  # Show the plot plt.show() <pre>&lt;Figure size 2400x2400 with 0 Axes&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>exclude_corr = [\n    'Unc_PLEC_IndexS', 'Unc_PL_Index', 'LP_Flux_Density', 'Unc_Flux_Peak',\n    'Unc_Energy_Flux100', 'PLEC_Flux_Density', 'Conf_95_SemiMinor', 'Unc_LP_EPeak',\n    'PLEC_SigCurv', 'Conf_68_SemiMajor',\n    'Pivot_Energy', 'PL_Flux_Density', 'Conf_68_SemiMinor', 'Energy_Flux100',\n    'Unc_PL_Flux_Density', 'Unc_LP_Index', 'Unc_PLEC_EPeak', 'Unc_LP_beta', 'LP_EPeak',\n    'PLEC_EPeak', 'PLEC_IndexS', 'Unc_PLEC_ExpfactorS', 'Unc_Flux1000', 'Unc_LP_Flux_Density',\n    'Unc_PLEC_Flux_Density', 'Conf_95_SemiMajor'\n]\n\nkeys = [k for k in df_reduced.columns if k not in exclude_corr]\ndf_reduced_corr = df_reduced[keys]\n</pre> exclude_corr = [     'Unc_PLEC_IndexS', 'Unc_PL_Index', 'LP_Flux_Density', 'Unc_Flux_Peak',     'Unc_Energy_Flux100', 'PLEC_Flux_Density', 'Conf_95_SemiMinor', 'Unc_LP_EPeak',     'PLEC_SigCurv', 'Conf_68_SemiMajor',     'Pivot_Energy', 'PL_Flux_Density', 'Conf_68_SemiMinor', 'Energy_Flux100',     'Unc_PL_Flux_Density', 'Unc_LP_Index', 'Unc_PLEC_EPeak', 'Unc_LP_beta', 'LP_EPeak',     'PLEC_EPeak', 'PLEC_IndexS', 'Unc_PLEC_ExpfactorS', 'Unc_Flux1000', 'Unc_LP_Flux_Density',     'Unc_PLEC_Flux_Density', 'Conf_95_SemiMajor' ]  keys = [k for k in df_reduced.columns if k not in exclude_corr] df_reduced_corr = df_reduced[keys] In\u00a0[\u00a0]: Copied! <pre># Calculate the correlation matrix\nkeys = list(df_reduced_corr.columns)\nkeys.remove(\"CLASS1\")\nkeys.remove(\"Source_Name\")\nprint (keys)\ncormat = df_reduced_corr[keys].corr()\n\nfig = plt.figure(figsize = (24,24))\n# Set the colormap and color range\ncmap = sns.color_palette(\"coolwarm\", as_cmap=True)  # You can change the colormap as needed\nvmin, vmax = -1, 1  # Set the range for the color scale\n\n# Create the correlation heatmap\nplt.figure(figsize=(10, 8))  # Set the figure size\nsns.heatmap(cormat, cmap=cmap, vmin=vmin, vmax=vmax, annot=False, fmt=\".2f\")  # You can add annotations with fmt=\".2f\"\n\n# Set plot title\nplt.title(\"Correlation Heatmap\")\n\n# Show the plot\nplt.show()\n</pre> # Calculate the correlation matrix keys = list(df_reduced_corr.columns) keys.remove(\"CLASS1\") keys.remove(\"Source_Name\") print (keys) cormat = df_reduced_corr[keys].corr()  fig = plt.figure(figsize = (24,24)) # Set the colormap and color range cmap = sns.color_palette(\"coolwarm\", as_cmap=True)  # You can change the colormap as needed vmin, vmax = -1, 1  # Set the range for the color scale  # Create the correlation heatmap plt.figure(figsize=(10, 8))  # Set the figure size sns.heatmap(cormat, cmap=cmap, vmin=vmin, vmax=vmax, annot=False, fmt=\".2f\")  # You can add annotations with fmt=\".2f\"  # Set plot title plt.title(\"Correlation Heatmap\")  # Show the plot plt.show() <pre>['GLON', 'GLAT', 'Signif_Avg', 'Flux1000', 'SpectrumType', 'PL_Index', 'LP_Index', 'LP_beta', 'LP_SigCurv', 'PLEC_ExpfactorS', 'PLEC_Exp_Index', 'Npred', 'Variability_Index', 'Frac_Variability', 'Flux_Peak']\n</pre> <pre>&lt;Figure size 2400x2400 with 0 Axes&gt;</pre> <p></p> In\u00a0[\u00a0]: Copied! <pre>known_key = df_reduced_corr[\"CLASS1\"] != \"unk\"\ndf_known = df_reduced_corr[known_key]\ndf_unknown = df_reduced_corr[~known_key]\n\n# Create training and testing datasets. We'll use a random 80-20% split between the two:\nn_known = len(df_known)\nsplit = int(0.8*n_known)\nindices = np.random.permutation(np.arange(n_known))\ndf_train = df_known.iloc[indices[:split]]\ndf_test = df_known.iloc[indices[split:]]\n</pre> known_key = df_reduced_corr[\"CLASS1\"] != \"unk\" df_known = df_reduced_corr[known_key] df_unknown = df_reduced_corr[~known_key]  # Create training and testing datasets. We'll use a random 80-20% split between the two: n_known = len(df_known) split = int(0.8*n_known) indices = np.random.permutation(np.arange(n_known)) df_train = df_known.iloc[indices[:split]] df_test = df_known.iloc[indices[split:]]  In\u00a0[\u00a0]: Copied! <pre>df_train.head()\n</pre> df_train.head() Out[\u00a0]: Source_Name GLON GLAT Signif_Avg Flux1000 SpectrumType PL_Index LP_Index LP_beta LP_SigCurv PLEC_ExpfactorS PLEC_Exp_Index Npred Variability_Index Frac_Variability Flux_Peak CLASS1 5593 4FGL J1830.0+1324 42.607689 10.746295 14.126889 5.199472e-10 0 2.130702 2.039060 0.085874 1.561396 0.110271 0.666667 800.970703 34.101921 0.479572 1.426055e-08 bll 6186 4FGL J2000.9-1748 24.008142 -23.112904 46.636375 2.367745e-09 2 2.237818 2.192322 0.057944 3.754773 0.047075 0.666667 3849.101807 601.417847 0.938786 1.308804e-07 fsrq 4131 4FGL J1515.7-2321 340.956207 28.653561 5.581728 1.474999e-10 0 1.902003 1.864740 0.135302 1.138470 0.224465 0.666667 183.075287 9.796313 0.000000 -inf bcu 98 4FGL J0019.6+7327 120.632820 10.726480 18.718227 8.252241e-10 2 2.608850 2.637368 0.107137 3.051930 0.197225 0.666667 2128.889648 494.149200 1.373587 1.119233e-07 fsrq 6983 4FGL J2311.0+3425 100.413605 -24.023064 95.334259 3.847638e-09 2 2.347123 2.237701 0.100465 9.300282 0.098526 0.666667 8989.656250 2648.036865 0.834465 2.186174e-07 fsrq In\u00a0[\u00a0]: Copied! <pre>df_test.head()\n</pre> df_test.head() Out[\u00a0]: Source_Name GLON GLAT Signif_Avg Flux1000 SpectrumType PL_Index LP_Index LP_beta LP_SigCurv PLEC_ExpfactorS PLEC_Exp_Index Npred Variability_Index Frac_Variability Flux_Peak CLASS1 3043 4FGL J1142.0+1548 244.509979 70.308556 20.880785 6.960336e-10 0 2.276100 2.265466 0.019486 0.766554 0.024500 0.666667 1676.332397 19.846550 0.182129 -inf bll 3829 4FGL J1417.9+4613 86.827217 64.369621 3.522235 4.159631e-11 0 2.957262 2.909102 0.081037 0.414308 0.234014 0.666667 466.157196 37.113350 0.800990 1.086692e-08 fsrq 341 4FGL J0116.5-2812 225.304443 -84.340752 6.045664 1.204978e-10 0 2.200092 2.254781 -0.035358 0.241058 0.065479 0.666667 286.141663 25.378225 0.577754 -inf bll 876 4FGL J0327.0-3751 241.156082 -55.765629 4.211840 8.257611e-11 0 2.448183 2.459917 -0.050974 0.502127 -0.014721 0.666667 316.358948 18.068428 0.204948 -inf bcu 449 4FGL J0143.3-0119 150.875214 -61.348251 3.466435 6.683348e-11 0 2.562245 2.527449 -0.098357 1.452685 -0.026672 0.666667 264.440887 19.146957 0.399527 -inf bcu In\u00a0[\u00a0]: Copied! <pre>df_unknown.head()\n</pre> df_unknown.head() Out[\u00a0]: Source_Name GLON GLAT Signif_Avg Flux1000 SpectrumType PL_Index LP_Index LP_beta LP_SigCurv PLEC_ExpfactorS PLEC_Exp_Index Npred Variability_Index Frac_Variability Flux_Peak CLASS1 0 4FGL J0000.3-7355 307.708984 -42.729538 8.492646 1.479606e-10 0 2.247396 2.128013 0.109999 1.117561 0.182318 0.666667 411.909851 12.834996 0.000000 -inf unk 1 4FGL J0000.5+0743 101.656479 -53.029457 5.681097 1.554103e-10 0 2.330765 2.246985 0.109660 1.194232 0.065669 0.666667 384.919708 25.568989 0.561723 -inf unk 2 4FGL J0000.7+2530 108.775070 -35.959175 4.197268 6.779151e-11 0 1.856152 1.801879 0.251122 1.689082 0.688003 0.666667 95.184700 13.149277 0.000000 -inf unk 7 4FGL J0001.6+3503 111.538116 -26.710260 6.475026 1.891931e-10 2 2.449254 2.154877 0.324241 2.418026 0.654485 0.666667 335.825531 11.198676 0.000000 -inf unk 11 4FGL J0002.1+6721c 118.203491 4.939485 6.391003 5.001652e-10 2 2.518880 2.641635 0.244720 3.182654 0.556309 0.666667 815.708618 7.720544 0.000000 -inf unk In\u00a0[\u00a0]: Copied! <pre>print(f\"Number of samples in training data: {len(df_train)}\")\nprint(f\"Number of samples in test data: {len(df_test)}\")\nprint(f\"Number of samples in unknown data: {len(df_unknown)}\")\n</pre> print(f\"Number of samples in training data: {len(df_train)}\") print(f\"Number of samples in test data: {len(df_test)}\") print(f\"Number of samples in unknown data: {len(df_unknown)}\") <pre>Number of samples in training data: 3705\nNumber of samples in test data: 927\nNumber of samples in unknown data: 2563\n</pre> In\u00a0[\u00a0]: Copied! <pre>df_train.describe()\n</pre> df_train.describe() <pre>/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py:4655: RuntimeWarning: invalid value encountered in subtract\n  diff_b_a = subtract(b, a)\n</pre> Out[\u00a0]: GLON GLAT Signif_Avg Flux1000 SpectrumType PL_Index LP_Index LP_beta LP_SigCurv PLEC_ExpfactorS PLEC_Exp_Index Npred Variability_Index Frac_Variability Flux_Peak count 3705.000000 3705.000000 3704.000000 3.705000e+03 3705.000000 3704.000000 3702.000000 3702.000000 3705.000000 3701.000000 3701.000000 3705.000000 3.705000e+03 3705.000000 3.705000e+03 mean 179.546936 1.595435 20.383627 1.905454e-09 0.838057 2.248081 2.176290 0.135326 3.063164 0.241653 0.665594 1872.632202 -inf -inf -inf std 104.957207 39.966671 36.608692 2.548311e-08 0.956510 0.295974 0.349826 0.136967 7.783012 0.283722 0.022208 7539.703613 NaN NaN NaN min 0.150241 -87.680092 0.024616 1.015092e-11 0.000000 1.213077 0.850825 -0.128125 0.000000 -0.254681 0.221034 14.380418 -inf -inf -inf 25% 89.305046 -28.714878 6.430545 1.541491e-10 0.000000 2.023779 1.924477 0.044535 0.800294 0.052365 0.666667 336.599762 1.312558e+01 0.000000 NaN 50% 176.023956 0.171085 10.444759 2.861671e-10 0.000000 2.251612 2.176648 0.099315 1.738781 0.127871 0.666667 639.618286 2.011504e+01 0.291604 NaN 75% 274.209961 32.813686 20.490911 7.084005e-10 2.000000 2.473383 2.431918 0.189513 3.177545 0.327779 0.666667 1341.418701 4.632078e+01 0.577096 1.563045e-08 max 359.992157 87.570763 897.974976 1.330866e-06 2.000000 3.328158 3.288755 1.144088 253.718277 2.241610 1.000000 284295.468750 9.603806e+04 3.661907 5.058845e-06 <p>We have some issues with the <code>Variability_Index</code>, <code>Frac_Variability</code>, and <code>Flux_Peak</code> columns. Let's review them:</p> <ul> <li><code>Variability_Index</code>: This is the sum of the log-likelihood difference between the flux fitted in each time interval and the average flux over the full catalog interval. A value greater than 27.69 over 12 intervals indicates less than a 1% chance of the source being steady.</li> <li><code>Frac_Variability</code>: This represents the fractional variability computed from the fluxes in each year. It measures the extent of variability in the source's emission over time.</li> <li><code>Flux_Peak</code>: This is the peak integral photon flux from 100 MeV to 100 GeV, expressed in photon/cm\u00b2/s. This measures the highest observed flux of the source.</li> </ul> <p>These columns are related to the variability of the source. Given that we're focused on variability, we might not need all of these features for the model. Let's start by excluding the <code>Flux_Peak</code> column, as it may not provide additional information on variability.</p> <p>Next, we can set <code>Variability_Index</code> and <code>Frac_Variability</code> to zero for sources that aren't variable. If a source isn't variable, both of these values should be low. Setting them to zero helps ensure consistency and removes the potential influence of steady sources on our model.</p> In\u00a0[\u00a0]: Copied! <pre>col_names = list(df_train.columns)\nif \"Flux_Peak\" in col_names:\n  col_names.remove(\"Flux_Peak\")\n\nprint (len(col_names))\n\n# Remove from all the dataset\ndf_train = df_train[col_names]\ndf_test = df_test[col_names]\ndf_unknown = df_unknown[col_names]\n\n# Get the minimum variabiltiy index from valid measurements\ninvalid_mask = ~np.isfinite(df_train[\"Variability_Index\"])\nmin_var = np.min(df_train[\"Variability_Index\"][~invalid_mask])\ndf_train.loc[invalid_mask, \"Variability_Index\"] = min_var\n\n# Apply same transformation to test and unknown data\ninvalid_mask = ~np.isfinite(df_test[\"Variability_Index\"])\ndf_test.loc[invalid_mask, \"Variability_Index\"] = min_var\ninvalid_mask = ~np.isfinite(df_unknown[\"Variability_Index\"])\ndf_unknown.loc[invalid_mask, \"Variability_Index\"] = min_var\n\n\n\ninvalid_mask = ~np.isfinite(df_train[\"Frac_Variability\"])\nmin_frac = np.min(df_train[\"Frac_Variability\"][~invalid_mask])\ndf_train.loc[invalid_mask, \"Frac_Variability\"] = min_frac\n\ninvalid_mask = ~np.isfinite(df_test[\"Frac_Variability\"])\ndf_test.loc[invalid_mask, \"Frac_Variability\"] = min_frac\ninvalid_mask = ~np.isfinite(df_unknown[\"Frac_Variability\"])\ndf_unknown.loc[invalid_mask, \"Frac_Variability\"] = min_frac\n\n\ndf_train.describe()\n</pre> col_names = list(df_train.columns) if \"Flux_Peak\" in col_names:   col_names.remove(\"Flux_Peak\")  print (len(col_names))  # Remove from all the dataset df_train = df_train[col_names] df_test = df_test[col_names] df_unknown = df_unknown[col_names]  # Get the minimum variabiltiy index from valid measurements invalid_mask = ~np.isfinite(df_train[\"Variability_Index\"]) min_var = np.min(df_train[\"Variability_Index\"][~invalid_mask]) df_train.loc[invalid_mask, \"Variability_Index\"] = min_var  # Apply same transformation to test and unknown data invalid_mask = ~np.isfinite(df_test[\"Variability_Index\"]) df_test.loc[invalid_mask, \"Variability_Index\"] = min_var invalid_mask = ~np.isfinite(df_unknown[\"Variability_Index\"]) df_unknown.loc[invalid_mask, \"Variability_Index\"] = min_var    invalid_mask = ~np.isfinite(df_train[\"Frac_Variability\"]) min_frac = np.min(df_train[\"Frac_Variability\"][~invalid_mask]) df_train.loc[invalid_mask, \"Frac_Variability\"] = min_frac  invalid_mask = ~np.isfinite(df_test[\"Frac_Variability\"]) df_test.loc[invalid_mask, \"Frac_Variability\"] = min_frac invalid_mask = ~np.isfinite(df_unknown[\"Frac_Variability\"]) df_unknown.loc[invalid_mask, \"Frac_Variability\"] = min_frac   df_train.describe() <pre>16\n</pre> Out[\u00a0]: GLON GLAT Signif_Avg Flux1000 SpectrumType PL_Index LP_Index LP_beta LP_SigCurv PLEC_ExpfactorS PLEC_Exp_Index Npred Variability_Index Frac_Variability count 3705.000000 3705.000000 3704.000000 3.705000e+03 3705.000000 3704.000000 3702.000000 3702.000000 3705.000000 3701.000000 3701.000000 3705.000000 3705.000000 3705.000000 mean 179.546936 1.595435 20.383627 1.905454e-09 0.838057 2.248081 2.176290 0.135326 3.063164 0.241653 0.665594 1872.632202 231.995209 0.364870 std 104.957207 39.966671 36.608692 2.548311e-08 0.956510 0.295974 0.349826 0.136967 7.783012 0.283722 0.022208 7539.703613 2110.518066 0.389758 min 0.150241 -87.680092 0.024616 1.015092e-11 0.000000 1.213077 0.850825 -0.128125 0.000000 -0.254681 0.221034 14.380418 3.196750 0.000000 25% 89.305046 -28.714878 6.430545 1.541491e-10 0.000000 2.023779 1.924477 0.044535 0.800294 0.052365 0.666667 336.599762 13.125583 0.000000 50% 176.023956 0.171085 10.444759 2.861671e-10 0.000000 2.251612 2.176648 0.099315 1.738781 0.127871 0.666667 639.618286 20.115042 0.291604 75% 274.209961 32.813686 20.490911 7.084005e-10 2.000000 2.473383 2.431918 0.189513 3.177545 0.327779 0.666667 1341.418701 46.320782 0.577096 max 359.992157 87.570763 897.974976 1.330866e-06 2.000000 3.328158 3.288755 1.144088 253.718277 2.241610 1.000000 284295.468750 96038.062500 3.661907 <p>Let's take a look at the dataset</p> In\u00a0[\u00a0]: Copied! <pre>col_names = list(df_train.columns)\n# These will be catagorical\ncol_names.remove(\"SpectrumType\")\ncol_names.remove(\"Source_Name\")\ncol_names.remove(\"CLASS1\")\n\nprint(len(col_names))\n</pre> col_names = list(df_train.columns) # These will be catagorical col_names.remove(\"SpectrumType\") col_names.remove(\"Source_Name\") col_names.remove(\"CLASS1\")  print(len(col_names)) <pre>13\n</pre> In\u00a0[\u00a0]: Copied! <pre>fig = plt.figure(figsize = (12,8))\ncols, rows = 4, 4\n\nfor i, lab in enumerate(col_names):\n    ax = plt.subplot(cols, rows, i +1)\n    ax.hist(df_train[lab], density = True, alpha = 0.5)\n    ax.hist(df_test[lab], density = True, alpha = 0.5)\n    ax.set_title(lab)\n\nfig.tight_layout()\n</pre> fig = plt.figure(figsize = (12,8)) cols, rows = 4, 4  for i, lab in enumerate(col_names):     ax = plt.subplot(cols, rows, i +1)     ax.hist(df_train[lab], density = True, alpha = 0.5)     ax.hist(df_test[lab], density = True, alpha = 0.5)     ax.set_title(lab)  fig.tight_layout() In\u00a0[\u00a0]: Copied! <pre>log_feat = [\"Npred\", \"Variability_Index\", \"LP_SigCurv\", \"Flux1000\", \"Signif_Avg\"]\n\nfor feat in log_feat:\n    # Add a small value as log10(0) = infinity\n    df_train[feat] = np.log10(df_train[feat] + 1e-9)\n    df_test[feat] = np.log10(df_test[feat] + 1e-9)\n    df_unknown[feat] = np.log10(df_unknown[feat] + 1e-9)\n</pre> log_feat = [\"Npred\", \"Variability_Index\", \"LP_SigCurv\", \"Flux1000\", \"Signif_Avg\"]  for feat in log_feat:     # Add a small value as log10(0) = infinity     df_train[feat] = np.log10(df_train[feat] + 1e-9)     df_test[feat] = np.log10(df_test[feat] + 1e-9)     df_unknown[feat] = np.log10(df_unknown[feat] + 1e-9)   <p>Min-Max normalization is given by the following formula:</p> <p>$$ f(x, min, max) = \\frac{x - min}{max - min} $$</p> <p>In this formula:</p> <ul> <li>( x ) represents the original value of the feature.</li> <li>( min ) is the minimum value of the feature in the dataset.</li> <li>( max ) is the maximum value of the feature in the dataset.</li> </ul> <p>This formula rescales the values of ( x ) so that they lie between 0 and 1. The minimum value of the feature becomes 0, and the maximum value becomes 1, with all other values linearly transformed to fit within this range. This ensures that all features are on a comparable scale, which helps the machine learning model perform more effectively.</p> In\u00a0[\u00a0]: Copied! <pre>def norm_min_max(x, x_min, x_max):\n    return (x - x_min) / (x_max - x_min)\n</pre> def norm_min_max(x, x_min, x_max):     return (x - x_min) / (x_max - x_min) In\u00a0[\u00a0]: Copied! <pre>for feat in col_names:\n    x_min = df_train[feat].min()\n    x_max = df_train[feat].max()\n    df_train[feat] = df_train[feat].map(lambda x : norm_min_max(x, x_min, x_max))\n\n    # Apply to test and unknown data\n    df_test[feat] = df_test[feat].map(lambda x : norm_min_max(x, x_min, x_max))\n    df_unknown[feat] = df_unknown[feat].map(lambda x : norm_min_max(x, x_min, x_max))\n</pre> for feat in col_names:     x_min = df_train[feat].min()     x_max = df_train[feat].max()     df_train[feat] = df_train[feat].map(lambda x : norm_min_max(x, x_min, x_max))      # Apply to test and unknown data     df_test[feat] = df_test[feat].map(lambda x : norm_min_max(x, x_min, x_max))     df_unknown[feat] = df_unknown[feat].map(lambda x : norm_min_max(x, x_min, x_max)) In\u00a0[\u00a0]: Copied! <pre>fig = plt.figure(figsize = (12,8))\ncols, rows = 4, 4\n\nfor i, lab in enumerate(col_names):\n    ax = plt.subplot(cols, rows, i +1)\n    ax.hist(df_train[lab], density = True, alpha = 0.5)\n    ax.hist(df_test[lab], density = True, alpha = 0.5)\n    ax.set_title(lab)\n\nfig.tight_layout()\n</pre> fig = plt.figure(figsize = (12,8)) cols, rows = 4, 4  for i, lab in enumerate(col_names):     ax = plt.subplot(cols, rows, i +1)     ax.hist(df_train[lab], density = True, alpha = 0.5)     ax.hist(df_test[lab], density = True, alpha = 0.5)     ax.set_title(lab)  fig.tight_layout() <p></p> <p>To prepare the target variable for classification, we need to convert the source type strings into integers. This is often referred to as label encoding, where each unique source type (string) is mapped to a unique integer label (0, 1, 2, ...).</p> <p>For example, suppose the source types are \"Galaxy\", \"Pulsar\", and \"Blazar\". We would assign them integer values as follows:</p> <ul> <li>\"Galaxy\" \u2192 0</li> <li>\"Pulsar\" \u2192 1</li> <li>\"Blazar\" \u2192 2</li> </ul> <p>We can accomplish this transformation using Python's <code>LabelEncoder</code> from the <code>sklearn.preprocessing</code> module. In this example we'll do it by hand!</p> In\u00a0[\u00a0]: Copied! <pre>targets = list(set(df_psc[\"CLASS1\"]))\nprint (targets)\n\n# We'll use a dictionary as an encoder\nencoder = {}\nfor i, tar in enumerate(targets):\n    encoder[tar] = i\n\nprint (encoder)\n\ndf_train[\"Labels\"] = df_train[\"CLASS1\"].apply(lambda x : encoder[x])\ndf_test[\"Labels\"] = df_test[\"CLASS1\"].apply(lambda x : encoder[x])\n\ndf_train.head()\n</pre> targets = list(set(df_psc[\"CLASS1\"])) print (targets)  # We'll use a dictionary as an encoder encoder = {} for i, tar in enumerate(targets):     encoder[tar] = i  print (encoder)  df_train[\"Labels\"] = df_train[\"CLASS1\"].apply(lambda x : encoder[x]) df_test[\"Labels\"] = df_test[\"CLASS1\"].apply(lambda x : encoder[x])  df_train.head() <pre>['rdg', 'unk', 'sfr', 'nov', 'psr', 'ssrq', 'gal', 'gc', 'snr', 'msp', 'css', 'sey', 'nlsy1', 'sbg', 'hmb', 'fsrq', 'pwn', 'bcu', 'bll', 'glc', 'bin', 'spp', 'agn', 'lmb']\n{'rdg': 0, 'unk': 1, 'sfr': 2, 'nov': 3, 'psr': 4, 'ssrq': 5, 'gal': 6, 'gc': 7, 'snr': 8, 'msp': 9, 'css': 10, 'sey': 11, 'nlsy1': 12, 'sbg': 13, 'hmb': 14, 'fsrq': 15, 'pwn': 16, 'bcu': 17, 'bll': 18, 'glc': 19, 'bin': 20, 'spp': 21, 'agn': 22, 'lmb': 23}\n</pre> Out[\u00a0]: Source_Name GLON GLAT Signif_Avg Flux1000 SpectrumType PL_Index LP_Index LP_beta LP_SigCurv PLEC_ExpfactorS PLEC_Exp_Index Npred Variability_Index Frac_Variability CLASS1 Labels 5593 4FGL J1830.0+1324 0.117989 0.561631 0.604735 0.056871 0 0.433849 0.487395 0.168210 0.806141 0.146198 0.572082 0.406389 0.229596 0.130962 bll 18 6186 4FGL J2000.9-1748 0.066301 0.368427 0.718429 0.167609 2 0.484492 0.550261 0.146256 0.839555 0.120882 0.572082 0.565081 0.507951 0.256365 fsrq 15 4131 4FGL J1515.7-2321 0.947099 0.663812 0.516337 0.017745 0 0.325721 0.415892 0.207062 0.794111 0.191943 0.572082 0.257183 0.108616 0.000000 bcu 17 98 4FGL J0019.6+7327 0.334821 0.561518 0.631526 0.082347 2 0.659915 0.732812 0.184924 0.831663 0.181031 0.572082 0.505210 0.488897 0.375101 fsrq 15 6983 4FGL J2311.0+3425 0.278632 0.363234 0.786496 0.218310 2 0.536171 0.568875 0.179679 0.874096 0.141493 0.572082 0.650832 0.651717 0.227877 fsrq 15 In\u00a0[\u00a0]: Copied! <pre>df_train.shape\n</pre> df_train.shape Out[\u00a0]: <pre>(3705, 17)</pre> In\u00a0[\u00a0]: Copied! <pre># Let's throw away some identifying features\nkeys = list(df_train.columns)\nkeys.remove(\"Source_Name\")\nkeys.remove(\"CLASS1\")\nprint(keys)\n</pre> # Let's throw away some identifying features keys = list(df_train.columns) keys.remove(\"Source_Name\") keys.remove(\"CLASS1\") print(keys)   <pre>['GLON', 'GLAT', 'Signif_Avg', 'Flux1000', 'SpectrumType', 'PL_Index', 'LP_Index', 'LP_beta', 'LP_SigCurv', 'PLEC_ExpfactorS', 'PLEC_Exp_Index', 'Npred', 'Variability_Index', 'Frac_Variability', 'Labels']\n</pre> In\u00a0[\u00a0]: Copied! <pre># Removing \"-1\", the last index, which is \"Labels\"\nX_train = df_train[keys].values[:, :-1]\ny_train = df_train['Labels'].values\n\nfeature_names = keys[:-1]\n\nX_test = df_test[keys].values[:, :-1]\ny_test = df_test['Labels'].values\n\nprint (f\"Training data shape {X_train.shape}\")\nprint (f\"Testing data shape {X_test.shape}\")\n</pre> # Removing \"-1\", the last index, which is \"Labels\" X_train = df_train[keys].values[:, :-1] y_train = df_train['Labels'].values  feature_names = keys[:-1]  X_test = df_test[keys].values[:, :-1] y_test = df_test['Labels'].values  print (f\"Training data shape {X_train.shape}\") print (f\"Testing data shape {X_test.shape}\")  <pre>Training data shape (3705, 14)\nTesting data shape (927, 14)\n</pre> <p></p> In\u00a0[\u00a0]: Copied! <pre>from sklearn.tree import DecisionTreeClassifier\n</pre> from sklearn.tree import DecisionTreeClassifier In\u00a0[\u00a0]: Copied! <pre>model_0 = DecisionTreeClassifier(criterion=\"gini\")\n</pre> model_0 = DecisionTreeClassifier(criterion=\"gini\") In\u00a0[\u00a0]: Copied! <pre>model_0.fit(X_train, y_train)\n</pre> model_0.fit(X_train, y_train) Out[\u00a0]: <pre>DecisionTreeClassifier()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFitted<pre>DecisionTreeClassifier()</pre> In\u00a0[\u00a0]: Copied! <pre>from sklearn.metrics import  accuracy_score\n</pre> from sklearn.metrics import  accuracy_score In\u00a0[\u00a0]: Copied! <pre>y_pred = model_0.predict(X_train)\ntraining_accuracy = accuracy_score(y_train, y_pred)\nprint (f\"Training accuracy: {training_accuracy}\")\n</pre> y_pred = model_0.predict(X_train) training_accuracy = accuracy_score(y_train, y_pred) print (f\"Training accuracy: {training_accuracy}\")  <pre>Training accuracy: 0.9964912280701754\n</pre> In\u00a0[\u00a0]: Copied! <pre>y_pred = model_0.predict(X_test)\ntesting_accuracy = accuracy_score(y_test, y_pred)\nprint (f\"Test accuracy: {testing_accuracy}\")\n</pre> y_pred = model_0.predict(X_test) testing_accuracy = accuracy_score(y_test, y_pred) print (f\"Test accuracy: {testing_accuracy}\")  <pre>Test accuracy: 0.5490830636461704\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>best_model = None\nbest_acc = -1\nbest_hyper = 0\nmax_depths = np.arange(1,20)\ntrain_acc = []\ntest_acc = []\n\nfor max_depth in max_depths:\n    test_model = DecisionTreeClassifier(criterion=\"gini\", max_depth=max_depth).fit(X_train, y_train)\n    train_acc.append(100*accuracy_score(y_train, test_model.predict(X_train)))\n    test_acc.append(100*accuracy_score(y_test, test_model.predict(X_test)))\n\n    if test_acc[-1] &gt; best_acc:\n        best_acc = test_acc[-1]\n        best_model = test_model\n        best_hyper = max_depth\n</pre> best_model = None best_acc = -1 best_hyper = 0 max_depths = np.arange(1,20) train_acc = [] test_acc = []  for max_depth in max_depths:     test_model = DecisionTreeClassifier(criterion=\"gini\", max_depth=max_depth).fit(X_train, y_train)     train_acc.append(100*accuracy_score(y_train, test_model.predict(X_train)))     test_acc.append(100*accuracy_score(y_test, test_model.predict(X_test)))      if test_acc[-1] &gt; best_acc:         best_acc = test_acc[-1]         best_model = test_model         best_hyper = max_depth  In\u00a0[\u00a0]: Copied! <pre>plt.plot(max_depths, train_acc, label = \"Training\")\nplt.plot(max_depths, test_acc, label = \"Testing\")\nplt.axhline(best_acc, ls = \"--\", color = \"k\")\nplt.axvline(best_hyper, ls = \"--\", color = \"k\", label = f\"Best Model -&gt; Accuracy {best_acc:0.1f} % (Max Depth = {best_hyper})\")\n\nplt.ylabel(\"Accuracy [%]\")\nplt.xlabel(\"Max Depth of Tree\")\nplt.legend()\nplt.grid()\n</pre> plt.plot(max_depths, train_acc, label = \"Training\") plt.plot(max_depths, test_acc, label = \"Testing\") plt.axhline(best_acc, ls = \"--\", color = \"k\") plt.axvline(best_hyper, ls = \"--\", color = \"k\", label = f\"Best Model -&gt; Accuracy {best_acc:0.1f} % (Max Depth = {best_hyper})\")  plt.ylabel(\"Accuracy [%]\") plt.xlabel(\"Max Depth of Tree\") plt.legend() plt.grid()  In\u00a0[\u00a0]: Copied! <pre>from sklearn.ensemble import RandomForestClassifier\n</pre> from sklearn.ensemble import RandomForestClassifier In\u00a0[\u00a0]: Copied! <pre>model_1 = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=10)\n</pre> model_1 = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=10) In\u00a0[\u00a0]: Copied! <pre>model_1.fit(X_train, y_train)\n</pre> model_1.fit(X_train, y_train) Out[\u00a0]: <pre>RandomForestClassifier(max_depth=10)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0RandomForestClassifier?Documentation for RandomForestClassifieriFitted<pre>RandomForestClassifier(max_depth=10)</pre> In\u00a0[\u00a0]: Copied! <pre>y_pred = model_1.predict(X_train)\ntraining_accuracy = accuracy_score(y_train, y_pred)\n\ny_pred = model_1.predict(X_test)\ntesting_accuracy = accuracy_score(y_test, y_pred)\n\nprint (f\"Training accuracy: {training_accuracy}\")\nprint (f\"Test accuracy: {testing_accuracy}\")\n</pre>   y_pred = model_1.predict(X_train) training_accuracy = accuracy_score(y_train, y_pred)  y_pred = model_1.predict(X_test) testing_accuracy = accuracy_score(y_test, y_pred)  print (f\"Training accuracy: {training_accuracy}\") print (f\"Test accuracy: {testing_accuracy}\")  <pre>Training accuracy: 0.8574898785425101\nTest accuracy: 0.6785329018338727\n</pre> In\u00a0[\u00a0]: Copied! <pre>best_model = None\nbest_acc = -1\nbest_hyper = 0\nmax_depths = np.arange(1,20)\ntrain_acc = []\ntest_acc = []\n\n# Loop over the max_depth\nfor max_depth in max_depths:\n    test_model = RandomForestClassifier(n_estimators=100, criterion=\"gini\", max_depth=max_depth).fit(X_train, y_train)\n    train_acc.append(100*accuracy_score(y_train, test_model.predict(X_train)))\n    test_acc.append(100*accuracy_score(y_test, test_model.predict(X_test)))\n\n    if test_acc[-1] &gt; best_acc:\n        best_acc = test_acc[-1]\n        best_model = test_model\n        best_hyper = max_depth\n</pre> best_model = None best_acc = -1 best_hyper = 0 max_depths = np.arange(1,20) train_acc = [] test_acc = []  # Loop over the max_depth for max_depth in max_depths:     test_model = RandomForestClassifier(n_estimators=100, criterion=\"gini\", max_depth=max_depth).fit(X_train, y_train)     train_acc.append(100*accuracy_score(y_train, test_model.predict(X_train)))     test_acc.append(100*accuracy_score(y_test, test_model.predict(X_test)))      if test_acc[-1] &gt; best_acc:         best_acc = test_acc[-1]         best_model = test_model         best_hyper = max_depth  In\u00a0[\u00a0]: Copied! <pre>plt.plot(max_depths, train_acc, label = \"Training\")\nplt.plot(max_depths, test_acc, label = \"Testing\")\nplt.axhline(best_acc, ls = \"--\", color = \"k\")\nplt.axvline(best_hyper, ls = \"--\", color = \"k\", label = f\"Best Model -&gt; Accuracy {best_acc:0.1f} % (Max Depth = {best_hyper})\")\n\nplt.ylabel(\"Accuracy [%]\")\nplt.xlabel(\"Max Depth of Tree\")\nplt.legend()\nplt.grid()\n</pre> plt.plot(max_depths, train_acc, label = \"Training\") plt.plot(max_depths, test_acc, label = \"Testing\") plt.axhline(best_acc, ls = \"--\", color = \"k\") plt.axvline(best_hyper, ls = \"--\", color = \"k\", label = f\"Best Model -&gt; Accuracy {best_acc:0.1f} % (Max Depth = {best_hyper})\")  plt.ylabel(\"Accuracy [%]\") plt.xlabel(\"Max Depth of Tree\") plt.legend() plt.grid()  In\u00a0[\u00a0]: Copied! <pre>best_model = None\nbest_acc = -1\nbest_hyper = 0\nn_estimators = np.logspace(0, 3, 10, dtype = int)\ntrain_acc = []\ntest_acc = []\n\n# Loop over the number of estimators (trees)\nfor n_estimator in n_estimators:\n\n    test_model = RandomForestClassifier(n_estimators=n_estimator, criterion=\"gini\", max_depth=16).fit(X_train, y_train)\n    train_acc.append(100*accuracy_score(y_train, test_model.predict(X_train)))\n    test_acc.append(100*accuracy_score(y_test, test_model.predict(X_test)))\n\n    if test_acc[-1] &gt; best_acc:\n        best_acc = test_acc[-1]\n        best_model = test_model\n        best_hyper = n_estimator\n</pre> best_model = None best_acc = -1 best_hyper = 0 n_estimators = np.logspace(0, 3, 10, dtype = int) train_acc = [] test_acc = []  # Loop over the number of estimators (trees) for n_estimator in n_estimators:      test_model = RandomForestClassifier(n_estimators=n_estimator, criterion=\"gini\", max_depth=16).fit(X_train, y_train)     train_acc.append(100*accuracy_score(y_train, test_model.predict(X_train)))     test_acc.append(100*accuracy_score(y_test, test_model.predict(X_test)))      if test_acc[-1] &gt; best_acc:         best_acc = test_acc[-1]         best_model = test_model         best_hyper = n_estimator  In\u00a0[\u00a0]: Copied! <pre>plt.plot(n_estimators, train_acc, label = \"Training\")\nplt.plot(n_estimators, test_acc, label = \"Testing\")\nplt.axhline(best_acc, ls = \"--\", color = \"k\")\nplt.axvline(best_hyper, ls = \"--\", color = \"k\", label = f\"Best Model -&gt; Accuracy {best_acc:0.1f} % (n_estimators = {best_hyper})\")\n\nplt.ylabel(\"Accuracy [%]\")\nplt.xlabel(\"Number of Estimators\")\nplt.xscale('log')\nplt.legend()\nplt.grid()\n</pre> plt.plot(n_estimators, train_acc, label = \"Training\") plt.plot(n_estimators, test_acc, label = \"Testing\") plt.axhline(best_acc, ls = \"--\", color = \"k\") plt.axvline(best_hyper, ls = \"--\", color = \"k\", label = f\"Best Model -&gt; Accuracy {best_acc:0.1f} % (n_estimators = {best_hyper})\")  plt.ylabel(\"Accuracy [%]\") plt.xlabel(\"Number of Estimators\") plt.xscale('log') plt.legend() plt.grid()  In\u00a0[\u00a0]: Copied! <pre>feature_importance = best_model.feature_importances_\n\n# We have n_estimator number of trees\n# Each tree will vary slightly on how they use features\nstd = np.std([tree.feature_importances_ for tree in best_model.estimators_],\n             axis=0)\nfeature_names = np.array(feature_names)\nasort = np.argsort(feature_importance)\n\nplt.title(\"Feature Importance\")\nplt.barh(feature_names[asort], feature_importance[asort], xerr = std[asort])\nplt.xlabel(\"Relative Importance\")\nplt.grid()\n</pre> feature_importance = best_model.feature_importances_  # We have n_estimator number of trees # Each tree will vary slightly on how they use features std = np.std([tree.feature_importances_ for tree in best_model.estimators_],              axis=0) feature_names = np.array(feature_names) asort = np.argsort(feature_importance)  plt.title(\"Feature Importance\") plt.barh(feature_names[asort], feature_importance[asort], xerr = std[asort]) plt.xlabel(\"Relative Importance\") plt.grid()  <p></p> In\u00a0[\u00a0]: Copied! <pre>X_unknown = df_unknown[keys[:-1]].values\ny_pred = best_model.predict(X_unknown)\n</pre> X_unknown = df_unknown[keys[:-1]].values y_pred = best_model.predict(X_unknown) <p>Let's write a \"decoder\" to retrieve the encoded source class from the label:</p> In\u00a0[\u00a0]: Copied! <pre>decoder = {}\nfor k in encoder:\n    decoder[encoder[k]] = k\nprint(decoder)\n\n# We want to apply this to every item in the array, so we'll use np.vectorize\n@np.vectorize\ndef decode(y):\n    return decoder[y]\n\ndf_unknown[\"Targets\"] = decode(y_pred)\n</pre> decoder = {} for k in encoder:     decoder[encoder[k]] = k print(decoder)  # We want to apply this to every item in the array, so we'll use np.vectorize @np.vectorize def decode(y):     return decoder[y]  df_unknown[\"Targets\"] = decode(y_pred) <pre>{0: 'rdg', 1: 'unk', 2: 'sfr', 3: 'nov', 4: 'psr', 5: 'ssrq', 6: 'gal', 7: 'gc', 8: 'snr', 9: 'msp', 10: 'css', 11: 'sey', 12: 'nlsy1', 13: 'sbg', 14: 'hmb', 15: 'fsrq', 16: 'pwn', 17: 'bcu', 18: 'bll', 19: 'glc', 20: 'bin', 21: 'spp', 22: 'agn', 23: 'lmb'}\n</pre> In\u00a0[\u00a0]: Copied! <pre>Physics and TSI\nclasses = [k for k in encoder]\n\nbinning = np.arange(len(classes) + 1) - 0.5\nbinw = binning[1:] - binning[:-1]\nbinc = binning[:-1] + 0.5 * binw\ntrain_counts, _ = np.histogram(y_train, bins=binning, density = True)\ntest_counts, _ = np.histogram(y_test, bins=binning, density = True)\nunknown_counts, _ = np.histogram(y_pred, bins=binning, density = True)\n</pre> Physics and TSI classes = [k for k in encoder]  binning = np.arange(len(classes) + 1) - 0.5 binw = binning[1:] - binning[:-1] binc = binning[:-1] + 0.5 * binw train_counts, _ = np.histogram(y_train, bins=binning, density = True) test_counts, _ = np.histogram(y_test, bins=binning, density = True) unknown_counts, _ = np.histogram(y_pred, bins=binning, density = True)    In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nsns.set_palette('colorblind')\n\n# Sample data: replace these with your actual counts\n\n\n# Bar width and positions\nbar_width = 0.25\npositions = np.arange(len(classes))\n\n# Plotting\nplt.figure(figsize=(12, 6))\nplt.bar(positions - bar_width, train_counts, width=bar_width, label='Training', color='C0')\nplt.bar(positions, test_counts, width=bar_width, label='Test', color='C1')\nplt.bar(positions + bar_width, unknown_counts, width=bar_width, label='Unknown', color='C2')\n\n# Labels and legend\nplt.xlabel('Classes')\nplt.ylabel('Sample Count')\nplt.title('Sample Distribution Across Classes')\nplt.xticks(positions, classes)  # Show class labels on the x-axis\nplt.legend()\n\n# Show the plot\nplt.tight_layout()\n</pre> import matplotlib.pyplot as plt import numpy as np import seaborn as sns sns.set_palette('colorblind')  # Sample data: replace these with your actual counts   # Bar width and positions bar_width = 0.25 positions = np.arange(len(classes))  # Plotting plt.figure(figsize=(12, 6)) plt.bar(positions - bar_width, train_counts, width=bar_width, label='Training', color='C0') plt.bar(positions, test_counts, width=bar_width, label='Test', color='C1') plt.bar(positions + bar_width, unknown_counts, width=bar_width, label='Unknown', color='C2')  # Labels and legend plt.xlabel('Classes') plt.ylabel('Sample Count') plt.title('Sample Distribution Across Classes') plt.xticks(positions, classes)  # Show class labels on the x-axis plt.legend()  # Show the plot plt.tight_layout()  <p></p> <p></p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Python/MachineLearningFermi_solutions/#machine-learning-in-astrophysics","title":"Machine Learning in Astrophysics\u00b6","text":"<p>In this workshop, we'll be learning how to apply a Decision Tree Classification algorithm to classify unknown gamma-ray emitters.</p>"},{"location":"Python/MachineLearningFermi_solutions/#detecting-gamma-rays","title":"Detecting Gamma-rays\u00b6","text":"<p>Because the atmosphere is opaque to gamma-rays, gamma-rays are absorbed by our atmosphere. One method to observe gamma-rays is to put a gamma-ray detector in space!</p> <p>We'll be using observations taken by the Fermi-LAT satellite. </p> <p>Fermi-LAT is a pair-conversion telescope that converts gamma-ray photons into electron-positron pairs. Remember, $E = mc^2$ means we can convert between electromagnetic energy (gamma-rays) and mass (electrons and positrons). These electron-positron pairs are then tracked, absorbed, and measured within the satellite's detector mass. This allows Fermi-LAT to detect gamma-ray photons and determine where in the sky the photon came from.</p>"},{"location":"Python/MachineLearningFermi_solutions/#ufos-rightarrow-unidentified-fermi-objects","title":"UFOs $\\rightarrow$ Unidentified Fermi Objects!\u00b6","text":"<p>One of Fermi-LAT's greatest achievements is the catalog of gamma-ray emitters that it has produced. This has revolutionized our understanding of the high-energy universe! Despite the interest in Fermi-LAT, we still don't know what a significant number of these gamma-ray emitters are. They could be anything from known objects like distant galaxies, high-energy pulsars, binaries. More exotic objects like protoplanetary nebula, giant molecular clouds, stellar bowshocks. Or even potential dark matter subhalos! sources...</p> <p>In this workshop, we'll try to understand what these unknown sources might be!</p>"},{"location":"Python/MachineLearningFermi_solutions/#workshop-steps","title":"Workshop Steps:\u00b6","text":"<ol> <li>Load the data</li> <li>Prepare and clean the data</li> <li>Feature engineering</li> <li>Split the dataset into training and testing data</li> <li>Train and improve our model</li> <li>Make predictions</li> <li>Discuss the limitations of our model</li> <li>Conclusions</li> </ol>"},{"location":"Python/MachineLearningFermi_solutions/#0-load-the-data","title":"0. Load the Data\u00b6","text":"<p>We'll be loading the Fermi-LAT 4FGL point source catalog. This is stored as a FITS file, so we'll use <code>astropy.io.fits</code> to open it. The catalog itself is stored as a <code>BinTableHDU</code>. We'll use <code>astropy.table.Table</code> to read the table and convert it to a <code>pandas</code> DataFrame.</p>"},{"location":"Python/MachineLearningFermi_solutions/#01-remove-data-with-multiple-entries","title":"0.1 Remove Data with Multiple Entries\u00b6","text":"<p>Some of the columns (e.g., <code>Flux_Band</code>) contain multiple entries. We'll remove these columns to simplify our dataset.</p>"},{"location":"Python/MachineLearningFermi_solutions/#02-relabel-the-data","title":"0.2 Relabel the Data\u00b6","text":"<p>The 4FGL catalog uses two naming conventions. When an association is confirmed, the class is shown in capital letters. When the association isn't strictly confirmed, lowercase letters are used.</p> <p>The identification vs. association depends on factors such as correlated variability, morphology, etc. For our purposes, we'll consider all associations to be confirmed as identifications.</p>"},{"location":"Python/MachineLearningFermi_solutions/#1-preparing-the-data","title":"1. Preparing the Data\u00b6","text":"<p>To prepare the data, we want to extract the useful information from the data frame. We also need to be careful not to leak any information that might bias our model.</p>"},{"location":"Python/MachineLearningFermi_solutions/#11-removing-association-and-getting-the-targetlabel","title":"1.1 Removing Association and Getting the Target/Label\u00b6","text":"<p>Since we want to classify unknown objects and try to determine what they are, let's remove the source's association and any other identifying information. This won't be available for unknown sources!</p> <p>Here, our \"target\" or \"label\" will be the source's association.</p>"},{"location":"Python/MachineLearningFermi_solutions/#12-categorical-vs-numerical-data","title":"1.2 Categorical vs. Numerical Data\u00b6","text":"<p>In machine learning, it's important to differentiate between categorical and numerical data, as they require different handling and processing methods.</p> <ul> <li><p>Categorical Data: This type of data represents categories or labels and can take on a limited number of distinct values. For example, in our dataset, a \"spectrum type\" could be a categorical feature, where the values might include labels like \"Gamma\", \"X-ray\", or \"Optical\". Categorical data is often non-ordinal (no inherent order) but can sometimes be ordinal (where the categories have a specific order, such as \"Low\", \"Medium\", \"High\").</p> <ul> <li>Handling Categorical Data: Categorical data needs to be converted into a format that machine learning algorithms can understand. Common techniques for this include:<ul> <li>Label Encoding: Assigning a unique integer to each category.</li> <li>One-Hot Encoding: Creating a binary column for each category, marking the presence of that category with a 1 and the absence with a 0.</li> </ul> </li> </ul> </li> <li><p>Numerical Data: This data type consists of numbers and can be continuous (e.g., height, weight) or discrete (e.g., count of items). Numerical data is often the most straightforward type of data to process for machine learning models.</p> <ul> <li>Handling Numerical Data: Numerical data may need to be scaled or normalized, especially if the values vary widely. This ensures that all features contribute equally to the model and helps algorithms converge more quickly. Common methods include:<ul> <li>Standardization: Scaling the data so it has a mean of 0 and a standard deviation of 1.</li> <li>Normalization: Rescaling the data to fit within a specific range, typically 0 to 1.</li> </ul> </li> </ul> </li> </ul> <p>Understanding how to properly handle and transform categorical and numerical data ensures that the model can make the best use of the available information and improve prediction accuracy.</p> <p>Normally we'd use SciKit learn to transform our data, but here we'll do it by hand to see what goes into the methods.</p>"},{"location":"Python/MachineLearningFermi_solutions/#13-remove-highly-correlated-columns","title":"1.3 Remove Highly Correlated Columns\u00b6","text":"<p>Highly correlated columns may indicate redundant information or features that are closely related to another variable. Including these columns in the model can lead to issues such as multicollinearity, where the model struggles to determine the individual effect of each feature on the target variable. This can reduce the model's ability to generalize and increase variance, leading to overfitting.</p> <p>To identify and remove highly correlated columns, we can compute the correlation matrix for the dataset. Features with a correlation coefficient above a certain threshold (e.g., 0.9) are considered highly correlated and can be dropped or combined. This ensures that the model uses only the most relevant, independent features, which can improve performance and reduce complexity.</p> <p>In summary, removing highly correlated columns helps in creating a more efficient model by reducing redundancy and improving generalization.</p>"},{"location":"Python/MachineLearningFermi_solutions/#2-feature-engineering","title":"2. Feature Engineering\u00b6","text":"<p>A lot of machine learning models work best with data that is either normally distributed or scaled between 0 and 1. To make the data easier to work with, we may need to apply some transformations.</p> <p>It's important to remember that we must apply the same transformation to all datasets to avoid data leakage. For instance, if we subtract a number from the training dataset, we need to subtract the same number from the test set and any other dataset (including the dataset we want to make predictions on).</p> <p>For example, if we perform the following transformation on the training data:</p> <pre>training_feature = training_feature - np.min(training_feature)\n</pre> <p>Then we should apply the same transformation to the test set and any future datasets as follows:</p> <pre>value = np.min(training_feature)\ntraining_feature = training_feature - value\ntest_feature = test_feature - value\nreal_feature = real_feature - value\n</pre> <p>This ensures that all datasets are on the same scale, allowing the model to make accurate predictions without bias introduced by differing data transformations.</p>"},{"location":"Python/MachineLearningFermi_solutions/#20-split-the-dataset","title":"2.0 Split the Dataset\u00b6","text":"<p>We'll split the dataset into three parts:</p> <ul> <li>Training Data: This is the data we'll use to train our machine learning model. The model learns from this data to make predictions.</li> <li>Test Data: This data is used to evaluate the performance of the trained model. It allows us to see how well the model generalizes to new, unseen data.</li> <li>Unknown Data: This will consist of the unknown sources that we want to make predictions on. Our trained model will be applied to this data to predict the class or label of these unknown sources.</li> </ul>"},{"location":"Python/MachineLearningFermi_solutions/#21-handling-missing-null-nan-and-infinite-values","title":"2.1 Handling Missing, Null, NaN, and Infinite Values\u00b6","text":"<p>In machine learning, it's common to encounter incomplete data. This might include missing values, null entries, or values that are deliberately set to unphysical values (such as infinity or very large numbers). It's important to handle these issues before training the model, as they can negatively impact model performance.</p> <p>There are several strategies for dealing with missing or invalid values, including:</p> <ul> <li>Removing Rows or Columns: If the missing data is minimal, one option is to simply remove the affected rows or columns from the dataset.</li> <li>Imputation: For missing values, we can replace them with a reasonable estimate. This could be the mean, median, or mode of the column, or even a value predicted from other features in the dataset.</li> <li>Replacing Infinite Values: We can replace infinite values (both positive and negative) with a large finite number, or remove the rows/columns containing them.</li> </ul> <p>By carefully addressing missing, null, NaN, and infinite values, we ensure the dataset is clean and ready for model training.</p>"},{"location":"Python/MachineLearningFermi_solutions/#22-feature-engineering","title":"2.2 Feature Engineering\u00b6","text":"<p>Feature engineering involves modifying and transforming the features in a dataset to make them more suitable for machine learning models. This process can include applying mathematical transformations, creating new features through combinations, or modifying existing values to improve their predictive power.</p> <p>In this workshop, we'll apply two common transformations:</p> <ul> <li><p>Logarithmic Transformation: Taking the $\\log_{10}$ of larger values. Often, data can be log-distributed, meaning that values span multiple orders of magnitude. In these cases, working with the logarithm of the values can make the data more manageable and improve the model's ability to learn. By converting the data into log-space, we can reduce the impact of large outliers and focus more on the underlying patterns.</p> </li> <li><p>Min-Max Normalization: This involves rescaling the values in the dataset so that the minimum value becomes 0 and the maximum value becomes 1. All other values are scaled linearly between 0 and 1. This normalization technique ensures that features with larger numerical ranges don't dominate the model and that all features are on a comparable scale, making them more effective predictors.</p> </li> </ul> <p>Both of these transformations will help make the data more consistent and easier for the model to process, potentially improving its accuracy and performance.</p>"},{"location":"Python/MachineLearningFermi_solutions/#3-splitting-the-dataset","title":"3. Splitting the Dataset\u00b6","text":"<p>Our goal is to classify all unknown targets. To focus on training and evaluating our model, we will temporarily remove these unknown targets from the dataset. This allows us to work with a dataset containing only known sources, which will help us build and test our model's performance before applying it to the unknown targets.</p> <p>Once the model is trained and validated, we can use it to predict the class of the unknown sources.</p>"},{"location":"Python/MachineLearningFermi_solutions/#31-split-data-into-training-and-testing-samples","title":"3.1 Split Data into Training and Testing Samples\u00b6","text":"<p>When training our machine learning model, it's important to split the data into two datasets:</p> <ul> <li>Training Data: This is the dataset that we'll use to train the model. The model will learn patterns and features from this data in order to make predictions.</li> <li>Testing Data: This dataset is used to evaluate the performance of the model. After training, we use this data to test how well the model generalizes to unseen data.</li> </ul> <p>In machine learning, it's a common convention to use:</p> <ul> <li><code>X</code> to represent the features (input data),</li> <li><code>y</code> to represent the labels (target data).</li> </ul> <p>By adopting this convention, our code will be more standardized and easier to follow. For example, we can split the dataset using <code>sklearn</code>:</p> <pre>from sklearn.model_selection import train_test_split\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n</pre> <p>In this example, we'll do this by hand.</p>"},{"location":"Python/MachineLearningFermi_solutions/#decision-trees","title":"Decision Trees\u00b6","text":"<p>A decision tree is a type of algorithm used in machine learning that mimics a tree-like structure to make decisions. It is one of the simplest and most interpretable models in supervised learning, used for both classification and regression tasks.</p> <p></p>"},{"location":"Python/MachineLearningFermi_solutions/#key-components-of-a-decision-tree","title":"Key Components of a Decision Tree\u00b6","text":"<ol> <li><p>Root Node:</p> <ul> <li>The top of the tree.</li> <li>Represents the entire dataset and the first decision point.</li> </ul> </li> <li><p>Decision Nodes:</p> <ul> <li>Branches that split the data based on conditions (e.g., \"Is brightness &gt; 10?\").</li> <li>Each node represents a question about the data.</li> </ul> </li> <li><p>Leaves:</p> <ul> <li>The endpoints of the tree.</li> <li>Represent the final decision or predicted outcome (e.g., \"Galaxy Type: Spiral\").</li> </ul> </li> <li><p>Splits:</p> <ul> <li>Criteria used to divide data into branches (e.g., thresholds for numerical features or categories for categorical features).</li> </ul> </li> </ol>"},{"location":"Python/MachineLearningFermi_solutions/#how-does-a-decision-tree-work","title":"How Does a Decision Tree Work?\u00b6","text":"<ol> <li><p>The algorithm evaluates all possible splits of the data at each node based on a specific metric, such as:</p> <ul> <li>Gini Impurity: Measures how often a randomly chosen element would be incorrectly labeled.</li> <li>Entropy: Measures the disorder or impurity in the data.</li> <li>Mean Squared Error (MSE): Used for regression tasks.</li> </ul> </li> <li><p>It selects the split that best separates the data into groups with the most distinct outcomes.</p> </li> <li><p>This process repeats recursively until:</p> <ul> <li>All data points in a branch belong to the same class.</li> <li>A maximum depth is reached.</li> <li>Further splitting does not improve results significantly.</li> </ul> </li> </ol> <p>See:</p> <ul> <li>What Is a Decision Tree?</li> <li>A visual introduction to machine learning</li> </ul>"},{"location":"Python/MachineLearningFermi_solutions/#scikit-learn","title":"Scikit-Learn\u00b6","text":"<p>In this workshop we'll be using Scikit-Learn (<code>sklearn</code>) to implement our decision trees.</p>"},{"location":"Python/MachineLearningFermi_solutions/#accuracy","title":"Accuracy\u00b6","text":"<p>To evaluate the model we'll use Accuracy, which is given by:</p> <p>$$ \\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}$$</p>"},{"location":"Python/MachineLearningFermi_solutions/#41-what-is-overfitting","title":"4.1 What is Overfitting?\u00b6","text":"<p>Overfitting occurs in machine learning when a model learns the training data too well, including its noise and random fluctuations, rather than just the underlying patterns. As a result, the model performs well on the training data but poorly on unseen or test data.</p>"},{"location":"Python/MachineLearningFermi_solutions/#how-overfitting-happens","title":"How Overfitting Happens\u00b6","text":"<ol> <li><p>Excessive Complexity:</p> <ul> <li>Models with too many parameters or layers (e.g., deep neural networks) can capture irrelevant details in the training data.</li> </ul> </li> <li><p>Small or Noisy Datasets:</p> <ul> <li>If the training dataset is too small or contains a lot of noise, the model may memorize these specifics instead of generalizing.</li> </ul> </li> <li><p>Insufficient Regularization:</p> <ul> <li>Without mechanisms like dropout, pruning, or weight penalties, the model may become overly specialized to the training set.</li> </ul> </li> </ol>"},{"location":"Python/MachineLearningFermi_solutions/#symptoms-of-overfitting","title":"Symptoms of Overfitting\u00b6","text":"<ul> <li>Low Training Error/High Training Performance: The model achieves very high accuracy on the training data.</li> <li>High Test Error: The performance drops significantly when evaluated on unseen data.</li> </ul>"},{"location":"Python/MachineLearningFermi_solutions/#41-hyperparameters-and-hyperparameter-tuning-in-decision-trees","title":"4.1 Hyperparameters and Hyperparameter Tuning in Decision Trees\u00b6","text":""},{"location":"Python/MachineLearningFermi_solutions/#what-are-hyperparameters","title":"What are Hyperparameters?\u00b6","text":"<p>Hyperparameters are settings or configurations that control the behavior of a machine learning algorithm. Unlike model parameters, which are learned from the data during training, hyperparameters are set before training and influence how the model learns.</p> <p>In the context of decision trees, common hyperparameters include:</p> <ul> <li>Max Depth: The maximum number of splits (or levels) the tree is allowed to make.</li> <li>Other hyperparameters (not covered here): Minimum samples per split, minimum leaf size, etc.</li> </ul>"},{"location":"Python/MachineLearningFermi_solutions/#the-role-of-max-depth-in-decision-trees","title":"The Role of Max Depth in Decision Trees\u00b6","text":"<p>The <code>max_depth</code> hyperparameter determines how \"deep\" a decision tree can grow. It controls the maximum number of levels or decision points in the tree. This directly affects the model's complexity and ability to generalize.</p>"},{"location":"Python/MachineLearningFermi_solutions/#effects-of-max-depth","title":"Effects of Max Depth:\u00b6","text":"<ul> <li>Small max_depth:<ul> <li>The tree is shallow and may not capture all patterns in the data.</li> <li>Risk of underfitting: The model is too simple and performs poorly on both training and test data.</li> </ul> </li> <li>Large max_depth:<ul> <li>The tree is deep and can capture even minor details in the training data.</li> <li>Risk of overfitting: The model learns the noise and specifics of the training data, performing poorly on unseen data.</li> </ul> </li> </ul>"},{"location":"Python/MachineLearningFermi_solutions/#43-random-forests","title":"4.3 Random Forests\u00b6","text":"<p>A random forest is an advanced machine learning algorithm that builds upon decision trees. Instead of relying on a single decision tree, it creates a collection (or \"forest\") of decision trees and combines their outputs to make more accurate and robust predictions. It is commonly used for both classification and regression tasks.</p> <p>See here for a visual explanation of random forests.</p>"},{"location":"Python/MachineLearningFermi_solutions/#how-does-a-random-forest-work","title":"How Does a Random Forest Work?\u00b6","text":"<ol> <li><p>Build Multiple Decision Trees:</p> <ul> <li>Each tree is trained on a random subset of the training data (using a method called bootstrap sampling)).</li> <li>At each split, the algorithm considers only a random subset of features to decide the best split, adding more randomness.</li> </ul> </li> <li><p>Combine Outputs:</p> <ul> <li>For classification tasks:<ul> <li>Each tree votes for a class.</li> <li>The random forest predicts the majority-voted class.</li> </ul> </li> <li>For regression tasks:<ul> <li>The random forest averages the predictions from all trees.</li> </ul> </li> </ul> </li> </ol>"},{"location":"Python/MachineLearningFermi_solutions/#why-use-a-random-forest-instead-of-a-single-decision-tree","title":"Why Use a Random Forest Instead of a Single Decision Tree?\u00b6","text":"<p>While decision trees are easy to understand and interpret, they can overfit the training data, especially if the tree is deep. Random forests address this by introducing randomness and aggregating results, which leads to:</p> <ul> <li>Better Generalization: Reduces overfitting and improves performance on unseen data.</li> <li>Stability: Less sensitive to small changes in the training data.</li> </ul>"},{"location":"Python/MachineLearningFermi_solutions/#key-concepts-in-random-forests","title":"Key Concepts in Random Forests\u00b6","text":"<ul> <li>Bootstrap Sampling:<ul> <li>Each tree is trained on a random subset of the data (with replacement), ensuring diversity among trees.</li> </ul> </li> <li>Feature Subsampling:<ul> <li>At each split, only a random subset of features is considered, preventing trees from becoming too similar.</li> </ul> </li> <li>Ensemble Learning:<ul> <li>The final prediction combines results from multiple trees, leveraging the \"wisdom of the crowd.\"</li> </ul> </li> </ul>"},{"location":"Python/MachineLearningFermi_solutions/#why-random-forests-are-useful-after-learning-decision-trees","title":"Why Random Forests are Useful After Learning Decision Trees\u00b6","text":"<p>Random forests build directly on the principles of decision trees but address their limitations. They are an excellent next step for improving accuracy and robustness while maintaining the interpretability and flexibility of tree-based methods.</p>"},{"location":"Python/MachineLearningFermi_solutions/#whats-happening-here","title":"What's Happening Here?\u00b6","text":"<p>We are no longer seeing a decrease in accuracy on the testing dataset, which suggests that our model is generalizing well and not overfitting.</p> <p>In a Random Forest model, each decision tree is trained on a random subset of the data. This means that each individual tree is only exposed to a smaller portion of the data, so it might overfit to that subset. However, because we have multiple trees in the forest, and each one is trained on a different subset, no two trees will overfit in the same way.</p> <p>When making predictions, we take the average of each tree's vote (or use the majority vote in classification tasks). By averaging the predictions of many trees, we reduce the impact of any single tree's overfitting. This process of aggregation leads to a more generalizable model.</p>"},{"location":"Python/MachineLearningFermi_solutions/#takeaway","title":"Takeaway:\u00b6","text":"<p>Random Forests, and other Ensemble Learning algorithms, leverage multiple estimators (in this case, decision trees) to make predictions. By training each estimator on a random subsample of the dataset, we mitigate the effects of overfitting. This helps the model remain generalizable and improves its performance on unseen data.</p>"},{"location":"Python/MachineLearningFermi_solutions/#43-feature-importance","title":"4.3 Feature importance\u00b6","text":"<p>Feature importance is a concept in machine learning that helps us understand which features (or inputs) are most influential in making predictions. It assigns a score to each feature, indicating how much it contributes to the model\u2019s decisions.</p> <p>In the context of decision trees and random forests, feature importance provides insights into how the model splits the data at various points in the tree.</p>"},{"location":"Python/MachineLearningFermi_solutions/#how-is-feature-importance-calculated","title":"How is Feature Importance Calculated?\u00b6","text":""},{"location":"Python/MachineLearningFermi_solutions/#for-decision-trees","title":"For Decision Trees:\u00b6","text":"<ol> <li><p>Node Splitting:</p> <ul> <li>At each split in the tree, the algorithm chooses a feature and a threshold that best separates the data.</li> <li>The \"quality\" of the split is evaluated using metrics like Gini Impurity or Entropy (for classification) or Mean Squared Error (for regression).</li> </ul> </li> <li><p>Importance Score:</p> <ul> <li>The reduction in the chosen metric (e.g., Gini Impurity) caused by the split is recorded for the selected feature.</li> <li>These reductions are accumulated across all splits where the feature is used.</li> </ul> </li> </ol>"},{"location":"Python/MachineLearningFermi_solutions/#for-random-forests","title":"For Random Forests:\u00b6","text":"<ol> <li>The feature importance from each decision tree in the forest is calculated.</li> <li>The scores are averaged across all trees, providing a robust overall measure of importance.</li> </ol>"},{"location":"Python/MachineLearningFermi_solutions/#why-is-feature-importance-useful","title":"Why is Feature Importance Useful?\u00b6","text":"<ol> <li><p>Insights into the Model:</p> <ul> <li>It reveals which features the model relies on most to make predictions.</li> <li>Helps in interpreting the model's behavior.</li> </ul> </li> <li><p>Feature Selection:</p> <ul> <li>Irrelevant or low-importance features can be removed to simplify the model.</li> <li>Reduces computational cost and risk of overfitting.</li> </ul> </li> <li><p>Understanding Data Relationships:</p> <ul> <li>Highlights the key factors driving the target variable, which can be valuable for domain-specific insights.</li> </ul> </li> </ol>"},{"location":"Python/MachineLearningFermi_solutions/#5-understanding-our-model","title":"5. Understanding our model\u00b6","text":"<p>Looking at the feature importance, we can start to understand what our model is picking up as important features of the sources that we've trained on.</p>"},{"location":"Python/MachineLearningFermi_solutions/#51-the-energy-spectrum","title":"5.1 The Energy Spectrum\u00b6","text":"<p>\"PL_Index\" refers to the \"Power Law\" spectral index ($\\Gamma$) of the fitted power-law model. Non-thermal emission is typically well modeled using a power-law distribution, which describes how the number of particles or photons varies with energy:</p> <p>$$ \\frac{dN}{dE} = N E^{-\\Gamma} $$</p> <p>Where:</p> <ul> <li>$ \\frac{dN}{dE} $ is the number of particles or photons per unit energy,</li> <li>$ E $ is the energy of the particles or photons,</li> <li>$ \\Gamma $ is the power-law index.</li> </ul> <p>More advanced emission mechanisms may manifest as curvature in an otherwise power-law distribution. We can model this curvature using a Log Parabola (LP) model, which includes a curvature term $ \\beta $:</p> <p>$$ \\frac{dN}{dE} = N E^{-\\Gamma - \\beta\\log(E)} $$</p> <p>Another common feature seen in the energy spectrum is an exponential cutoff (EC). This occurs when there is attenuation in the emission, possibly due to some physical process limiting the emission at higher energies. The model for this is:</p> <p>$$ \\frac{dN}{dE} = N E^{-\\Gamma} \\times \\exp(-a E^{\\Gamma_2}) $$</p> <p>Where:</p> <ul> <li>$ a $ is the exponential factor (<code>PLEC_ExpfactorS</code>),</li> <li>$ \\Gamma_2 $ is the exponential index (<code>PLEX_Exp_Index</code>).</li> </ul> <p>To simplify the fitting procedure, $ \\Gamma_2 $ is typically frozen to a fixed value, which is why we don't gain much additional information from it.</p>"},{"location":"Python/MachineLearningFermi_solutions/#key-takeaways","title":"Key Takeaways:\u00b6","text":"<ul> <li>PL_Index / LP_Index: As the index $ \\Gamma $ approaches 1, the spectrum becomes dominated by higher energy photons (more energetic emission). As $ \\Gamma $ increases, the spectrum shifts toward lower energy emissions.</li> <li>Curvature (LP_beta) and Curvature Significance (LP_SigCurv): These are often weak indicators. This might be due to Fermi-LAT's sensitivity limits, or because most sources do not exhibit significant spectral curvature within the energy range of Fermi-LAT.</li> <li>Exponential Cutoff: The energy cutoff is similarly weak in Fermi-LAT's range, possibly due to the attenuation mechanisms not being prominent within the observed energy range.</li> </ul> <p>This understanding helps guide our interpretation of the spectral indices and model parameters, allowing for better insight into the nature of the sources we are studying.</p>"},{"location":"Python/MachineLearningFermi_solutions/#52-galactic-coordinates","title":"5.2 Galactic Coordinates\u00b6","text":"<p>The Galactic Latitude and Longitude have very different importances, suggesting some spatial dependence on the objects, but only in one direction: the Latitude.</p> <p>As we observe in Longitude, we don't see any strong dependence. This makes sense because we expect the universe to be roughly homogeneous. Similarly, we expect our galaxy to look roughly the same wherever we observe, with one major exception: the galactic plane.</p> <p></p> <p>The central plane of our galaxy is the densest region, so we expect to see a higher density of gamma-ray emitters along the plane.</p> <p>However, when we move out of this plane, we're no longer observing sources within our galaxy. Instead, we're more likely to be observing extragalactic sources, such as blazars or starburst galaxies. In fact, in gamma-rays, we typically ignore galactic emission at galactic longitudes greater than 10 degrees.</p> <p></p> <p>More information on galactic coordinates</p>"},{"location":"Python/MachineLearningFermi_solutions/#53-so-what-are-the-unknown-sources","title":"5.3 So what are the unknown sources?\u00b6","text":"<p>They're mostly <code>bcu</code>, <code>bll</code>, and <code>fsrq</code>, but what are these?</p> <ul> <li>These are all blazars. Blazars are active galactic nuclei with relativistic jets oriented closely to the line of sight. <code>bll</code> and <code>fsrq</code> are BL Lac objects and Flat Spectrum Radio Quasars, both sub-classes of blazars. <code>bcu</code> is a blazar of unknown classification (i.e. we're not sure if they're a BL Lac or an FSRQ).</li> <li>This is consistent with previous results, see here where they also obtain a similar accuracy using a more advanced model: <p>With the state-of-the-art CATBOOST algorithm, based on gradient boosting decision trees, we are able to reach a 67 percent accuracy on a 23-class data set.</p> </li> <li>See also here where they see a similar distribution of AGN and Pulsars: <p>From our primary sample of 174 Fermi sources with a single X-ray/UV/optical counterpart, we present 132 $P_{bzr}$ &gt; 0.99 likely blazars and 14 $P_{bzr}$ &lt; 0.01 likely pulsars, with 28 remaining ambiguous.</p> </li> </ul> <p>What about the others?</p> <ul> <li><code>msp</code> are Millisecond Pulsars. These are pulsars with a period in the millisecond range. It is believed that there are old pulsars, and that there are many undiscovered MSPs near the center of the galaxy. In fact, this is an alternative hypothesis for potential dark matter signals from the center of the galaxy.</li> <li><code>spp</code> are \"sources of unknown nature but overlapping with known SNRs or PWNe and thus candidates to these classes\". This means that they are unknown sources that happen to overlap with supernova remnants and pulsar wind nebulae.</li> </ul>"},{"location":"Python/MachineLearningFermi_solutions/#54-what-does-our-model-tell-us","title":"5.4 What does our model tell us?\u00b6","text":"<p>We can pretty much group the unknown targets into two categories:</p> <ul> <li>Extragalactic - Sources outside of our galaxy, likely just yet to be identified blazars.</li> <li>Galactic - Likely Millisecond Pulsars, Supernova Remnants, or Pulsar Wind Nebulae.</li> </ul> <p>But this opens some more questions:</p> <ul> <li><p>Why haven't we identified the blazars? Fermi-LAT's spatial resolution is around &lt;0.1 degree. This covers a large area of the sky, making identification challenging!</p> </li> <li><p>Why haven't we seen the Millisecond Pulsars? MSPs are difficult to detect and require a dedicated survey instrument for discovery.</p> </li> <li><p>What about Supernova Remnants and Pulsar Wind Nebulae? This is questionable. We know that the <code>spp</code> classification arises due to known overlapping sources. So, why aren't these being classified as <code>snr</code> or <code>pwn</code>? Our model only has information from Fermi, and we don't know about possible overlapping sources.</p> <ul> <li>My hypothesis (Can you test this in your own time?): These sources are located in denser regions (e.g., the galactic plane and near the galactic center). In these coordinates, source confusion might be more likely. Clearly identified <code>snr</code> or <code>pwn</code> classifications might correspond to sources in less dense regions.</li> </ul> </li> </ul>"},{"location":"Python/MachineLearningFermi_solutions/#55-was-this-scientifically-useful","title":"5.5 Was this scientifically useful?\u00b6","text":"<p>Now that we have predictions, we could request time on a dedicated instrument for follow-up observations. I see two potential interesting studies:</p> <ul> <li><p>Blazars: We have identified a number of blazar candidates. Are any of these blazars located in poorly studied areas of the sky, especially those aligned with the galactic plane? These might be worth following up on. Distant blazars are excellent laboratories to study fundamental physics, such as Lorentz Invariance Violation, Axion Like Particles, and the Extragalactic Background Light.</p> </li> <li><p>Millisecond Pulsar candidates: We have some potential millisecond pulsar candidates. These could be followed up with dedicated radio or Very High Energy (VHE) instruments to see if we can detect pulsed emission from nearby sources.</p> </li> <li><p>Supernova Remnants or Pulsar Wind Nebulae: We have identified some potential supernova remnants or pulsar wind nebulae. With a dedicated follow-up instrument, we could try to pinpoint the source of the gamma-ray emission.</p> </li> <li><p>Dark Matter: Simulations of galaxy formation tell us that we need to have Dark Matter Subhalos. Particle physics also tells us that, depending on the type of Dark Matter, we should expect gamma-ray emission from decay and annihilation of Dark Matter. So if we can catagorize all the unknown sources of gamma-ray emission as normal \"stuff\", then we can strongly constrain what Dark Matter is!</p> </li> </ul>"},{"location":"Python/MachineLearningFermi_solutions/#6-limitations-of-our-model","title":"6. Limitations of Our Model\u00b6","text":"<p>Using a random forest algorithm, we achieved around 70% accuracy on our testing dataset. Let's consider some of the limitations:</p> <ol> <li><p>Size of the Training Dataset: We only had around 3,500 samples in our training dataset, which is relatively small. Real-world problems often use datasets containing hundreds of thousands to millions of samples, which could provide better generalization.</p> </li> <li><p>Hyperparameters: We limited ourselves to tuning only two hyperparameters. In real-world applications, it's common to search a multi-dimensional grid of hyperparameters to find the optimal combination.</p> </li> <li><p>Features: We used a subset of the available features, discarding many correlated ones. There might still be useful information in these features. Perhaps we could have performed Principal Component Analysis (PCA) to help reduce the dimensionality and retain more information.</p> </li> <li><p>Labels: Many labels in the dataset represent similar classes. For example, <code>bcu</code>, <code>bll</code>, and <code>fsrq</code> are all types of blazars. Combining similar labels could have improved the model\u2019s performance by reducing complexity.</p> </li> <li><p>Model: We limited ourselves to decision trees and random forests, which are relatively simple models. More advanced models, such as XGBoost, Neural Networks, or Boosted Decision Trees, tend to outperform simple decision trees in many cases.</p> </li> </ol>"},{"location":"Python/MachineLearningFermi_solutions/#7-conclusion","title":"7. Conclusion\u00b6","text":"<p>In this workshop, we learned:</p> <ul> <li>How to use <code>astropy</code> and <code>pandas</code> to work with astronomical catalogs, enabling us to handle and manipulate large sets of astronomical data efficiently.</li> <li>The importance of data cleaning and transformation in preparing datasets for machine learning models.</li> <li>That cleaning and preprocessing our dataset is often the most crucial step in a successful machine learning workflow, as it directly impacts model performance.</li> <li>The fundamentals of decision trees and how to implement them using <code>scikit-learn</code> to make predictions.</li> <li>The concept of ensemble learning, how to create a random forest using <code>scikit-learn</code>, and how it helps mitigate overfitting by leveraging multiple estimators.</li> <li>That real-world datasets often have limitations, and it's important to recognize these limitations to improve model accuracy and reliability.</li> </ul>"},{"location":"Python/N_BodySimulation/","title":"N-Body Simulation","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nfrom astropy import constants, units\nimport matplotlib.pyplot as plt\n</pre> import numpy as np from astropy import constants, units import matplotlib.pyplot as plt In\u00a0[\u00a0]: Copied! <pre># Assign the variable x to be 1 m (\"meter\")\nx = 1 * units.m\nprint(x)\n# Convert x to cm\nprint(x.to(units.cm))\n\n# Use imperial units of feet\ny = 1 * units.imperial.foot\nprint(y)\nprint(y.to(units.m))\n\n# We can add the two together and let astropy handle the conversion\nz = x + y\nprint(z)\n</pre> # Assign the variable x to be 1 m (\"meter\") x = 1 * units.m print(x) # Convert x to cm print(x.to(units.cm))  # Use imperial units of feet y = 1 * units.imperial.foot print(y) print(y.to(units.m))  # We can add the two together and let astropy handle the conversion z = x + y print(z) <pre>1.0 m\n100.0 cm\n1.0 ft\n0.3048 m\n1.3048 m\n</pre> <p>We can also import <code>constants</code> from <code>astropy</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Get the speed of light\nc = constants.c\nprint(c)\n# Convert to km/s\nprint(c.to(units.km / units.s))\nprint(c.to(units.imperial.inch / units.s))\n</pre> # Get the speed of light c = constants.c print(c) # Convert to km/s print(c.to(units.km / units.s)) print(c.to(units.imperial.inch / units.s))   <pre>  Name   = Speed of light in vacuum\n  Value  = 299792458.0\n  Uncertainty  = 0.0\n  Unit  = m / s\n  Reference = CODATA 2018\n299792.458 km / s\n11802852677.165352 inch / s\n</pre> <p>For our simulation we'll use:</p> <ul> <li>$G$ - the gravitational constant</li> <li>$m_{sun}$ - the mass of the Sun</li> <li>$m_{earth}$ - the mass of the earth</li> <li>$AU$ - AU or \"astronomocial unit\" is the average distance between the Sun and the Earth.</li> </ul> <p>Since we live on Earth, we'll define the planets in our simulation in terms of Earth mass and Earth Distance (AU).</p> In\u00a0[\u00a0]: Copied! <pre>G = constants.G\nprint(G)\nprint(\"\\n----\\n\")\nm_sun = constants.M_sun\nprint(m_sun)\nprint(\"\\n----\\n\")\nm_earth = constants.M_earth\nprint(m_earth)\nprint(\"\\n----\\n\")\nAU = constants.au\nprint(AU)\n</pre> G = constants.G print(G) print(\"\\n----\\n\") m_sun = constants.M_sun print(m_sun) print(\"\\n----\\n\") m_earth = constants.M_earth print(m_earth) print(\"\\n----\\n\") AU = constants.au print(AU)   <pre>  Name   = Gravitational constant\n  Value  = 6.6743e-11\n  Uncertainty  = 1.5e-15\n  Unit  = m3 / (kg s2)\n  Reference = CODATA 2018\n\n----\n\n  Name   = Solar mass\n  Value  = 1.988409870698051e+30\n  Uncertainty  = 4.468805426856864e+25\n  Unit  = kg\n  Reference = IAU 2015 Resolution B 3 + CODATA 2018\n\n----\n\n  Name   = Earth mass\n  Value  = 5.972167867791379e+24\n  Uncertainty  = 1.3422009501651213e+20\n  Unit  = kg\n  Reference = IAU 2015 Resolution B 3 + CODATA 2018\n\n----\n\n  Name   = Astronomical Unit\n  Value  = 149597870700.0\n  Uncertainty  = 0.0\n  Unit  = m\n  Reference = IAU 2012 Resolution B2\n</pre> In\u00a0[\u00a0]: Copied! <pre>x = np.array([1, 2]) * units.m\ny = np.array([3, 4]) * units.m\nprint(x)\nprint(y)\nprint(x + y)\n</pre> x = np.array([1, 2]) * units.m y = np.array([3, 4]) * units.m print(x) print(y) print(x + y) <pre>[1. 2.] m\n[3. 4.] m\n[4. 6.] m\n</pre> <p>Astropy also handles combining units:</p> In\u00a0[\u00a0]: Copied! <pre>distances = np.array([0,1,2,3,4,5,6,7,8,9]) * units.m\ntimes = 0.5*np.ones(len(distances))*units.s\nprint(distances)\nprint(times)\nvelocities = distances / times\nprint(velocities)\nprint(velocities.to(units.imperial.mile/units.hour))\n</pre> distances = np.array([0,1,2,3,4,5,6,7,8,9]) * units.m times = 0.5*np.ones(len(distances))*units.s print(distances) print(times) velocities = distances / times print(velocities) print(velocities.to(units.imperial.mile/units.hour))  <pre>[0. 1. 2. 3. 4. 5. 6. 7. 8. 9.] m\n[0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5] s\n[ 0.  2.  4.  6.  8. 10. 12. 14. 16. 18.] m / s\n[ 0.          4.47387258  8.94774517 13.42161775 17.89549034 22.36936292\n 26.8432355  31.31710809 35.79098067 40.26485326] mi / h\n</pre> In\u00a0[\u00a0]: Copied! <pre># Define an empty dictionary\nmy_dict = dict()\nprint(my_dict)\nmy_dict = {}\nprint(my_dict)\n</pre> # Define an empty dictionary my_dict = dict() print(my_dict) my_dict = {} print(my_dict) <pre>{}\n{}\n</pre> In\u00a0[\u00a0]: Copied! <pre># Define a dictionary with a list of floats and a key\nmy_dict = {\n    \"first_object\" : [1.0, 2.0, 3.0],\n    \"second_object\" : [4.0, 5.0, 6.0]\n}\nprint(my_dict)\n\n# Access an element of the dictionary using the key\nprint(my_dict[\"first_object\"])\n\n# Modify an element of one of the lists stored in the dictionary\nmy_dict[\"first_object\"][0] = 10.0\nprint(my_dict[\"first_object\"])\n\n\n# add a new element to the dictionary\nmy_dict[\"message\"] = \"hello\"\nprint(my_dict)\n\n# Get all the keys and loop over them\nmy_keys = my_dict.keys()\nfor key in my_keys:\n    print(key)\n    print(\"\\t\", my_dict[key])\n</pre> # Define a dictionary with a list of floats and a key my_dict = {     \"first_object\" : [1.0, 2.0, 3.0],     \"second_object\" : [4.0, 5.0, 6.0] } print(my_dict)  # Access an element of the dictionary using the key print(my_dict[\"first_object\"])  # Modify an element of one of the lists stored in the dictionary my_dict[\"first_object\"][0] = 10.0 print(my_dict[\"first_object\"])   # add a new element to the dictionary my_dict[\"message\"] = \"hello\" print(my_dict)  # Get all the keys and loop over them my_keys = my_dict.keys() for key in my_keys:     print(key)     print(\"\\t\", my_dict[key])  <pre>{'first_object': [1.0, 2.0, 3.0], 'second_object': [4.0, 5.0, 6.0]}\n[1.0, 2.0, 3.0]\n[10.0, 2.0, 3.0]\n{'first_object': [10.0, 2.0, 3.0], 'second_object': [4.0, 5.0, 6.0], 'message': 'hello'}\nfirst_object\n\t [10.0, 2.0, 3.0]\nsecond_object\n\t [4.0, 5.0, 6.0]\nmessage\n\t hello\n</pre> In\u00a0[\u00a0]: Copied! <pre># Store as a \"dictionary of dictionaries\"\ndef create_bodies():\n  bodies = {\n      \"sun\" : {\n          \"pos\" : np.array([0,0]) * AU,\n          \"vel\" : np.array([0,0]) * units.km / units.s,\n          \"mass\" : m_sun\n      },\n      \"earth\" : {\n          \"pos\" : np.array([1,0]) * AU,\n          \"vel\" : np.array([0,29.8]) * units.km / units.s,\n          \"mass\" : m_earth\n      },\n      \"mercury\" : {\n          \"pos\" : np.array([0.39,0]) * AU,\n          \"vel\" : np.array([0,47.4]) * units.km / units.s,\n          \"mass\" : 0.06*m_earth\n      },\n      \"venus\" : {\n          \"pos\" : np.array([0.72,0]) * AU,\n          \"vel\" : np.array([0,35.0]) * units.km / units.s,\n          \"mass\" : 0.81*m_earth\n      },\n      \"mars\" : {\n          \"pos\" : np.array([1.52,0]) * AU,\n          \"vel\" : np.array([0,24.1]) * units.km / units.s,\n          \"mass\" : 0.11*m_earth\n      }\n  }\n  return bodies\n\nbodies = create_bodies()\nprint(bodies)\n</pre> # Store as a \"dictionary of dictionaries\" def create_bodies():   bodies = {       \"sun\" : {           \"pos\" : np.array([0,0]) * AU,           \"vel\" : np.array([0,0]) * units.km / units.s,           \"mass\" : m_sun       },       \"earth\" : {           \"pos\" : np.array([1,0]) * AU,           \"vel\" : np.array([0,29.8]) * units.km / units.s,           \"mass\" : m_earth       },       \"mercury\" : {           \"pos\" : np.array([0.39,0]) * AU,           \"vel\" : np.array([0,47.4]) * units.km / units.s,           \"mass\" : 0.06*m_earth       },       \"venus\" : {           \"pos\" : np.array([0.72,0]) * AU,           \"vel\" : np.array([0,35.0]) * units.km / units.s,           \"mass\" : 0.81*m_earth       },       \"mars\" : {           \"pos\" : np.array([1.52,0]) * AU,           \"vel\" : np.array([0,24.1]) * units.km / units.s,           \"mass\" : 0.11*m_earth       }   }   return bodies  bodies = create_bodies() print(bodies) <pre>{'sun': {'pos': &lt;Quantity [0., 0.] m&gt;, 'vel': &lt;Quantity [0., 0.] km / s&gt;, 'mass': &lt;&lt;class 'astropy.constants.iau2015.IAU2015'&gt; name='Solar mass' value=1.988409870698051e+30 uncertainty=4.468805426856864e+25 unit='kg' reference='IAU 2015 Resolution B 3 + CODATA 2018'&gt;}, 'earth': {'pos': &lt;Quantity [1.49597871e+11, 0.00000000e+00] m&gt;, 'vel': &lt;Quantity [ 0. , 29.8] km / s&gt;, 'mass': &lt;&lt;class 'astropy.constants.iau2015.IAU2015'&gt; name='Earth mass' value=5.972167867791379e+24 uncertainty=1.3422009501651213e+20 unit='kg' reference='IAU 2015 Resolution B 3 + CODATA 2018'&gt;}, 'mercury': {'pos': &lt;Quantity [5.83431696e+10, 0.00000000e+00] m&gt;, 'vel': &lt;Quantity [ 0. , 47.4] km / s&gt;, 'mass': &lt;Quantity 3.58330072e+23 kg&gt;}, 'venus': {'pos': &lt;Quantity [1.07710467e+11, 0.00000000e+00] m&gt;, 'vel': &lt;Quantity [ 0., 35.] km / s&gt;, 'mass': &lt;Quantity 4.83745597e+24 kg&gt;}, 'mars': {'pos': &lt;Quantity [2.27388763e+11, 0.00000000e+00] m&gt;, 'vel': &lt;Quantity [ 0. , 24.1] km / s&gt;, 'mass': &lt;Quantity 6.56938465e+23 kg&gt;}}\n</pre> In\u00a0[\u00a0]: Copied! <pre>fig = plt.figure(figsize = (10,6))\nax = plt.axes()\nfor body in bodies:\n    ax.plot(bodies[body][\"pos\"][0], bodies[body][\"pos\"][1], 'o')\n    # add and arrow with the planet name\n    ax.annotate(body,\n     (bodies[body][\"pos\"][0].value, bodies[body][\"pos\"][1].value + 0.1*AU.value)\n    )\n\nplt.ylim(-2 * AU.value ,2 * AU.value )\nplt.xlim(-2 * AU.value ,2 * AU.value )\nplt.ylabel(\"y [m]\")\nplt.xlabel(\"x [m]\")\nplt.grid()\n</pre> fig = plt.figure(figsize = (10,6)) ax = plt.axes() for body in bodies:     ax.plot(bodies[body][\"pos\"][0], bodies[body][\"pos\"][1], 'o')     # add and arrow with the planet name     ax.annotate(body,      (bodies[body][\"pos\"][0].value, bodies[body][\"pos\"][1].value + 0.1*AU.value)     )  plt.ylim(-2 * AU.value ,2 * AU.value ) plt.xlim(-2 * AU.value ,2 * AU.value ) plt.ylabel(\"y [m]\") plt.xlabel(\"x [m]\") plt.grid() <p>This might be useful for later, let's make it into a function so that we don't need to rewrite the code:</p> In\u00a0[\u00a0]: Copied! <pre>def plot_system(bodies):\n  fig = plt.figure(figsize = (9,9))\n  ax = plt.axes()\n  for body in bodies:\n      ax.plot(bodies[body][\"pos\"][0], bodies[body][\"pos\"][1], 'o')\n      # add and arrow with the planet name\n      ax.annotate(body,\n      (bodies[body][\"pos\"][0].value, bodies[body][\"pos\"][1].value + 0.1*AU.value)\n      )\n\n  plt.ylim(-2 * AU.value ,2 * AU.value )\n  plt.xlim(-2 * AU.value ,2 * AU.value )\n  plt.ylabel(\"y [m]\")\n  plt.xlabel(\"x [m]\")\n  plt.grid()\n  return fig, ax\n\n\nplot_system(bodies)\n</pre> def plot_system(bodies):   fig = plt.figure(figsize = (9,9))   ax = plt.axes()   for body in bodies:       ax.plot(bodies[body][\"pos\"][0], bodies[body][\"pos\"][1], 'o')       # add and arrow with the planet name       ax.annotate(body,       (bodies[body][\"pos\"][0].value, bodies[body][\"pos\"][1].value + 0.1*AU.value)       )    plt.ylim(-2 * AU.value ,2 * AU.value )   plt.xlim(-2 * AU.value ,2 * AU.value )   plt.ylabel(\"y [m]\")   plt.xlabel(\"x [m]\")   plt.grid()   return fig, ax   plot_system(bodies) Out[\u00a0]: <pre>(&lt;Figure size 900x900 with 1 Axes&gt;, &lt;Axes: xlabel='x [m]', ylabel='y [m]'&gt;)</pre> In\u00a0[\u00a0]: Copied! <pre>delta_t = 1 * units.day\nduration = 10 * units.year\nn_steps = int(duration / delta_t)\n# We'll run for this number of steps\nprint(n_steps)\n</pre> delta_t = 1 * units.day duration = 10 * units.year n_steps = int(duration / delta_t) # We'll run for this number of steps print(n_steps) <pre>3652\n</pre> In\u00a0[\u00a0]: Copied! <pre>n_bodies = len(bodies)\n# Let's store our data as numpy arrays\nx_loc = np.zeros((n_bodies, n_steps)) * units.m\ny_loc = np.zeros((n_bodies, n_steps)) * units.m\nx_vel = np.zeros((n_bodies, n_steps)) * units.m / units.s\ny_vel = np.zeros((n_bodies, n_steps)) * units.m / units.s\n</pre> n_bodies = len(bodies) # Let's store our data as numpy arrays x_loc = np.zeros((n_bodies, n_steps)) * units.m y_loc = np.zeros((n_bodies, n_steps)) * units.m x_vel = np.zeros((n_bodies, n_steps)) * units.m / units.s y_vel = np.zeros((n_bodies, n_steps)) * units.m / units.s In\u00a0[\u00a0]: Copied! <pre>def get_forces(bodies):\n  forces = np.zeros((n_bodies, 2))\n  for i, body1 in enumerate(bodies):\n    for j, body2 in enumerate(bodies):\n      if i != j:\n        # Get the vector representation of the distace\n        r = bodies[body2][\"pos\"] - bodies[body1][\"pos\"]\n        # Get the absolute distance\n        # sqrt((x1-x2)^2 + (y1-x2)^2)\n        distance = np.linalg.norm(r)\n        # Newtonian Gravity\n        f = G * bodies[body1][\"mass\"] * bodies[body2][\"mass\"] / distance**2\n        # Get the vector of forces\n        # using r and distance to get the fraction of force in x and y directions\n        # Strip the units and only add the value\n        tmp = f * r / distance\n        forces[i] += tmp.value\n  # Add the units back in\n  return forces * tmp.unit\n</pre> def get_forces(bodies):   forces = np.zeros((n_bodies, 2))   for i, body1 in enumerate(bodies):     for j, body2 in enumerate(bodies):       if i != j:         # Get the vector representation of the distace         r = bodies[body2][\"pos\"] - bodies[body1][\"pos\"]         # Get the absolute distance         # sqrt((x1-x2)^2 + (y1-x2)^2)         distance = np.linalg.norm(r)         # Newtonian Gravity         f = G * bodies[body1][\"mass\"] * bodies[body2][\"mass\"] / distance**2         # Get the vector of forces         # using r and distance to get the fraction of force in x and y directions         # Strip the units and only add the value         tmp = f * r / distance         forces[i] += tmp.value   # Add the units back in   return forces * tmp.unit  In\u00a0[\u00a0]: Copied! <pre>%timeit get_forces(bodies)\n</pre> %timeit get_forces(bodies) <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n&lt;ipython-input-2-6264c27f7ac8&gt; in &lt;cell line: 0&gt;()\n----&gt; 1 get_ipython().run_line_magic('timeit', 'get_forces(bodies)')\n\n/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py in run_line_magic(self, magic_name, line, _stack_depth)\n   2416                 kwargs['local_ns'] = self.get_local_scope(stack_depth)\n   2417             with self.builtin_trap:\n-&gt; 2418                 result = fn(*args, **kwargs)\n   2419             return result\n   2420 \n\n&lt;decorator-gen-53&gt; in timeit(self, line, cell, local_ns)\n\n/usr/local/lib/python3.11/dist-packages/IPython/core/magic.py in &lt;lambda&gt;(f, *a, **k)\n    185     # but it's overkill for just that one bit of state.\n    186     def magic_deco(arg):\n--&gt; 187         call = lambda f, *a, **k: f(*a, **k)\n    188 \n    189         if callable(arg):\n\n/usr/local/lib/python3.11/dist-packages/IPython/core/magics/execution.py in timeit(self, line, cell, local_ns)\n   1178             for index in range(0, 10):\n   1179                 number = 10 ** index\n-&gt; 1180                 time_number = timer.timeit(number)\n   1181                 if time_number &gt;= 0.2:\n   1182                     break\n\n/usr/local/lib/python3.11/dist-packages/IPython/core/magics/execution.py in timeit(self, number)\n    167         gc.disable()\n    168         try:\n--&gt; 169             timing = self.inner(it, self.timer)\n    170         finally:\n    171             if gcold:\n\n&lt;magic-timeit&gt; in inner(_it, _timer)\n\nNameError: name 'bodies' is not defined</pre> <p>Let's write a function to update the positions and velocities</p> In\u00a0[\u00a0]: Copied! <pre>def update_positions_and_velocities(bodies, forces):\n  for i, body in enumerate(bodies):\n    # Update velocity based on force and mass\n    bodies[body][\"vel\"] += forces[i] / bodies[body][\"mass\"] * delta_t\n    # Update position based on velocity\n    bodies[body][\"pos\"] += bodies[body][\"vel\"] * delta_t\n\n\nbodies = create_bodies()\n\nprint(bodies[\"earth\"][\"pos\"])\nprint(bodies[\"earth\"][\"vel\"])\nforces = get_forces(bodies)\nupdate_positions_and_velocities(bodies, forces)\nprint(bodies[\"earth\"][\"pos\"])\nprint(bodies[\"earth\"][\"vel\"][0])\n</pre> def update_positions_and_velocities(bodies, forces):   for i, body in enumerate(bodies):     # Update velocity based on force and mass     bodies[body][\"vel\"] += forces[i] / bodies[body][\"mass\"] * delta_t     # Update position based on velocity     bodies[body][\"pos\"] += bodies[body][\"vel\"] * delta_t   bodies = create_bodies()  print(bodies[\"earth\"][\"pos\"]) print(bodies[\"earth\"][\"vel\"]) forces = get_forces(bodies) update_positions_and_velocities(bodies, forces) print(bodies[\"earth\"][\"pos\"]) print(bodies[\"earth\"][\"vel\"][0]) In\u00a0[\u00a0]: Copied! <pre># Create a new set of bodies\nbodies = create_bodies()\nfrom tqdm import tqdm\nfor i in tqdm(range(n_steps)):\n  # get the forces\n  forces = get_forces(bodies)\n  # apply the forces and update the positions\n  update_positions_and_velocities(bodies, forces)\n  # Store locations\n  for j, body in enumerate(bodies):\n    x_loc[j, i] = bodies[body][\"pos\"][0]\n    y_loc[j, i] = bodies[body][\"pos\"][1]\n    x_vel[j, i] = bodies[body][\"vel\"][0]\n    y_vel[j, i] = bodies[body][\"vel\"][1]\n</pre> # Create a new set of bodies bodies = create_bodies() from tqdm import tqdm for i in tqdm(range(n_steps)):   # get the forces   forces = get_forces(bodies)   # apply the forces and update the positions   update_positions_and_velocities(bodies, forces)   # Store locations   for j, body in enumerate(bodies):     x_loc[j, i] = bodies[body][\"pos\"][0]     y_loc[j, i] = bodies[body][\"pos\"][1]     x_vel[j, i] = bodies[body][\"vel\"][0]     y_vel[j, i] = bodies[body][\"vel\"][1] <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3652/3652 [00:34&lt;00:00, 104.72it/s]\n</pre> In\u00a0[\u00a0]: Copied! <pre># Let's visualise the data\n\nfig = plt.figure(figsize = (9,9))\nax = plt.axes()\nfor i, body in enumerate(bodies):\n    ax.plot(x_loc[i], y_loc[i], 'o')\n    # add and arrow with the planet name\n    ax.annotate(body,\n    (bodies[body][\"pos\"][0].value, bodies[body][\"pos\"][1].value + 0.1*AU.value)\n    )\n\nplt.ylim(-2 * AU.value ,2 * AU.value )\nplt.xlim(-2 * AU.value ,2 * AU.value )\nplt.ylabel(\"y [m]\")\nplt.xlabel(\"x [m]\")\nplt.grid()\n</pre> # Let's visualise the data  fig = plt.figure(figsize = (9,9)) ax = plt.axes() for i, body in enumerate(bodies):     ax.plot(x_loc[i], y_loc[i], 'o')     # add and arrow with the planet name     ax.annotate(body,     (bodies[body][\"pos\"][0].value, bodies[body][\"pos\"][1].value + 0.1*AU.value)     )  plt.ylim(-2 * AU.value ,2 * AU.value ) plt.xlim(-2 * AU.value ,2 * AU.value ) plt.ylabel(\"y [m]\") plt.xlabel(\"x [m]\") plt.grid() In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>Let's make an animation of the above to see the motion of the planets</p> In\u00a0[\u00a0]: Copied! <pre>from matplotlib.animation import FuncAnimation, PillowWriter\n\ncolors_key = {\n    \"sun\" : \"y\",\n    \"earth\" : \"b\",\n    \"mercury\" : \"g\",\n    \"venus\" : \"m\",\n    \"mars\" : \"r\"\n}\n\nfig, ax = plt.subplots()\nax.set_aspect('equal')\nax.set_xlim(-2 * AU.value, 2 * AU.value)\nax.set_ylim(-2 * AU.value, 2 * AU.value)\nscatters = [ax.plot([], [], 'o', color=colors_key[body])[0] for body in bodies]\n\ndef animate(frame):\n    for i, body in enumerate(bodies):\n        scatters[i].set_data(x_loc[i,frame].value, y_loc[i,frame].value)\n    return scatters\n\nanim = FuncAnimation(fig, animate, frames=n_steps, interval=1, blit=True)\n\nplt.show()\nanim.save(\"solar.gif\", writer=PillowWriter(fps=2))\n</pre> from matplotlib.animation import FuncAnimation, PillowWriter  colors_key = {     \"sun\" : \"y\",     \"earth\" : \"b\",     \"mercury\" : \"g\",     \"venus\" : \"m\",     \"mars\" : \"r\" }  fig, ax = plt.subplots() ax.set_aspect('equal') ax.set_xlim(-2 * AU.value, 2 * AU.value) ax.set_ylim(-2 * AU.value, 2 * AU.value) scatters = [ax.plot([], [], 'o', color=colors_key[body])[0] for body in bodies]  def animate(frame):     for i, body in enumerate(bodies):         scatters[i].set_data(x_loc[i,frame].value, y_loc[i,frame].value)     return scatters  anim = FuncAnimation(fig, animate, frames=n_steps, interval=1, blit=True)  plt.show() anim.save(\"solar.gif\", writer=PillowWriter(fps=2)) <pre>&lt;ipython-input-19-80aad0a1dc92&gt;:19: MatplotlibDeprecationWarning: Setting data with a non sequence type is deprecated since 3.7 and will be remove two minor releases later\n  scatters[i].set_data(x_loc[i,frame].value, y_loc[i,frame].value)\n</pre> <pre>&lt;ipython-input-19-80aad0a1dc92&gt;:19: MatplotlibDeprecationWarning: Setting data with a non sequence type is deprecated since 3.7 and will be remove two minor releases later\n  scatters[i].set_data(x_loc[i,frame].value, y_loc[i,frame].value)\n</pre> In\u00a0[\u00a0]: Copied! <pre>from IPython.display import Image\nImage('solar.gif')\n</pre> from IPython.display import Image Image('solar.gif') Out[\u00a0]: <pre>&lt;IPython.core.display.Image object&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Python/N_BodySimulation/#n-body-simulation","title":"N-Body Simulation\u00b6","text":"<p>Learning goals:</p> <ul> <li>Scientific Programming</li> <li>Working with <code>numpy</code> and <code>astropy</code></li> <li>Numerical Simulations</li> <li>Functions and <code>dictionaries</code></li> </ul>"},{"location":"Python/N_BodySimulation/#0-introduction","title":"0. Introduction\u00b6","text":"<p>In this notebook we will simulate planetary orbits using Python, Newtonian Gravity and AstroPy.</p> <p>We'll start by creating a new class to represent a body in our simulation. We'll use <code>numpy</code> to calculate the forces applied to each body as they move through space. We'll also use <code>astropy</code> for astrophysical constants and unit conversion.</p> <p>Our simulation will focus on implementing the Newton's law of universal gravitation: $$ F = G \\frac{m_1 m_2}{r^2}, $$ where $F$ is the force, $G$ is the gravitational constant, $m_1$ and $m_2$ are the masses of two bodies and $r$ is the distance between the two bodies.</p>"},{"location":"Python/N_BodySimulation/#1-importing-our-packages","title":"1. Importing our packages\u00b6","text":"<p>We'll be using the following packages:</p> <ul> <li><code>numpy</code> - for numerical calculations in Python</li> <li><code>astropy</code> - for converting units and getting astrophysical constants</li> <li><code>matplotlib</code> - for making nice visualization.</li> </ul>"},{"location":"Python/N_BodySimulation/#11-astropy","title":"1.1 AstroPy\u00b6","text":"<p>Let's take a look at how to use AstroPy.</p>"},{"location":"Python/N_BodySimulation/#12-workding-with-arrays-and-units","title":"1.2 Workding with arrays and units\u00b6","text":"<p>We'll only consider 2 dimensions. This means that the location of our bodies can be represented as a 2D vector. We'll define the origin [0,0] as the centre of the Solar System.</p> <p><code>numpy</code> works well with <code>astropy</code>. Let's see how we can perform some math with units.</p>"},{"location":"Python/N_BodySimulation/#2-storing-information","title":"2. Storing information\u00b6","text":"<p>We'll store the information about our bodies using a <code>dictionary</code>. A <code>dictionary</code> or <code>dict</code> is a non-ordered collection of objects in Python. We can access entries in the <code>dict</code> using <code>key</code>. A <code>key</code> is simply the name that we give to an entry.</p>"},{"location":"Python/N_BodySimulation/#21-working-with-a-dict","title":"2.1 Working with a <code>dict</code>\u00b6","text":"<p>We can define a dictionary with either <code>dict()</code> or using curly brackets <code>{}</code>:</p>"},{"location":"Python/N_BodySimulation/#22-setting-up-our-data-dictionaries","title":"2.2 Setting up our data dictionaries\u00b6","text":"<p>Now that we know how to use and modify dictionaries, let's look at storing some information about our solar system in a dictionary.</p> <p>We'll start with 4 objects, we can get some orbital details from here and here:</p> <ul> <li>The Sun, which will be located at the centre of the solar system (0,0) and be stationary ( velocity of (0,0) ) and have a mass of 1 $m_{sun}$.</li> <li>The Earth, which will be located at 1 AU, have a mass of 1 $m_{earth}$ and an starting velocity of (0, 29.8 km/s).</li> <li>Mercury, which will be located at 0.39 AU, have a mass of 0.06 $m_{earth}$ and an starting velocity of (0, 47.4 km/s).</li> <li>Venus, which will be located at 0.72 AU, have a mass of 0.81 $m_{earth}$ and an starting velocity of (0, 35.0 km/s).</li> <li>Mars, which will be located at 1.52 AU, have a mass of 0.11 $m_{earth}$ and an starting velocity of (0, 24.1 km/s).</li> </ul>"},{"location":"Python/N_BodySimulation/#23-visualize-the-solar-system","title":"2.3 Visualize the solar system\u00b6","text":"<p>Let's plot the locations of the planets within our solar system:</p>"},{"location":"Python/N_BodySimulation/#3-the-simulation","title":"3. The simulation\u00b6","text":"<p>Now that we have our data, let's look at how we're going to simulate the motions of the sun and planets.</p> <p>We're going to simulate a \"time step\" ($\\Delta t$). This means that we'll calculate some set of parameters (e.g. the acceleration) and run that for a time step (e.g. a day) and record the change in parameters (e.g. the position and velocity).</p> <p>What time step should we use?</p> <ul> <li>Seconds? -&gt; We'll be running for a very long time before we see changes</li> <li>Years? -&gt; This is too large, we'll likely miss some details</li> </ul> <p>Let's use 1 Earth day.</p> <p>How can we update the position and velocity? We'll need to know the forces: $$ F = G\\frac{m_1 m_2}{r} $$</p> <p>We also know how this relates to the acceleration: $$ F = ma $$ or $$ a = \\frac{F}{m} $$</p> <p>We also know that the acceleration can be represented as the change in velocity (calculus): $$ a = \\frac{dv}{dt} $$ If we deal with finite time steps giving a finite change in velocity: $$ a = \\frac{\\Delta v}{\\Delta t} $$ Rearranging and combing with our relation for the acceleration: $$\\Delta v = \\frac{F}{m} \\Delta t $$ For time step $\\Delta t$ we can now calculate the change in velocity of the body.</p> <p>Similarly the change in position of the body can be calculated as: $$ x = x_{old} + v \\Delta t,$$ Where $v = v_{old} \\Delta t$, $x_{old}$ and $v_{old}$ are the position and velocity from the previous step of the code.</p> <p>Because we have many bodies within our simulation, we'll need to account for the force of gravity from all bodies. The force on body $i$ is: $$ F_{i} = \\sum_{j = 0, j\\neq i} G\\frac{m_i m_j}{r_{ij}} $$ Where $r_{ij}$ is the distance between bodies $i$ and $j$. Note we're skipping $i ==j $. This is the case when we're getting the force on a body, when the second body is itself. Here $r_{ij} = 0$ and would result in infinite force ($\\frac{1}{0} \\rightarrow \\infty$), in reality the body doesn't experience any force due to itself so it is safe to ignore this term.</p> <p>So for our simulation we have the follow steps:</p> <ol> <li>Apply the time step $\\Delta t$.</li> <li>Determine the forces on each body due to each other body.</li> <li>Using the force, calculate the acceleration ($a$) and get the change in velocity $\\Delta v$.</li> <li>Applying Update the velocity and positions for each body.</li> </ol>"},{"location":"Python/N_BodySimulation/#31-setting-up-the-simulation","title":"3.1 Setting up the simulation\u00b6","text":"<p>It will also be useful to record some of the simulated values. Let's store the (x,y) position and velocity for each body.</p> <p>We'll run our simulation for 10 earth years with a time resolution of 1 day.</p>"},{"location":"Python/N_BodySimulation/#32-determining-the-forces","title":"3.2 Determining the forces\u00b6","text":"<p>Let's write a function to get the forces on each body.</p>"},{"location":"Python/N_BodySimulation/#4-running-our-simulation","title":"4. Running our simulation\u00b6","text":"<p>We'll run our simulation for 10 earth years with a time resolution of 1 day. Let's also use tqdm to get a progress bar.</p>"},{"location":"Python/N_BodySimulation/#5-whats-next","title":"5. What's next?\u00b6","text":"<p>We can use this base simulation to study our solar system. Here are some interesting extensions:</p> <ul> <li>How do the orbits change over time? Does the distance remain constant, does it change?</li> <li>Where is the centre of gravity of the solar system? In reality we say planets and stars orbit about the centre of mass/gravity or barycentre).</li> <li>What happens to the Sun? Does it remain in a fixed location? Is the mass of the planets enough to move the Sun?</li> <li>What is the impact of larger planet? Jupiter is the largest planet in our solar system, how does its inclusion effect the motion of the other planets and the Sun?</li> <li>Planetary defence? Jupiter plays an important role in \"cleaning\" the solar system of asteroid.</li> <li>How are the asteroids such as those in the astroid belt and the Trojans effected by Jupiter?</li> <li>Can we detect the signature of planets on the orbital behaviour of other planets?</li> <li>Can we predict the effect of hypothectical planets?</li> <li>Planet 9 is an hypothetical planet that would exist in the outer solar system</li> <li>Vulcan) is a hypothtical planet that was of interest in early solar system modelling.</li> <li>Stars don't always exist by themselves. What would happen to our solar system if we additional Stars. Can we model some of our favourite real of Scifi systems:</li> <li>Trappist-1</li> <li>Tatooine (Star Wars)</li> <li>Trisolaris (Alpha Centauri, Three Body Problem)</li> <li>Can we use this to simulate the long term light curve of a star?</li> <li>Can we use Kepler's Laws to simulate hypothetical solar systems?</li> </ul>"},{"location":"Python/ScientificPythonProgramming/","title":"0. Scientific Computing","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install numpy numba matplotlib scipy matplotlib line_profiler\n</pre> !pip install numpy numba matplotlib scipy matplotlib line_profiler <pre>Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (0.61.0)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.13.1)\nCollecting line_profiler\n  Downloading line_profiler-4.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\nRequirement already satisfied: llvmlite&lt;0.45,&gt;=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba) (0.44.0)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\nRequirement already satisfied: pillow&gt;=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil&gt;=2.7-&gt;matplotlib) (1.17.0)\nDownloading line_profiler-4.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (750 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 750.2/750.2 kB 16.3 MB/s eta 0:00:00\nInstalling collected packages: line_profiler\nSuccessfully installed line_profiler-4.2.0\n</pre> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\narr = np.array([0,1,2,3,4,5,6,7,8,9])\nprint (np.sum(arr))\n</pre> import numpy as np arr = np.array([0,1,2,3,4,5,6,7,8,9]) print (np.sum(arr)) <pre>45\n</pre> <p> Question: What happens when we combine different data types? </p> In\u00a0[\u00a0]: Copied! <pre>arr = np.array([0,1.2,True, \"hello\"])\nprint (type(arr[0]))\n</pre> arr = np.array([0,1.2,True, \"hello\"]) print (type(arr[0])) <pre>&lt;class 'numpy.str_'&gt;\n</pre> <p> Question: Modify the below code to convert the data type to an array of booleans (True/False) </p> In\u00a0[\u00a0]: Copied! <pre>arr_bool = np.array([1.2, 1.4, 0, 0, 8.9, None, True, \"False\"], dtype = bool)\nprint (arr_bool)\n</pre> arr_bool = np.array([1.2, 1.4, 0, 0, 8.9, None, True, \"False\"], dtype = bool) print (arr_bool) <pre>[ True  True False False  True False  True  True]\n</pre> In\u00a0[\u00a0]: Copied! <pre>my_even_array = np.arange(0,100, 2)\nprint (my_even_array)\n</pre> my_even_array = np.arange(0,100, 2) print (my_even_array) <pre>[ 0  2  4  6  8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46\n 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 92 94\n 96 98]\n</pre> <p>We can combine two or more NumPy arrays using the <code>np.append</code> function. This can help us allocate memory as we need it.</p> <p>Memory allocation can be an expensive operation. Calling <code>np.append</code> requires memory to be reallocated each time. The pure Python method of appending a list and then converting to a NumPy array is often a more effective method of dynamically allocating memory. Run the following cell to compare the run time for the two different methods.</p> In\u00a0[\u00a0]: Copied! <pre>from time import time\n\nlarge_n = int(10**4)\n\n# Append list using append\ntstart = time()\nmy_array = np.array([])\n\nfor i in range(large_n):\n    my_array = np.append(my_array, i)\n\ntstop = time()\nnumpy_duration = tstop - tstart\n\ntstart = time()\nmy_array = []\n# Convert to an array\nfor i in range(large_n):\n    my_array.append(i)\nmy_array = np.array(my_array)\ntstop = time()\nlist_duration = tstop - tstart\n\nprint (f\"numpy append took {numpy_duration:0.2e} s\")\nprint (f\"list append took {list_duration:0.2e} s\")\n</pre> from time import time  large_n = int(10**4)  # Append list using append tstart = time() my_array = np.array([])  for i in range(large_n):     my_array = np.append(my_array, i)  tstop = time() numpy_duration = tstop - tstart  tstart = time() my_array = [] # Convert to an array for i in range(large_n):     my_array.append(i) my_array = np.array(my_array) tstop = time() list_duration = tstop - tstart  print (f\"numpy append took {numpy_duration:0.2e} s\") print (f\"list append took {list_duration:0.2e} s\")    <pre>numpy append took 2.03e-01 s\nlist append took 6.84e-03 s\n</pre> <p>While NumPy is highly optimized, but often not the optimal solution. It's always important to test various solutions to find the optimal solution for our use case.</p> <p>In the above example we can see that if we don't know the size of our data arrays, then appending NumPy arrays will be a slow option.</p> In\u00a0[\u00a0]: Copied! <pre>import math\n\nx = 1.2\ny = math.sin(x)\ny_np = np.sin(x)\n\nprint (y, y_np)\n</pre> import math  x = 1.2 y = math.sin(x) y_np = np.sin(x)  print (y, y_np) <pre>0.9320390859672263 0.9320390859672263\n</pre> <p>Let's create some data and look at how <code>math</code> varies from <code>numpy</code>.</p> In\u00a0[\u00a0]: Copied! <pre>np.random.seed(42)\nx_data = np.random.normal(0,1,(100,100))\nprint(x_data[:5, :5])\n</pre> np.random.seed(42) x_data = np.random.normal(0,1,(100,100)) print(x_data[:5, :5]) <pre>[[ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337]\n [-1.41537074 -0.42064532 -0.34271452 -0.80227727 -0.16128571]\n [ 0.35778736  0.56078453  1.08305124  1.05380205 -1.37766937]\n [-0.82899501 -0.56018104  0.74729361  0.61037027 -0.02090159]\n [-1.59442766 -0.59937502  0.0052437   0.04698059 -0.45006547]]\n</pre> In\u00a0[\u00a0]: Copied! <pre>print (np.sin(x_data[:5, :5]))\n</pre> print (np.sin(x_data[:5, :5])) <pre>[[ 0.47653935 -0.13782419  0.60334467  0.9988594  -0.23201955]\n [-0.98794574 -0.40834961 -0.33604498 -0.71894082 -0.16058736]\n [ 0.35020257  0.53185073  0.88339184  0.86930874 -0.98140888]\n [-0.73725276 -0.53133958  0.67965603  0.57317091 -0.02090007]\n [-0.99972079 -0.56412655  0.00524368  0.04696331 -0.43502449]]\n</pre> In\u00a0[\u00a0]: Copied! <pre>print (math.sin(x_data[:5, :5]))\n</pre> print (math.sin(x_data[:5, :5])) <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-11-968b7f8b6039&gt; in &lt;cell line: 0&gt;()\n----&gt; 1 print (math.sin(x_data[:5, :5]))\n\nTypeError: only length-1 arrays can be converted to Python scalars</pre> <p><code>math.sin</code> expects a float or int value not a numpy array. We need to loop over the values one-by-one.</p> In\u00a0[\u00a0]: Copied! <pre>%%timeit\nx_prime = np.empty(x_data.shape)\nfor i in range(x_data.shape[0]):\n    for j in range(x_data.shape[1]):\n        x_prime[i][j] = math.sin(x_data[i][j])\n</pre> %%timeit x_prime = np.empty(x_data.shape) for i in range(x_data.shape[0]):     for j in range(x_data.shape[1]):         x_prime[i][j] = math.sin(x_data[i][j]) <pre>The slowest run took 4.49 times longer than the fastest. This could mean that an intermediate result is being cached.\n10.9 ms \u00b1 8.07 ms per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</pre> In\u00a0[\u00a0]: Copied! <pre>%%timeit\nx_prime = np.sin(x_data)\n</pre> %%timeit x_prime = np.sin(x_data) <pre>169 \u00b5s \u00b1 26.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n</pre> In\u00a0[\u00a0]: Copied! <pre>my_array = np.arange(0,101)\nis_even = my_array % 2 == 0\nprint (my_array[:5])\nprint (is_even[:5])\n</pre> my_array = np.arange(0,101) is_even = my_array % 2 == 0 print (my_array[:5]) print (is_even[:5]) <pre>[0 1 2 3 4]\n[ True False  True False  True]\n</pre> <p>We can use <code>is_even</code> as a mask when selecting data and performing operations. Let's see if we can calculate the sum of all even numbers between 0 and 100.</p> In\u00a0[\u00a0]: Copied! <pre>my_mask = my_array % 2 == 0\nmy_array[my_mask].sum()\n</pre> my_mask = my_array % 2 == 0 my_array[my_mask].sum() Out[\u00a0]: <pre>2550</pre> <p> Question: What is the sum of of all odd numbers between 0 and 100? </p> In\u00a0[\u00a0]: Copied! <pre>my_array[~my_mask].sum()\n</pre> my_array[~my_mask].sum() Out[\u00a0]: <pre>2500</pre> In\u00a0[\u00a0]: Copied! <pre># Define a function that will get the sqrt of a number\n# Return -1 instead of complexed numbers\ndef capped_sqrt(x):\n    if x &lt; 0:\n        return -1\n    else :\n        return np.sqrt(x)\n</pre> # Define a function that will get the sqrt of a number # Return -1 instead of complexed numbers def capped_sqrt(x):     if x &lt; 0:         return -1     else :         return np.sqrt(x) In\u00a0[\u00a0]: Copied! <pre>capped_sqrt(5), capped_sqrt(-100)\n</pre> capped_sqrt(5), capped_sqrt(-100) Out[\u00a0]: <pre>(2.23606797749979, -1)</pre> <p>Define an array of floats evenly spaced between -1 and 1.</p> <p>We can use <code>np.linspace</code> to create an array of linearly spaced numbers over an interval and <code>np.logspace</code> to create logarithmically spaced numbers over an interval.</p> In\u00a0[\u00a0]: Copied! <pre>x_data = np.linspace(-1,1)\n</pre> x_data = np.linspace(-1,1) In\u00a0[\u00a0]: Copied! <pre>capped_sqrt(x_data)\n</pre> capped_sqrt(x_data) <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-20-06fdb22e6c3c&gt; in &lt;cell line: 0&gt;()\n----&gt; 1 capped_sqrt(x_data)\n\n&lt;ipython-input-17-a5603be53ac4&gt; in capped_sqrt(x)\n      2 # Return -1 instead of complexed numbers\n      3 def capped_sqrt(x):\n----&gt; 4     if x &lt; 0:\n      5         return -1\n      6     else :\n\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()</pre> <p>We should see a <code>ValueError</code> with the error message</p> <p>The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\".</p> <p>This refers to the line:</p> <pre>if x &lt; 0:\n</pre> <p><code>x &lt; 0</code> will evaluate the entire array, returning an array of <code>True</code> or <code>False</code> where <code>x &lt; 0</code>. So <code>if x &lt; 0</code> will be evaluating <code>[False, False, ... , True, True]</code>. Python doesn't know how to evaluate this.</p> <p> Let's solve this using a for loop </p> In\u00a0[\u00a0]: Copied! <pre>%%timeit\nx_prime = np.empty(x_data.shape)\nfor i in range(x_data.shape[0]):\n    x_prime[i] = capped_sqrt(x_data[i])\n</pre> %%timeit x_prime = np.empty(x_data.shape) for i in range(x_data.shape[0]):     x_prime[i] = capped_sqrt(x_data[i]) <pre>37.8 \u00b5s \u00b1 653 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n</pre> <p><code>np.vectorize</code> allows the function to be recreated such that it can be applied to all elements of the array.</p> In\u00a0[\u00a0]: Copied! <pre>capped_sqrt_vectorized = np.vectorize(capped_sqrt)\n</pre> capped_sqrt_vectorized = np.vectorize(capped_sqrt) In\u00a0[\u00a0]: Copied! <pre>%%timeit\nx_prime = capped_sqrt_vectorized(x_data)\n</pre> %%timeit x_prime = capped_sqrt_vectorized(x_data) <pre>39 \u00b5s \u00b1 1.07 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n</pre> <p>It is important to note that <code>np.vectorize</code> doesn't always speed up functions. Its main goal is to vectorize functions by rewriting them in a form that can be applied to an entire vector of data.</p> In\u00a0[\u00a0]: Copied! <pre>x_prime = capped_sqrt_vectorized(x_data)\nx_prime[:10], x_prime[-10:]\n</pre> x_prime = capped_sqrt_vectorized(x_data) x_prime[:10], x_prime[-10:] Out[\u00a0]: <pre>(array([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]),\n array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1]))</pre> <p>What's going on here?</p> <p>We'll when we use the <code>vectorize</code> function, numpy will make some assumptions about the data so that we can speed up. In this case, the data assumed output type is an integer:</p> In\u00a0[\u00a0]: Copied! <pre>def capped_sqrt(x):\n    if x &lt; 0:\n        # return -1\n        # Explicitly setting the output to be a float\n        return -1.0\n    else :\n        return np.sqrt(x)\n\ncapped_sqrt_vectorized = np.vectorize(capped_sqrt)\n</pre> def capped_sqrt(x):     if x &lt; 0:         # return -1         # Explicitly setting the output to be a float         return -1.0     else :         return np.sqrt(x)  capped_sqrt_vectorized = np.vectorize(capped_sqrt) In\u00a0[\u00a0]: Copied! <pre>x_prime = capped_sqrt_vectorized(x_data)\nx_prime[:10], x_prime[-10:]\n</pre> x_prime = capped_sqrt_vectorized(x_data) x_prime[:10], x_prime[-10:] Out[\u00a0]: <pre>(array([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]),\n array([0.79539491, 0.82065181, 0.84515425, 0.86896608, 0.89214257,\n        0.91473203, 0.93677693, 0.95831485, 0.97937923, 1.        ]))</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>def monte_carlo_pi(nsamples):\n    acc = 0\n    for i in range(nsamples):\n        x = np.random.random()\n        y = np.random.random()\n        if (x ** 2 + y ** 2) &lt; 1.0:\n            acc += 1\n    return 4.0 * acc / nsamples\n</pre> def monte_carlo_pi(nsamples):     acc = 0     for i in range(nsamples):         x = np.random.random()         y = np.random.random()         if (x ** 2 + y ** 2) &lt; 1.0:             acc += 1     return 4.0 * acc / nsamples In\u00a0[\u00a0]: Copied! <pre>monte_carlo_pi(10**4)\n</pre> monte_carlo_pi(10**4) Out[\u00a0]: <pre>3.1212</pre> <p>Let's use <code>%timeit</code> to see how long this takes:</p> In\u00a0[\u00a0]: Copied! <pre>%timeit monte_carlo_pi(10**4)\n</pre> %timeit monte_carlo_pi(10**4) <pre>15.1 ms \u00b1 3.07 ms per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</pre> <p>Not bad, but if we want to optimize our code, we need to understand what we're spending most of our runtime on. For this we use line profiles like <code>line_profile</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Load the line profile\n%load_ext line_profiler\n</pre> # Load the line profile %load_ext line_profiler In\u00a0[\u00a0]: Copied! <pre>%lprun -f monte_carlo_pi monte_carlo_pi(10**4)\n</pre> %lprun -f monte_carlo_pi monte_carlo_pi(10**4) <p>Line profilers are perfect for trying to identify where in the code we have a bottleneck. From the above we can see that we're spending about 60% of the time generating random numbers and a further 20% of the time checking if the random numbers land within the circle. If we want to optmize this code, these should be the areas to focus on.</p> <p> Question: How can we speed up this calculation?</p> In\u00a0[\u00a0]: Copied! <pre>def numpy_calculate_pi(nsamples):\n    x = np.random.random(size = nsamples)\n    y = np.random.random(size = nsamples)\n    # Recall we can filter/mask using logic and applying to the entire array\n    # sum will cast true -&gt; 1 and false -&gt; 0\n    n_in = sum(x**2 + y**2 &lt; 1)\n    return 4. * n_in / nsamples\n</pre> def numpy_calculate_pi(nsamples):     x = np.random.random(size = nsamples)     y = np.random.random(size = nsamples)     # Recall we can filter/mask using logic and applying to the entire array     # sum will cast true -&gt; 1 and false -&gt; 0     n_in = sum(x**2 + y**2 &lt; 1)     return 4. * n_in / nsamples In\u00a0[\u00a0]: Copied! <pre>%timeit monte_carlo_pi(10**4)\n</pre> %timeit monte_carlo_pi(10**4) <pre>15.2 ms \u00b1 3.52 ms per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</pre> In\u00a0[\u00a0]: Copied! <pre>%timeit numpy_calculate_pi(10**4)\n</pre> %timeit numpy_calculate_pi(10**4) <pre>1.65 ms \u00b1 468 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n</pre> <p>Rewriting the code to use numpy, avoid looping and performing the summation in a single step provides a significant speedup!</p> In\u00a0[\u00a0]: Copied! <pre>from numba import jit\n</pre> from numba import jit In\u00a0[\u00a0]: Copied! <pre>@jit\ndef jit_calculate_pi(nsamples):\n    x = np.random.random(size = nsamples)\n    y = np.random.random(size = nsamples)\n    n_in = sum(x**2 + y**2 &lt; 1)\n    return 4. * n_in / nsamples\n</pre> @jit def jit_calculate_pi(nsamples):     x = np.random.random(size = nsamples)     y = np.random.random(size = nsamples)     n_in = sum(x**2 + y**2 &lt; 1)     return 4. * n_in / nsamples <p>Let's run this and the previous example one and time the execution using <code>%time</code>.</p> In\u00a0[\u00a0]: Copied! <pre>%time numpy_calculate_pi(10**4)\n</pre> %time numpy_calculate_pi(10**4) <pre>CPU times: user 2.4 ms, sys: 6 \u00b5s, total: 2.41 ms\nWall time: 2.81 ms\n</pre> Out[\u00a0]: <pre>3.1316</pre> In\u00a0[\u00a0]: Copied! <pre>%time jit_calculate_pi(10**4)\n</pre> %time jit_calculate_pi(10**4) <pre>CPU times: user 1.88 s, sys: 187 ms, total: 2.06 s\nWall time: 2.25 s\n</pre> Out[\u00a0]: <pre>3.1644</pre> <p>Why did it take longer to execute? As the phrase just-in-time suggests, we only compile the code in time for it to be executed. This means that the first time the function is executed, it is first compiled by Numba. After the first time, we'll use the compiled version of the function, not needing to recompile it.</p> <p>Let's run the function again:</p> In\u00a0[\u00a0]: Copied! <pre>%time jit_calculate_pi(10**4)\n</pre> %time jit_calculate_pi(10**4) <pre>CPU times: user 268 \u00b5s, sys: 0 ns, total: 268 \u00b5s\nWall time: 364 \u00b5s\n</pre> Out[\u00a0]: <pre>3.1368</pre> <p>Thats better!</p> <p>It is good practice to pre-compile and pre-run the functions before they are actually used. This is especially important if we're going to be running a long process for the first execution. Using the example above, instead of running with $10^4$ iterations on the first time, we could simply run with 1 iteration.</p> In\u00a0[\u00a0]: Copied! <pre>@jit(parallel= True)\ndef parallel_calculate_pi(nsamples):\n    x = np.random.random(size = nsamples)\n    y = np.random.random(size = nsamples)\n    n_in = sum(x**2 + y**2 &lt; 1)\n    return 4. * n_in / nsamples\n\nparallel_calculate_pi(1)\n</pre> @jit(parallel= True) def parallel_calculate_pi(nsamples):     x = np.random.random(size = nsamples)     y = np.random.random(size = nsamples)     n_in = sum(x**2 + y**2 &lt; 1)     return 4. * n_in / nsamples  parallel_calculate_pi(1) Out[\u00a0]: <pre>4.0</pre> In\u00a0[\u00a0]: Copied! <pre>%timeit jit_calculate_pi(10**4)\n</pre> %timeit jit_calculate_pi(10**4) <pre>166 \u00b5s \u00b1 37.3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n</pre> In\u00a0[\u00a0]: Copied! <pre>%timeit parallel_calculate_pi(10**4)\n</pre> %timeit parallel_calculate_pi(10**4) <pre>184 \u00b5s \u00b1 64.9 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n</pre> <p>We (might) observe a slight improvement in the runtime, which will depend on your own machine and the number of processors available. The performance will roughly scale with the number of cores your CPU has.</p> <p>We can use <code>prange</code> to specify a <code>for</code> loop that we want to run in parallel.</p> In\u00a0[\u00a0]: Copied! <pre>from numba import prange\n</pre> from numba import prange In\u00a0[\u00a0]: Copied! <pre>@jit(parallel=True)\ndef prange_pi(nsamples):\n    acc = 0\n    for i in prange(nsamples):\n        x = np.random.random()\n        y = np.random.random()\n        if (x ** 2 + y ** 2) &lt; 1.0:\n            acc += 1\n    return 4.0 * acc / nsamples\nprange_pi(1)\n</pre> @jit(parallel=True) def prange_pi(nsamples):     acc = 0     for i in prange(nsamples):         x = np.random.random()         y = np.random.random()         if (x ** 2 + y ** 2) &lt; 1.0:             acc += 1     return 4.0 * acc / nsamples prange_pi(1) Out[\u00a0]: <pre>0.0</pre> In\u00a0[\u00a0]: Copied! <pre>%timeit prange_pi(10**4)\n</pre> %timeit prange_pi(10**4) <pre>277 \u00b5s \u00b1 114 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n</pre> In\u00a0[\u00a0]: Copied! <pre>%timeit monte_carlo_pi(10**4)\n</pre> %timeit monte_carlo_pi(10**4) <pre>14.7 ms \u00b1 3.46 ms per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</pre> <p>Using Numba's just in time compiler can provide significant speed up to our code with very little overhead.</p> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport scipy as sp\nimport numpy as np\n</pre> import matplotlib.pyplot as plt import scipy as sp import numpy as np In\u00a0[\u00a0]: Copied! <pre>from scipy.optimize import minimize\n# Define the potential energy function with non-zero minimum\ndef potential_energy(x, k=0.5, lam=0.1, c=-0.2, d=5):\n    return 0.5 * k * x**2 + c * x + d\n\n# Initial guess for the minimum position\nx0 = np.array([0.5])\n\n# Perform the minimization\nresult = minimize(potential_energy, x0)\n\n\n# Plot the potential energy function and the minimum point\nx_values = np.linspace(-2, 2, 400)\ny_values = potential_energy(x_values)\n\nplt.plot(x_values, y_values, label='Potential Energy U(x)')\nplt.scatter(result.x, result.fun, color='red', zorder=5, label=f'Minimum x = {result.x[0]:0.2f}')\nplt.xlabel('Displacement (x)')\nplt.ylabel('Potential Energy U(x)')\nplt.title('Potential Energy with Non-Zero Minimum')\nplt.legend()\nplt.grid(True)\n\nprint (result.x[0])\nprint (- (-0.2) / 0.5)\n</pre> from scipy.optimize import minimize # Define the potential energy function with non-zero minimum def potential_energy(x, k=0.5, lam=0.1, c=-0.2, d=5):     return 0.5 * k * x**2 + c * x + d  # Initial guess for the minimum position x0 = np.array([0.5])  # Perform the minimization result = minimize(potential_energy, x0)   # Plot the potential energy function and the minimum point x_values = np.linspace(-2, 2, 400) y_values = potential_energy(x_values)  plt.plot(x_values, y_values, label='Potential Energy U(x)') plt.scatter(result.x, result.fun, color='red', zorder=5, label=f'Minimum x = {result.x[0]:0.2f}') plt.xlabel('Displacement (x)') plt.ylabel('Potential Energy U(x)') plt.title('Potential Energy with Non-Zero Minimum') plt.legend() plt.grid(True)  print (result.x[0]) print (- (-0.2) / 0.5) <pre>0.40000009536743164\n0.4\n</pre> In\u00a0[\u00a0]: Copied! <pre>from scipy.optimize import rosen\n\nfig = plt.figure(figsize = (11,6))\nx = np.linspace(-2, 2, 50)\nX, Y = np.meshgrid(x, x)\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(X, Y, rosen([X, Y]))\n</pre> from scipy.optimize import rosen  fig = plt.figure(figsize = (11,6)) x = np.linspace(-2, 2, 50) X, Y = np.meshgrid(x, x) ax = fig.add_subplot(111, projection='3d') ax.plot_surface(X, Y, rosen([X, Y]))  Out[\u00a0]: <pre>&lt;mpl_toolkits.mplot3d.art3d.Poly3DCollection at 0x7b1be70392d0&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>p = plt.contour(X, Y, rosen([X, Y]), levels=100 )\nplt.scatter(1,1)\nplt.colorbar()\n</pre> p = plt.contour(X, Y, rosen([X, Y]), levels=100 ) plt.scatter(1,1) plt.colorbar() Out[\u00a0]: <pre>&lt;matplotlib.colorbar.Colorbar at 0x7b1be930ee50&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>minimize(rosen, [0,0])\n</pre> minimize(rosen, [0,0]) Out[\u00a0]: <pre>  message: Optimization terminated successfully.\n  success: True\n   status: 0\n      fun: 2.8439915001532524e-11\n        x: [ 1.000e+00  1.000e+00]\n      nit: 19\n      jac: [ 3.987e-06 -2.844e-06]\n hess_inv: [[ 4.948e-01  9.896e-01]\n            [ 9.896e-01  1.984e+00]]\n     nfev: 72\n     njev: 24</pre> <p> Exercise:</p> <p>Find the point of intersection between the line and the parabola using <code>scipy.optimize.minimize</code></p> <p>Let:</p> <p>$$ f(x) = m x + c $$</p> <p>$$ g(y) = p_0 y^2 + p_1 y + p_2 $$</p> <p>m = 0.1, c = 0, $p_0$ = -1, $p_1 = 3$, $p_2 = 7$</p> In\u00a0[\u00a0]: Copied! <pre># Define a line\ndef line(x, m, c):\n    return x * m + c\n\n# Define a parabola\ndef para(x, args):\n    return args[0] * x ** 2 + args[1] * x + args[2]\n</pre> # Define a line def line(x, m, c):     return x * m + c  # Define a parabola def para(x, args):     return args[0] * x ** 2 + args[1] * x + args[2] In\u00a0[\u00a0]: Copied! <pre>m = 0.1\nc = 0\nx = np.linspace(0, 10)\ny = np.linspace(0,1)\n\nargs = [-1, 3, 7]\nplt.plot(x, line(x, m, c), label = \"$f(x) = mx + c$\")\nplt.plot(para(y, args), y, label = \"$g(y) = p_0 y^2 + p_1 y + p_2$\")\nplt.grid()\nplt.legend()\nplt.ylabel(\"Y\")\nplt.xlabel(\"X\")\n</pre> m = 0.1 c = 0 x = np.linspace(0, 10) y = np.linspace(0,1)  args = [-1, 3, 7] plt.plot(x, line(x, m, c), label = \"$f(x) = mx + c$\") plt.plot(para(y, args), y, label = \"$g(y) = p_0 y^2 + p_1 y + p_2$\") plt.grid() plt.legend() plt.ylabel(\"Y\") plt.xlabel(\"X\")   Out[\u00a0]: <pre>Text(0.5, 0, 'X')</pre> <p> How can we approach this? </p> In\u00a0[\u00a0]: Copied! <pre>def dist(x):\n    m = 0.1\n    c = 0\n    args = [-1, 3, 7]\n    y_test = line(x,m,c)\n    return np.abs( x - para(y_test, args))\n</pre> def dist(x):     m = 0.1     c = 0     args = [-1, 3, 7]     y_test = line(x,m,c)     return np.abs( x - para(y_test, args)) In\u00a0[\u00a0]: Copied! <pre>x_opt = sp.optimize.minimize(dist, 9).x[0]\ny_opt = line(x_opt, m, c)\nprint (x_opt, y_opt)\n</pre> x_opt = sp.optimize.minimize(dist, 9).x[0] y_opt = line(x_opt, m, c) print (x_opt, y_opt) <pre>8.87482193358592 0.8874821933585921\n</pre> In\u00a0[\u00a0]: Copied! <pre>fig, axs = plt.subplots(2,1, figsize = (11,6), sharex = True)\naxs[0].plot(x, line(x, m, c), label = \"$f(x) = mx + c$\")\naxs[0].plot(para(y, args), y, label = \"$g(y) = p_0 y^2 + p_1 y + p_2$\")\naxs[0].scatter(x_opt, y_opt, color = \"C3\", label = f\"x = {x_opt:0.2f}, y = {y_opt:0.2f}\")\naxs[0].axvline(x_opt, color = \"k\", ls = \"--\")\n\naxs[0].grid()\naxs[0].legend()\naxs[0].set_ylabel(\"Y\")\naxs[0].set_xlabel(\"X\")\n\nx_plot = np.linspace(0,10,100)\naxs[1].plot(x_plot, dist(x_plot))\naxs[1].axvline(x_opt, color = \"k\", ls = \"--\")\naxs[1].grid()\naxs[1].set_ylabel(\"Distance between Lines\")\naxs[1].set_xlabel(\"X\")\n\nfig.tight_layout()\n</pre>  fig, axs = plt.subplots(2,1, figsize = (11,6), sharex = True) axs[0].plot(x, line(x, m, c), label = \"$f(x) = mx + c$\") axs[0].plot(para(y, args), y, label = \"$g(y) = p_0 y^2 + p_1 y + p_2$\") axs[0].scatter(x_opt, y_opt, color = \"C3\", label = f\"x = {x_opt:0.2f}, y = {y_opt:0.2f}\") axs[0].axvline(x_opt, color = \"k\", ls = \"--\")  axs[0].grid() axs[0].legend() axs[0].set_ylabel(\"Y\") axs[0].set_xlabel(\"X\")  x_plot = np.linspace(0,10,100) axs[1].plot(x_plot, dist(x_plot)) axs[1].axvline(x_opt, color = \"k\", ls = \"--\") axs[1].grid() axs[1].set_ylabel(\"Distance between Lines\") axs[1].set_xlabel(\"X\")  fig.tight_layout() <p>Let's define an exponential decay model:</p> <p>$$ f(x) = N e^{-x \\tau} + c$$</p> In\u00a0[\u00a0]: Copied! <pre>def model(x, *args):\n    return args[0] * np.exp(- x * args[1]) + args[2]\n</pre> def model(x, *args):     return args[0] * np.exp(- x * args[1]) + args[2] In\u00a0[\u00a0]: Copied! <pre>true_parameters = [ 10, 0.5, 3]\nplt.plot(x_plot, model(x_plot, *true_parameters))\nplt.xlabel(\"Time\")\nplt.ylabel(\"Abundence\")\nplt.grid()\n</pre> true_parameters = [ 10, 0.5, 3] plt.plot(x_plot, model(x_plot, *true_parameters)) plt.xlabel(\"Time\") plt.ylabel(\"Abundence\") plt.grid() In\u00a0[\u00a0]: Copied! <pre># Generate some noisy data using numpy.random\nnp.random.seed(12345)\nx_data = np.random.uniform(0,10, size = 50)\n# Adding noise to the data\ny_data = np.random.normal(0, 0.9, size = 50) + model(x_data, *true_parameters)\n\nplt.plot(x_plot, model(x_plot, *true_parameters))\nplt.plot(x_data, y_data, \"C1o\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Abundence\")\nplt.grid()\n</pre> # Generate some noisy data using numpy.random np.random.seed(12345) x_data = np.random.uniform(0,10, size = 50) # Adding noise to the data y_data = np.random.normal(0, 0.9, size = 50) + model(x_data, *true_parameters)  plt.plot(x_plot, model(x_plot, *true_parameters)) plt.plot(x_data, y_data, \"C1o\") plt.xlabel(\"Time\") plt.ylabel(\"Abundence\") plt.grid() In\u00a0[\u00a0]: Copied! <pre># Get get the best fit parameters\nguess = [ 1, 1, 1]\n\npopt, pcov = sp.optimize.curve_fit(model, x_data, y_data, p0 = guess)\nprint (popt)\nprint (pcov)\n</pre> # Get get the best fit parameters guess = [ 1, 1, 1]  popt, pcov = sp.optimize.curve_fit(model, x_data, y_data, p0 = guess) print (popt) print (pcov) <pre>[10.28404428  0.46501049  2.70482802]\n[[ 0.26219225  0.00951629 -0.01244426]\n [ 0.00951629  0.00316383  0.01059212]\n [-0.01244426  0.01059212  0.05940681]]\n</pre> <p><code>curve_fit</code> returns the optimal parameters (<code>popt</code>) and the covariance matrix (<code>pcov</code>).</p> <p>We can get the estimated uncertinty from the covariance matrix: $$pcov_{i,j} = \\partial x_{i} \\partial x_{j}$$</p> <p>So the uncertiy on parameter $i$ is: $$\\Delta x_i = \\sqrt{\\partial x_{i}^2} = \\sqrt{pcov_{i,i}}$$</p> In\u00a0[\u00a0]: Copied! <pre>perr = np.sqrt(np.diag(pcov))\nfor t, p, e in zip(true_parameters, popt, perr):\n  print (f\"True {t:0.2f} -&gt; {p:0.2f} +/- {e:0.2f}\")\n</pre> perr = np.sqrt(np.diag(pcov)) for t, p, e in zip(true_parameters, popt, perr):   print (f\"True {t:0.2f} -&gt; {p:0.2f} +/- {e:0.2f}\") <pre>True 10.00 -&gt; 10.28 +/- 0.51\nTrue 0.50 -&gt; 0.47 +/- 0.06\nTrue 3.00 -&gt; 2.70 +/- 0.24\n</pre> <p>Here we have gaussian noise, where everything is very well behaved. Let's look at a more challanging exampe.</p> In\u00a0[\u00a0]: Copied! <pre>np.random.seed(12345)\nx_data = np.random.uniform(0, 10, 100)\ny_data = np.array([ np.random.poisson(model(x, *true_parameters)) for x in x_data ])\n</pre> np.random.seed(12345) x_data = np.random.uniform(0, 10, 100) y_data = np.array([ np.random.poisson(model(x, *true_parameters)) for x in x_data ]) In\u00a0[\u00a0]: Copied! <pre>x_plot = np.linspace(0,10)\nplt.plot(x_plot, model(x_plot, *true_parameters))\nplt.plot(x_data, y_data, \"C1o\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Abundence\")\nplt.grid()\n</pre> x_plot = np.linspace(0,10) plt.plot(x_plot, model(x_plot, *true_parameters)) plt.plot(x_data, y_data, \"C1o\") plt.xlabel(\"Time\") plt.ylabel(\"Abundence\") plt.grid() In\u00a0[\u00a0]: Copied! <pre># Get get the best fit parameters\nguess = [ 1, 1, 1]\n\npopt, pcov = sp.optimize.curve_fit(model, x_data, y_data, p0 = guess)\n</pre> # Get get the best fit parameters guess = [ 1, 1, 1]  popt, pcov = sp.optimize.curve_fit(model, x_data, y_data, p0 = guess) In\u00a0[\u00a0]: Copied! <pre>perr = np.sqrt(np.diag(pcov))\nfor t, p, e in zip(true_parameters, popt, perr):\n  print (f\"True {t:0.2f} -&gt; {p:0.2f} +/- {e:0.2f}\")\n</pre> perr = np.sqrt(np.diag(pcov)) for t, p, e in zip(true_parameters, popt, perr):   print (f\"True {t:0.2f} -&gt; {p:0.2f} +/- {e:0.2f}\") <pre>True 10.00 -&gt; 9.58 +/- 1.08\nTrue 0.50 -&gt; 0.55 +/- 0.11\nTrue 3.00 -&gt; 3.16 +/- 0.33\n</pre> <p>Write a function to perform a bootstrap fit. It should take in a function, x-data, y-data, and some initial guess. Use the following as a guide:</p> <pre><code>{code-block}\ndef bootstrap(func, x, y, yerr=None, nboot=100, p0=None):\n    \"\"\"Function performing a bootstrap fit to data\n\n    Args:\n        func: The function to fit our data to.\n        x: The x data\n        y: The y data\n        yerr: The uncertainty on y (optional)\n        n_boot: The number of bootstrap samples\n        p0: The initial guess for the fitter\n\n    Returns:\n        Bootstrap samples with shape [n_boot, n_free_parameters]\n\n    \"\"\"\n</code></pre> In\u00a0[\u00a0]: Copied! <pre>def bootstrap(func, x, y, yerr = None, nboot = 100, p0 = None):\n    \"\"\"Function performing a bootstrap fit to data\n\n    Args:\n        func: The function to fit our data to.\n        x: The x data\n        y: The y data\n        yerr: The uncertainty on y (optional)\n        n_boot: The number of bootstrap samples\n        p0: The initial guess for the fitter\n\n    Returns:\n        Bootstrap samples with shape [n_boot, n_free_parameters]\n\n    \"\"\"\n    samples = []\n\n    for i in range(nboot):\n        try:\n            rnd_int = np.random.choice(\n                np.arange(len(x)),\n                size=len(x),\n                replace=True\n            )\n            x_samp = x[rnd_int]\n            y_samp = y[rnd_int]\n            if yerr is not None:\n                y_samp_err = y_err[rnd_int]\n            else:\n                y_samp_err = None\n\n            p, _ = sp.optimize.curve_fit(func, x_samp, y_samp, p0=p0, maxfev = 10000 )\n            samples.append(p)\n        except RuntimeError as e:\n            continue\n    return np.array(samples)\n</pre> def bootstrap(func, x, y, yerr = None, nboot = 100, p0 = None):     \"\"\"Function performing a bootstrap fit to data      Args:         func: The function to fit our data to.         x: The x data         y: The y data         yerr: The uncertainty on y (optional)         n_boot: The number of bootstrap samples         p0: The initial guess for the fitter      Returns:         Bootstrap samples with shape [n_boot, n_free_parameters]      \"\"\"     samples = []      for i in range(nboot):         try:             rnd_int = np.random.choice(                 np.arange(len(x)),                 size=len(x),                 replace=True             )             x_samp = x[rnd_int]             y_samp = y[rnd_int]             if yerr is not None:                 y_samp_err = y_err[rnd_int]             else:                 y_samp_err = None              p, _ = sp.optimize.curve_fit(func, x_samp, y_samp, p0=p0, maxfev = 10000 )             samples.append(p)         except RuntimeError as e:             continue     return np.array(samples) In\u00a0[\u00a0]: Copied! <pre>samples = bootstrap(model, x_data, y_data, p0 = [5, 1, 1], nboot = 1000)\n</pre> samples = bootstrap(model, x_data, y_data, p0 = [5, 1, 1], nboot = 1000) In\u00a0[\u00a0]: Copied! <pre>fig, axs = plt.subplots(1,3, figsize = (12,4))\nparam_names = [\"N\", \"$\\\\tau$\", \"C\"]\nfor i in range(3):\n    quants = np.quantile(samples[:,i], q = [0.16, 0.5, 0.84])\n    binning = np.linspace(true_parameters[i] - 5*perr[i], true_parameters[i] + 5*perr[i], 20)\n    # axs[i].hist(samples[:,i], bins = binning)\n    axs[i].hist(samples[:,i], bins = 30)\n    axs[i].axvline(true_parameters[i], label = \"True\", color = \"r\", ls = \"-\")\n    axs[i].axvline(popt[i], label = \"Curve_fit\", color = \"C4\", ls = \"-.\")\n    axs[i].axvline(popt[i] + perr[i], color = \"C4\", ls = \"-.\")\n    axs[i].axvline(popt[i] - perr[i], color = \"C4\", ls = \"-.\")\n\n\n    axs[i].set_xlabel(param_names[i])\n    axs[i].grid()\n    if_label = \"BootStrap\"\n\n    skew = sp.stats.skew(samples[:,i])\n    kur = sp.stats.kurtosis(samples[:,i])\n    axs[i].set_title(f\"Skewness = {skew:0.2f}, Kurtosis = {kur:0.2f}\")\n\n    for q in quants:\n        axs[i].axvline(q, color = \"k\", ls = \"--\", label = if_label)\n        if_label = None\naxs[0].legend();\n</pre> fig, axs = plt.subplots(1,3, figsize = (12,4)) param_names = [\"N\", \"$\\\\tau$\", \"C\"] for i in range(3):     quants = np.quantile(samples[:,i], q = [0.16, 0.5, 0.84])     binning = np.linspace(true_parameters[i] - 5*perr[i], true_parameters[i] + 5*perr[i], 20)     # axs[i].hist(samples[:,i], bins = binning)     axs[i].hist(samples[:,i], bins = 30)     axs[i].axvline(true_parameters[i], label = \"True\", color = \"r\", ls = \"-\")     axs[i].axvline(popt[i], label = \"Curve_fit\", color = \"C4\", ls = \"-.\")     axs[i].axvline(popt[i] + perr[i], color = \"C4\", ls = \"-.\")     axs[i].axvline(popt[i] - perr[i], color = \"C4\", ls = \"-.\")       axs[i].set_xlabel(param_names[i])     axs[i].grid()     if_label = \"BootStrap\"      skew = sp.stats.skew(samples[:,i])     kur = sp.stats.kurtosis(samples[:,i])     axs[i].set_title(f\"Skewness = {skew:0.2f}, Kurtosis = {kur:0.2f}\")      for q in quants:         axs[i].axvline(q, color = \"k\", ls = \"--\", label = if_label)         if_label = None axs[0].legend(); <pre>/usr/local/lib/python3.11/dist-packages/numpy/core/function_base.py:158: RuntimeWarning: invalid value encountered in multiply\n  y *= step\n/usr/local/lib/python3.11/dist-packages/numpy/core/function_base.py:168: RuntimeWarning: invalid value encountered in add\n  y += start\n</pre> In\u00a0[\u00a0]: Copied! <pre>for i in range(3):\n    p = np.quantile(samples[:,i], q=[0.5, 0.05, 0.95] )\n    print (f\"{param_names[i]} (Truth: {true_parameters[i]}) \\t = {p[0]:0.2f} [{p[1]:0.2f}, {p[2]:0.2f}] \\n\\t-&gt; (Curve Fit) {popt[i]:0.2f} +/- {perr[i] : 0.2f}\")\n</pre> for i in range(3):     p = np.quantile(samples[:,i], q=[0.5, 0.05, 0.95] )     print (f\"{param_names[i]} (Truth: {true_parameters[i]}) \\t = {p[0]:0.2f} [{p[1]:0.2f}, {p[2]:0.2f}] \\n\\t-&gt; (Curve Fit) {popt[i]:0.2f} +/- {perr[i] : 0.2f}\") <pre>N (Truth: 10) \t = 9.79 [7.06, 12.57] \n\t-&gt; (Curve Fit) 5.24 +/-  inf\n$\\tau$ (Truth: 0.5) \t = 0.55 [0.33, 0.92] \n\t-&gt; (Curve Fit) 5.05 +/-  inf\nC (Truth: 3) \t = 3.12 [2.38, 3.64] \n\t-&gt; (Curve Fit) 0.93 +/-  inf\n</pre> In\u00a0[\u00a0]: Copied! <pre># @title\nimport numpy as np\nimport scipy.optimize as opt\nfrom numba import vectorize\n\nnp.random.seed(42)\nx = np.linspace(-5, 10, 100)\n\n# True parameters\nA_true, x0_true, sigma_true, B_true, C_true, cutoff_true = 5.0, 5.0, 1.0, 0.5, 2.0, 1.0  # Cutoff at x = 3\np_true = [A_true, x0_true, sigma_true, B_true, C_true, cutoff_true]\n# Generate true values\ny_true = A_true * np.exp(-((x - x0_true) ** 2) / (2 * sigma_true ** 2)) + B_true * x + C_true\n\n# Apply cutoff to x\ny_true[x &lt; cutoff_true] = 0\n\n# Add noise\ny_noisy = y_true + np.random.randn(len(x))\n</pre> # @title import numpy as np import scipy.optimize as opt from numba import vectorize  np.random.seed(42) x = np.linspace(-5, 10, 100)  # True parameters A_true, x0_true, sigma_true, B_true, C_true, cutoff_true = 5.0, 5.0, 1.0, 0.5, 2.0, 1.0  # Cutoff at x = 3 p_true = [A_true, x0_true, sigma_true, B_true, C_true, cutoff_true] # Generate true values y_true = A_true * np.exp(-((x - x0_true) ** 2) / (2 * sigma_true ** 2)) + B_true * x + C_true  # Apply cutoff to x y_true[x &lt; cutoff_true] = 0  # Add noise y_noisy = y_true + np.random.randn(len(x)) In\u00a0[\u00a0]: Copied! <pre>plt.plot(x, y_true, label = \"True\")\nplt.plot(x, y_noisy, label = \"Observed\")\nplt.xlabel(\"Distance from origin [mm]\")\nplt.ylabel(\"Amplitude [AU]\")\nplt.grid()\nplt.legend();\n</pre>  plt.plot(x, y_true, label = \"True\") plt.plot(x, y_noisy, label = \"Observed\") plt.xlabel(\"Distance from origin [mm]\") plt.ylabel(\"Amplitude [AU]\") plt.grid() plt.legend();  In\u00a0[\u00a0]: Copied! <pre># Unoptimized function\ndef gaussian_with_background(x, A, x0, sigma, B, C, C_cutoff):\n    result = []\n    for xi in x:\n        if xi &lt; C_cutoff:\n            result.append(0.)\n        else:\n            value = A * np.exp(-((xi - x0) ** 2) / (2 * sigma ** 2)) + B * xi + C\n            result.append(value)\n    return np.array(result)\n\n\nguess = [1, 5, 1, 0.1, 1, 2]\n\n# Fit the data\n%timeit popt, pcov = opt.curve_fit(gaussian_with_background, x, y_noisy, p0=guess)\npopt, pcov = opt.curve_fit(gaussian_with_background, x, y_noisy, p0=guess)\nperr = np.sqrt(np.diag(pcov))\n\nprint(\"Fitted Parameters (Unoptimized):\")\nfor t, p, e in zip(p_true, popt, perr):\n  print (f\"{t} -&gt; {p:0.2f} +/- {e:0.2f}\")\n</pre> # Unoptimized function def gaussian_with_background(x, A, x0, sigma, B, C, C_cutoff):     result = []     for xi in x:         if xi &lt; C_cutoff:             result.append(0.)         else:             value = A * np.exp(-((xi - x0) ** 2) / (2 * sigma ** 2)) + B * xi + C             result.append(value)     return np.array(result)   guess = [1, 5, 1, 0.1, 1, 2]  # Fit the data %timeit popt, pcov = opt.curve_fit(gaussian_with_background, x, y_noisy, p0=guess) popt, pcov = opt.curve_fit(gaussian_with_background, x, y_noisy, p0=guess) perr = np.sqrt(np.diag(pcov))  print(\"Fitted Parameters (Unoptimized):\") for t, p, e in zip(p_true, popt, perr):   print (f\"{t} -&gt; {p:0.2f} +/- {e:0.2f}\")  <pre>&lt;magic-timeit&gt;:1: OptimizeWarning: Covariance of the parameters could not be estimated\n</pre> <pre>7.49 ms \u00b1 1.95 ms per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\nFitted Parameters (Unoptimized):\n5.0 -&gt; 5.24 +/- inf\n5.0 -&gt; 5.05 +/- inf\n1.0 -&gt; 0.93 +/- inf\n0.5 -&gt; 0.45 +/- inf\n2.0 -&gt; 2.37 +/- inf\n1.0 -&gt; 2.00 +/- inf\n</pre> <pre>&lt;ipython-input-138-f63519f95ccd&gt;:17: OptimizeWarning: Covariance of the parameters could not be estimated\n  popt, pcov = opt.curve_fit(gaussian_with_background, x, y_noisy, p0=guess)\n</pre> In\u00a0[\u00a0]: Copied! <pre>%lprun -f gaussian_with_background gaussian_with_background(x, *p_true)\n</pre> %lprun -f gaussian_with_background gaussian_with_background(x, *p_true) In\u00a0[\u00a0]: Copied! <pre># Optimized function using NumPy\ndef gaussian_with_background_numpy(x, A, x0, sigma, B, C, C_cutoff):\n    result = A * np.exp(-((x - x0) ** 2) / (2 * sigma ** 2)) + B * x + C\n    return np.where(x &gt; C_cutoff, result, 0.)  # Apply cutoff in x\n\n# Fit using the NumPy-optimized function\n%timeit popt_numpy, pcov_numpy = opt.curve_fit(gaussian_with_background_numpy, x, y_noisy, p0=guess)\npopt_numpy, pcov_numpy = opt.curve_fit(gaussian_with_background_numpy, x, y_noisy, p0=guess)\nperr_numpy = np.sqrt(np.diag(pcov_numpy))\nprint(\"Fitted Parameters (NumPy):\")\nfor t, p, e in zip(p_true, popt_numpy, perr_numpy):\n  print (f\"{t} -&gt; {p:0.2f} +/- {e:0.2f}\")\n</pre> # Optimized function using NumPy def gaussian_with_background_numpy(x, A, x0, sigma, B, C, C_cutoff):     result = A * np.exp(-((x - x0) ** 2) / (2 * sigma ** 2)) + B * x + C     return np.where(x &gt; C_cutoff, result, 0.)  # Apply cutoff in x  # Fit using the NumPy-optimized function %timeit popt_numpy, pcov_numpy = opt.curve_fit(gaussian_with_background_numpy, x, y_noisy, p0=guess) popt_numpy, pcov_numpy = opt.curve_fit(gaussian_with_background_numpy, x, y_noisy, p0=guess) perr_numpy = np.sqrt(np.diag(pcov_numpy)) print(\"Fitted Parameters (NumPy):\") for t, p, e in zip(p_true, popt_numpy, perr_numpy):   print (f\"{t} -&gt; {p:0.2f} +/- {e:0.2f}\") <pre>&lt;magic-timeit&gt;:1: OptimizeWarning: Covariance of the parameters could not be estimated\n</pre> <pre>2.1 ms \u00b1 542 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\nFitted Parameters (NumPy):\n5.0 -&gt; 5.24 +/- inf\n5.0 -&gt; 5.05 +/- inf\n1.0 -&gt; 0.93 +/- inf\n0.5 -&gt; 0.45 +/- inf\n2.0 -&gt; 2.37 +/- inf\n1.0 -&gt; 2.00 +/- inf\n</pre> <pre>&lt;ipython-input-140-bcd810126830&gt;:8: OptimizeWarning: Covariance of the parameters could not be estimated\n  popt_numpy, pcov_numpy = opt.curve_fit(gaussian_with_background_numpy, x, y_noisy, p0=guess)\n</pre> In\u00a0[\u00a0]: Copied! <pre># Fully optimized using Numba's vectorize\n@jit(nopython=True)\ndef gaussian_with_background_numba(x, A, x0, sigma, B, C, C_cutoff):\n    res = A * np.exp(-((x - x0) ** 2) / (2 * sigma ** 2)) + B * x + C\n    res[x&lt;C_cutoff] = 0.0\n    return res\n\n# Fit using the Numba-optimized function\n%timeit popt_numba, pcov_numba = opt.curve_fit(gaussian_with_background_numba, x, y_noisy, p0=guess)\n\npopt_numba, pcov_numba = opt.curve_fit(gaussian_with_background_numba, x, y_noisy, p0=guess)\nperr_numba = np.sqrt(np.diag(pcov_numba))\n\nprint(\"Fitted Parameters (Numba):\")\nfor t, p, e in zip(p_true, popt_numba, perr_numba):\n  print (f\"{t} -&gt; {p:0.2f} +/- {e:0.2f}\")\n</pre> # Fully optimized using Numba's vectorize @jit(nopython=True) def gaussian_with_background_numba(x, A, x0, sigma, B, C, C_cutoff):     res = A * np.exp(-((x - x0) ** 2) / (2 * sigma ** 2)) + B * x + C     res[x {p:0.2f} +/- {e:0.2f}\") <pre>833 \u00b5s \u00b1 103 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\nFitted Parameters (Numba):\n5.0 -&gt; 5.24 +/- inf\n5.0 -&gt; 5.05 +/- inf\n1.0 -&gt; 0.93 +/- inf\n0.5 -&gt; 0.45 +/- inf\n2.0 -&gt; 2.37 +/- inf\n1.0 -&gt; 2.00 +/- inf\n</pre> <pre>&lt;magic-timeit&gt;:1: OptimizeWarning: Covariance of the parameters could not be estimated\n&lt;ipython-input-141-e3e4f175d99e&gt;:11: OptimizeWarning: Covariance of the parameters could not be estimated\n  popt_numba, pcov_numba = opt.curve_fit(gaussian_with_background_numba, x, y_noisy, p0=guess)\n</pre> In\u00a0[\u00a0]: Copied! <pre>samples = bootstrap(gaussian_with_background_numba, x, y_noisy, p0 = guess, nboot = 1000)\n</pre> samples = bootstrap(gaussian_with_background_numba, x, y_noisy, p0 = guess, nboot = 1000) <pre>&lt;ipython-input-132-72b055b281b7&gt;:32: OptimizeWarning: Covariance of the parameters could not be estimated\n  p, _ = sp.optimize.curve_fit(func, x_samp, y_samp, p0=p0, maxfev = 10000 )\n</pre> In\u00a0[\u00a0]: Copied! <pre>samples.shape\n</pre> samples.shape Out[\u00a0]: <pre>(1000, 6)</pre> In\u00a0[\u00a0]: Copied! <pre>param_names = [\"A\", \"x0\", \"sigma\", \"B\", \"C\", \"C_cutoff\"]\nfig, axs = plt.subplots(2,samples.shape[1] // 2, figsize = (12,4))\nfor i, ax in enumerate(axs.ravel()):\n    quants = np.quantile(samples[:,i], q = [0.16, 0.5, 0.84])\n    binning = np.linspace(0.8*quants[0], 1.2*quants[-1], 20)\n    ax.hist(samples[:,i], bins = binning)\n    # ax.hist(samples[:,i], bins = 30)\n    ax.axvline(p_true[i], label = \"True\", color = \"r\", ls = \"-\")\n    # axs[i].axvline(popt[i], label = \"Curve_fit\", color = \"C4\", ls = \"-.\")\n    # axs[i].axvline(popt[i] + perr[i], color = \"C4\", ls = \"-.\")\n    # axs[i].axvline(popt[i] - perr[i], color = \"C4\", ls = \"-.\")\n\n\n    ax.set_xlabel(param_names[i])\n    ax.grid()\n    if_label = \"BootStrap\"\n\n    skew = sp.stats.skew(samples[:,i])\n    kur = sp.stats.kurtosis(samples[:,i])\n    ax.set_title(f\"Skewness = {skew:0.2f}, Kurtosis = {kur:0.2f}\")\n\n    for q in quants:\n        ax.axvline(q, color = \"k\", ls = \"--\", label = if_label)\n        if_label = None\naxs[0,0].legend();\nfig.tight_layout()\n</pre> param_names = [\"A\", \"x0\", \"sigma\", \"B\", \"C\", \"C_cutoff\"] fig, axs = plt.subplots(2,samples.shape[1] // 2, figsize = (12,4)) for i, ax in enumerate(axs.ravel()):     quants = np.quantile(samples[:,i], q = [0.16, 0.5, 0.84])     binning = np.linspace(0.8*quants[0], 1.2*quants[-1], 20)     ax.hist(samples[:,i], bins = binning)     # ax.hist(samples[:,i], bins = 30)     ax.axvline(p_true[i], label = \"True\", color = \"r\", ls = \"-\")     # axs[i].axvline(popt[i], label = \"Curve_fit\", color = \"C4\", ls = \"-.\")     # axs[i].axvline(popt[i] + perr[i], color = \"C4\", ls = \"-.\")     # axs[i].axvline(popt[i] - perr[i], color = \"C4\", ls = \"-.\")       ax.set_xlabel(param_names[i])     ax.grid()     if_label = \"BootStrap\"      skew = sp.stats.skew(samples[:,i])     kur = sp.stats.kurtosis(samples[:,i])     ax.set_title(f\"Skewness = {skew:0.2f}, Kurtosis = {kur:0.2f}\")      for q in quants:         ax.axvline(q, color = \"k\", ls = \"--\", label = if_label)         if_label = None axs[0,0].legend(); fig.tight_layout() <pre>&lt;ipython-input-144-15ac1bd7c2b3&gt;:18: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n  skew = sp.stats.skew(samples[:,i])\n&lt;ipython-input-144-15ac1bd7c2b3&gt;:19: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n  kur = sp.stats.kurtosis(samples[:,i])\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>Minimizers often rely on the gradient of a function to find the minimum. Some methods, like stochastic gradient descent, explicitly require gradient information, while others determine it numerically using techniques such as Automatic Differentiation.</p> <p>A key requirement for many minimization algorithms is that the function being fitted must be smooth and continuous. In other words, the function should not have abrupt jumps or discontinuities, and its first derivative should not change abruptly. Mathematically, this means that at any point ( x ), the left-hand and right-hand derivatives should be equal:</p> <p>$$ \\lim_{x \\to x_0^-} \\frac{df}{dx} = \\lim_{x \\to x_0^+} \\frac{df}{dx} $$</p> <p>Now, consider the given function:</p> <p>$$ f(x) = \\begin{cases} A e^{-\\frac{(x - x_0)^2}{2\\sigma^2}} + Bx + C, &amp; \\text{if } x &gt; C_{\\text{cutoff}} \\\\ 0, &amp; \\text{otherwise} \\end{cases} $$</p> <p>At $ x = C_{\\text{cutoff}} $, the function is discontinuous because the value of $ f(x) $ jumps from $ 0 $ to a nonzero value. Moreover, the derivative exhibits a discontinuity at this point:</p> <p>$$ \\lim_{x \\to C_{\\text{cutoff}}^-} \\frac{\\partial f}{\\partial x} \\neq \\lim_{x \\to C_{\\text{cutoff}}^+} \\frac{\\partial f}{\\partial x} $$</p> <p>Because of this discontinuity, minimization methods that rely on derivatives will struggle to estimate parameter uncertainties reliably. Many standard error estimation techniques, such as those based on the Hessian matrix (which requires second derivatives), assume smooth behavior and will fail or yield incorrect uncertainty estimates.</p> <p>Can we approximate this \"if statement\" with a function? There are many that can be used, but let's consider a sigmoid function, with a constant offset ($c$) -- which will approximate $C_{cutoff}$-- and a weight parameter $w$ to control the transition between the two states:</p> <p>$$ \\text{sigmoid}(x, c, w) = \\frac{1}{ 1 + e ^{- (x - c) / w}} $$</p> In\u00a0[\u00a0]: Copied! <pre>@jit(nopython=True)\ndef sigmoid(x, c =0, w =1):\n    '''\n    Sigmoid function with an additional offset and weight\n    '''\n    return 1/(1+np.exp(-(x - c) / w))\n</pre> @jit(nopython=True) def sigmoid(x, c =0, w =1):     '''     Sigmoid function with an additional offset and weight     '''     return 1/(1+np.exp(-(x - c) / w)) <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n&lt;ipython-input-1-148351852a5d&gt; in &lt;cell line: 0&gt;()\n----&gt; 1 @jit(nopython=True)\n      2 def sigmoid(x, c =0, w =1):\n      3     '''\n      4     Sigmoid function with an additional offset and weight\n      5     '''\n\nNameError: name 'jit' is not defined</pre> In\u00a0[\u00a0]: Copied! <pre>plt.plot(x, sigmoid(x))\nplt.plot(x, sigmoid(x,2, 1e-1))\n</pre>  plt.plot(x, sigmoid(x)) plt.plot(x, sigmoid(x,2, 1e-1)) Out[\u00a0]: <pre>[&lt;matplotlib.lines.Line2D at 0x7b1bde36a810&gt;]</pre> In\u00a0[\u00a0]: Copied! <pre>@jit(nopython=True)\ndef gaussian_with_background_numba_sigmoid(x, A, x0, sigma, B, C, C_cutoff):\n    res = A * np.exp(-((x - x0) ** 2) / (2 * sigma ** 2)) + B * x + C\n\n    res *= sigmoid(x, C_cutoff, w = 1e-1)\n    return res\n\ngaussian_with_background_numba_sigmoid(1,1,1,1,1,1,1)\n</pre> @jit(nopython=True) def gaussian_with_background_numba_sigmoid(x, A, x0, sigma, B, C, C_cutoff):     res = A * np.exp(-((x - x0) ** 2) / (2 * sigma ** 2)) + B * x + C      res *= sigmoid(x, C_cutoff, w = 1e-1)     return res  gaussian_with_background_numba_sigmoid(1,1,1,1,1,1,1) Out[\u00a0]: <pre>1.5</pre> In\u00a0[\u00a0]: Copied! <pre>samples = bootstrap(gaussian_with_background_numba_sigmoid, x, y_noisy, p0 = guess, nboot = 1000)\n</pre> samples = bootstrap(gaussian_with_background_numba_sigmoid, x, y_noisy, p0 = guess, nboot = 1000) <pre>&lt;ipython-input-132-72b055b281b7&gt;:32: OptimizeWarning: Covariance of the parameters could not be estimated\n  p, _ = sp.optimize.curve_fit(func, x_samp, y_samp, p0=p0, maxfev = 10000 )\n</pre> In\u00a0[\u00a0]: Copied! <pre>param_names = [\"A\", \"x0\", \"sigma\", \"B\", \"C\", \"C_cutoff\"]\nfig, axs = plt.subplots(2,samples.shape[1] // 2, figsize = (12,4))\nfor i, ax in enumerate(axs.ravel()):\n    quants = np.quantile(samples[:,i], q = [0.16, 0.5, 0.84])\n    binning = np.linspace(0.8*quants[0], 1.2*quants[-1], 20)\n    ax.hist(samples[:,i], bins = binning)\n    # ax.hist(samples[:,i], bins = 30)\n    ax.axvline(p_true[i], label = \"True\", color = \"r\", ls = \"-\")\n    # axs[i].axvline(popt[i], label = \"Curve_fit\", color = \"C4\", ls = \"-.\")\n    # axs[i].axvline(popt[i] + perr[i], color = \"C4\", ls = \"-.\")\n    # axs[i].axvline(popt[i] - perr[i], color = \"C4\", ls = \"-.\")\n\n\n    ax.set_xlabel(param_names[i])\n    ax.grid()\n    if_label = \"BootStrap\"\n\n    skew = sp.stats.skew(samples[:,i])\n    kur = sp.stats.kurtosis(samples[:,i])\n    ax.set_title(f\"Skewness = {skew:0.2f}, Kurtosis = {kur:0.2f}\")\n\n    for q in quants:\n        ax.axvline(q, color = \"k\", ls = \"--\", label = if_label)\n        if_label = None\naxs[0,0].legend();\nfig.tight_layout()\n</pre> param_names = [\"A\", \"x0\", \"sigma\", \"B\", \"C\", \"C_cutoff\"] fig, axs = plt.subplots(2,samples.shape[1] // 2, figsize = (12,4)) for i, ax in enumerate(axs.ravel()):     quants = np.quantile(samples[:,i], q = [0.16, 0.5, 0.84])     binning = np.linspace(0.8*quants[0], 1.2*quants[-1], 20)     ax.hist(samples[:,i], bins = binning)     # ax.hist(samples[:,i], bins = 30)     ax.axvline(p_true[i], label = \"True\", color = \"r\", ls = \"-\")     # axs[i].axvline(popt[i], label = \"Curve_fit\", color = \"C4\", ls = \"-.\")     # axs[i].axvline(popt[i] + perr[i], color = \"C4\", ls = \"-.\")     # axs[i].axvline(popt[i] - perr[i], color = \"C4\", ls = \"-.\")       ax.set_xlabel(param_names[i])     ax.grid()     if_label = \"BootStrap\"      skew = sp.stats.skew(samples[:,i])     kur = sp.stats.kurtosis(samples[:,i])     ax.set_title(f\"Skewness = {skew:0.2f}, Kurtosis = {kur:0.2f}\")      for q in quants:         ax.axvline(q, color = \"k\", ls = \"--\", label = if_label)         if_label = None axs[0,0].legend(); fig.tight_layout() In\u00a0[\u00a0]: Copied! <pre>p_best_fit = []\nfor i in range(len(param_names)):\n    p = np.quantile(samples[:,i], q=[0.5, 0.05, 0.95] )\n    p_best_fit.append(p[0])\n    print (f\"{param_names[i]} (Truth: {p_true[i]}) \\t = {p[0]:0.2f} [{p[1]:0.2f}, {p[2]:0.2f}] \\n\\t-&gt; (Curve Fit) {popt[i]:0.2f} +/- {perr[i] : 0.2f}\")\n</pre> p_best_fit = [] for i in range(len(param_names)):     p = np.quantile(samples[:,i], q=[0.5, 0.05, 0.95] )     p_best_fit.append(p[0])     print (f\"{param_names[i]} (Truth: {p_true[i]}) \\t = {p[0]:0.2f} [{p[1]:0.2f}, {p[2]:0.2f}] \\n\\t-&gt; (Curve Fit) {popt[i]:0.2f} +/- {perr[i] : 0.2f}\") <pre>A (Truth: 5.0) \t = 5.44 [4.81, 6.07] \n\t-&gt; (Curve Fit) 5.24 +/-  inf\nx0 (Truth: 5.0) \t = 5.03 [4.86, 5.17] \n\t-&gt; (Curve Fit) 5.05 +/-  inf\nsigma (Truth: 1.0) \t = 0.98 [-0.80, 1.16] \n\t-&gt; (Curve Fit) 0.93 +/-  inf\nB (Truth: 0.5) \t = 0.50 [0.39, 0.58] \n\t-&gt; (Curve Fit) 0.45 +/-  inf\nC (Truth: 2.0) \t = 1.91 [1.21, 2.82] \n\t-&gt; (Curve Fit) 2.37 +/-  inf\nC_cutoff (Truth: 1.0) \t = 0.98 [0.66, 1.70] \n\t-&gt; (Curve Fit) 2.00 +/-  inf\n</pre> In\u00a0[\u00a0]: Copied! <pre>for i in range(500):\n    rnd_indx = np.random.randint(0, samples.shape[0])\n    plt.plot(x, gaussian_with_background_numba_sigmoid(x, *samples[i,:]), color = \"gray\", alpha = 0.1)\n\n\nplt.plot(x, y_true, label = \"True\")\nplt.plot(x, y_noisy, label = \"Observed\")\nplt.plot(x, gaussian_with_background_numba_sigmoid(x, *p_best_fit), label = \"Best-Fit\")\n\nplt.xlabel(\"Distance from origin [mm]\")\nplt.ylabel(\"Amplitude [AU]\")\nplt.grid()\nplt.legend();\n</pre>    for i in range(500):     rnd_indx = np.random.randint(0, samples.shape[0])     plt.plot(x, gaussian_with_background_numba_sigmoid(x, *samples[i,:]), color = \"gray\", alpha = 0.1)   plt.plot(x, y_true, label = \"True\") plt.plot(x, y_noisy, label = \"Observed\") plt.plot(x, gaussian_with_background_numba_sigmoid(x, *p_best_fit), label = \"Best-Fit\")  plt.xlabel(\"Distance from origin [mm]\") plt.ylabel(\"Amplitude [AU]\") plt.grid() plt.legend();  In\u00a0[\u00a0]: Copied! <pre>samples.shape\n</pre> samples.shape Out[\u00a0]: <pre>(1000, 6)</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Python/ScientificPythonProgramming/#0-scientific-computing","title":"0. Scientific Computing\u00b6","text":"<p>Scientific computing is an interdisciplinary field that encompasses the development and application of computational techniques and algorithms to solve complex problems in science, engineering, and other disciplines.</p> <p>At its core, scientific computing leverages computational tools and methodologies to model, simulate, analyze, and visualize phenomena that are often too complex or impractical to study through traditional analytical methods alone.</p> <p>By combining principles from mathematics, computer science, and domain-specific areas of study, scientific computing enables researchers and practitioners to explore and understand natural and artificial systems, predict their behavior, and make informed decisions based on quantitative analysis and computational experiments. From simulating physical processes and designing new materials to analyzing biological data and optimizing engineering systems, scientific computing plays a crucial role in advancing knowledge, driving innovation, and solving real-world problems across diverse fields of study.</p> <p>Scientific Computing differs from other forms of computing due to the different requirements that are needed. Scientific Computing often requires large complicated calculations to be performed on large datasets with some degree of high performance. Some of the key things we need to consider are:</p> <ul> <li>Size of our dataset</li> <li>Numerical accuracy required</li> <li>Complex mathematical computations</li> <li>Complex data visualizations</li> </ul> <p>Scientific computing often requires large computer clusters to perform calculations (e.g., simulating climate and weather patterns^ice) or even just to manipulate large datasets (e.g., CHIME^chime telescope takes O(TB/s))</p> <p>Today we're going to cover the following topics:</p> <ul> <li>Numerical analysis with NumPy</li> <li>Methods to improve performance</li> <li>Scientific Programming with SciPy</li> </ul>"},{"location":"Python/ScientificPythonProgramming/#1-what-is-numpy","title":"1. What is NumPy?\u00b6","text":"<p>NumPy[^note] (Numerical Python) is one of the fundamental packages used in Scientific Python. It is a highly optimized library for working with multi-dimensional arrays and matrices. It also contains a collection of mathematical functions that can operate on these arrays and matrices while maintaining a high level of performance.</p> <p>NumPy achieves its level of performance by relying on highly optimized lower-level languages like C and Fortran! Because NumPy's low-level backend, calls to NumPy happen outside of the Python interpreter, circumventing the Python Global Interpreter Lock (GIL)[^gil]. This makes NumPy an excellent framework when running concurrent code.</p>"},{"location":"Python/ScientificPythonProgramming/#11-arrays-in-numpy","title":"1.1 Arrays in NumPy\u00b6","text":"<p>One of the most fundamental data types in NumPy is the powerful array. An array can be defined as follows:</p> <pre>import numpy as np\nmy_list = [1,2,3,4]\nmy_array = np.array(my_list)\nmy_second_array = np.array([5,6,7,8])\n</pre> <p>On line 1, we <code>import</code> NumPy as <code>np</code>. <code>np</code> is the standard name that folks use when importing NumPy. We can define an array by calling <code>np.array()</code>. This function expects a list. On line 3, we pass the previously defined <code>my_list</code> to <code>np.array()</code> to create an array. On line 4, we skip the allocation of the list by directly passing the list to <code>np.array()</code>.</p> <p> Question: Create an array with the numbers between 0-10 </p>"},{"location":"Python/ScientificPythonProgramming/#12-creating-arrays-from-constructor-functions","title":"1.2 Creating arrays from constructor functions\u00b6","text":"<p>It can be pretty cumbersome to always write out each element in the list that we want to convert into an array. NumPy has many pre-made functions to create NumPy arrays:</p> <ul> <li><code>np.arange</code> works similarly to range, allowing us to get evenly spaced values within an interval. By default, the lower value will be 0.</li> <li><code>np.ones</code> creates an array with a predefined shape with all the values equal to 1.</li> <li><code>np.zeros</code> creates an array with a predefined shape with all the values equal to 0.</li> <li><code>np.empty</code> creates an array with a predefined shape without initializing the values.</li> </ul> <p> Question: Create an array of only even numbers between 0 and 100 (hint use the help() function to examine <code>np.arange</code>) </p>"},{"location":"Python/ScientificPythonProgramming/#13-math-operations-with-numpy","title":"1.3 Math operations with NumPy\u00b6","text":"<p>Working with NumPy arrays allows us to easily apply operations to the entirety of the array:</p> <ul> <li>Arithmetic: <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, <code>**</code></li> <li>Common mathematical operations: <code>np.sin</code>, <code>np.cos</code>, <code>np.tan</code>, <code>np.sqrt</code>, <code>np.power</code>, <code>np.abs</code>, <code>np.log</code>, <code>np.log10</code>, etc.</li> <li>Matrix operations: <code>np.transpose</code>, <code>np.dot</code>, <code>np.matmul()</code> (matrix multiplication), <code>np.linalg.inv</code> (matrix inversion), <code>np.linalg.det()</code> (matrix determinant), etc.</li> <li>Statistical operations: <code>np.mean</code>, <code>np.median</code>, <code>np.max</code>/<code>np.min</code>, <code>np.quantile</code>, <code>np.percentile</code>, etc.</li> <li>Random numbers: <code>np.random.rand</code> (uniform), <code>np.random.randn</code> (normal/gaussian), <code>np.random.randint</code> (random integers), <code>np.random.choice</code> (random sampling)</li> </ul> <p>While a lot of this functionality exists in the Python standard library (<code>math</code>), NumPy offers highly optimized versions of these functions.</p> <p> Let's look at how <code>math</code> compares to <code>numpy</code> </p>"},{"location":"Python/ScientificPythonProgramming/#14-measuring-runtime-performance-using-timeit","title":"1.4 Measuring runtime performance using <code>timeit</code>\u00b6","text":"<p>Python provides the <code>timeit</code> module, which enables the measurement of execution time for specific code segments. This module offers a simple yet effective way to evaluate the performance of individual code snippets or functions:</p> <ul> <li><code>%%time</code> will time the contents of an entire cell with a single run</li> <li><code>%%timeit</code> will time the contents of an entire cell, averaging over multiple runs of that cell</li> <li><code>%time</code> will time a single line with a single run</li> <li><code>%timeit</code> will time a single line, averaging over multiple runs of that cell</li> </ul> <p><code>timeit</code> will also suspend some features, such as the garbage collector, to get an accurate measurement of the code's runtime.</p> <p> Let's use <code>%%timeit</code> to time how long <code>math.sin</code> will take on the entire <code>x_data</code> array. How does <code>np.sin</code> compare?</p>"},{"location":"Python/ScientificPythonProgramming/#15-masking-numpy-arrays","title":"1.5 Masking NumPy arrays\u00b6","text":"<p>Since we can apply operations to the entire NumPy array, we can combine logical operations and indexing to select data. Consider the following:</p>"},{"location":"Python/ScientificPythonProgramming/#16-vectorization","title":"1.6 Vectorization\u00b6","text":"<p>NumPy has a built-in feature called vectorization. <code>np.vectorize</code> allows us to transform functions that nominally operate on single scales to operate on entire NumPy arrays. Let's look at the following example:</p>"},{"location":"Python/ScientificPythonProgramming/#2-optimizing-python","title":"2. Optimizing Python\u00b6","text":"<p>Let's imagine a flat square tile outside on a rainy day. For simplicity, let's call the width and length of the square to be 2. Now, let's say we draw a circle within the square such that the radius of the circle is 1 (i.e., its diameter is equal to 2, or the length of the square). The areas of the two shapes are:</p> <p>$$ A_{square} = l \\times w = 2 \\times 2 = 4 $$ $$ A_{circle} = \\pi r^2 = \\pi (1)^2 = \\pi $$</p> <p>If we were to count the number of raindrops that fall in the square and in the circle, we can use the ratio of the two to estimate $\\pi$:</p> <p>$$ \\frac{N_{circle}}{N_{square}} = \\frac{\\pi}{4}, $$</p> <p>or</p> <p>$$ \\pi = 4 \\times \\frac{N_{circle}}{N_{square}}. $$</p> <p>This holds true as the number of raindrops increases to infinity. In general, we can say:</p> <p>$$ \\pi \\approx 4 \\times \\frac{N_{circle}}{N_{tot}}. $$</p> <p>We'll aim to reproduce the above equation using a Monte Carlo simulation.</p>"},{"location":"Python/ScientificPythonProgramming/#21-monte-carlo-code","title":"2.1 Monte Carlo Code\u00b6","text":""},{"location":"Python/ScientificPythonProgramming/#22-working-with-numba","title":"2.2 Working with Numba\u00b6","text":"<p>Numba is a just-in-time (JIT) compiler that allows us to compile Python code to machine code at runtime. This optimization provides a significant boost in speed.</p> <p>We can use Numba by decorating functions with the <code>@jit</code> decorator.</p>"},{"location":"Python/ScientificPythonProgramming/#23-running-code-in-parallel-with-numba","title":"2.3 Running code in Parallel with Numba\u00b6","text":"<p>We can pass optional arguments to the <code>@jit</code> decorator to further improve the runtime performance. We might want to utilize our entire CPU when running the code. This is known as parallelism or parallel programming. Simply adding <code>parallel=True</code> to the decorator unlocks that ability:</p>"},{"location":"Python/ScientificPythonProgramming/#3-working-with-scipy","title":"3. Working with SciPy\u00b6","text":"<p>SciPy, short for Scientific Python, is a powerful open-source library built on top of the Python programming language, designed to facilitate scientific and technical computing. SciPy is part of the NumPy ecosystem, allowing for SciPy to be easily incorperated into a NumPy-based analysis pipeline. SciPy offers a vast array of tools and functions to handle a wide range of mathematical and scientific tasks commonly encountered in physics studies and research.</p> <p>Some functionality includes:</p> <ul> <li>Solving differential equations (e.g. <code>scipy.integrate.solve_ivp</code>, <code>scipy.integrate.ode</code>)</li> <li>Performing numerical integration (e.g. <code>scipy.integrate.simps</code>, <code>scipy.integrate.quad</code>)</li> <li>Conducting statistical analysis (e.g. <code>scipy.stats</code>)</li> <li>Minimization and model fitting (e.g. <code>scipy.optimize.minimize</code>, <code>scipy.optimize.curve_fit</code>)</li> </ul> <p>SciPy provides efficient and user-friendly solutions that streamline complex computational tasks. Its extensive functionality, combined with the simplicity and flexibility of Python syntax, makes SciPy an indispensable tool for tackling various challenges in theoretical and experimental physics. From simulating physical systems to analyzing experimental data and visualizing results.</p>"},{"location":"Python/ScientificPythonProgramming/#31-minimizing-functions-using-minimize","title":"3.1 Minimizing functions using minimize\u00b6","text":"<p>We can use SciPy to numerically minimize a function using <code>scipy.optimize.minimize</code>. Let's see an example of this. Let's define a simple potential energy of a mass attached to a spring with linear and constant offset components: $$ U(x) = \\frac{1}{2} k x^2 + c x + d $$</p> <p>Here $k$ is our spring constant, $c$ and $d$ are coefficients for a linear and constant component. We can analytically solve this as: $$ \\frac{dU}{dx} = 0 \\Rightarrow x_{min} = \\frac{-c}{k} $$</p> <p>Let's see how to use <code>scipy.optimize.minimize</code> to numerically find $x_{\\text{min}}$.</p>"},{"location":"Python/ScientificPythonProgramming/#32-minimizing-n-dimensional-functions","title":"3.2 Minimizing n-dimensional functions\u00b6","text":"<p>We can also easily minimize n-dimensional functions using <code>scipy.optimize.minimize</code>. The number of dimensions, or parameters that a function takes, is assumed from the shape of the initial guess. We can access the parameters at which the minimum occurs using:</p> <pre>results = sp.optimize.minimize(func, x0)\nx_min = results.x\n</pre> <p>Where <code>x_min</code> will have the same shape as <code>x0</code>.</p> <p>Let's try to minimize a 2D function known as the Rosenbrock function, a function that is commonly used as a benchmark for minimization algorithms.</p> <p>$$ f(x,y) = (a - x)^2 + b(y - x^2)^2 $$</p> <p>With typically <code>a = 1</code>, <code>b = 100</code>.</p>"},{"location":"Python/ScientificPythonProgramming/#34-fitting-a-model-to-data","title":"3.4 Fitting a Model to Data\u00b6","text":"<p><code>scipy.optimize.curve_fit</code> provides an easy-to-use function for fitting a model to a dataset.</p> <p><code>scipy.optimize.curve_fit</code> will provide a least-squares minimization of a function such that: $$ \\text{minimize} \\sum_{i} (y_i - f(x_i, \\theta))^2 $$ Where $y_i$ and $x_i$ are the measurements, $f(x, \\theta)$ is the model, and $\\theta$ is the model parameters.</p> <p>If errors are provided to <code>scipy.optimize.curve_fit</code> (e.g., <code>curve_fit(f, x, y, sigma=y_err)</code>), then a $\\chi^2$ minimization is performed: $$ \\text{minimize} \\sum_{i} \\frac{(y_i - f(x_i, \\theta))^2}{\\Delta y_i^2} $$</p>"},{"location":"Python/ScientificPythonProgramming/#35-how-can-we-be-confident-in-our-best-fit-parameters","title":"3.5 How can we be confident in our best-fit parameters?\u00b6","text":"<p>It can often be difficult to validate parameters. Sometimes we might not have a good grasp of the measured uncertainties, making the uncertainty on the fit parameters also questionable.</p> <p>One method we can use to help with this is called Bootstrapping. Bootstrapping can be done as follows:</p> <ol> <li>Start with a sample of data with n observations.</li> <li>Resample the data to draw n random samples from the dataset. Here we can repeat samples from the original dataset (i.e., if our original dataset is [1,2,3,4], our resampled dataset could be [1,1,3,2]).</li> <li>Fit the model to the resampled datasets and store parameters of interest (e.g., best-fit parameters).</li> <li>Repeat steps 2 and 3 a large number of times (e.g., 1000).</li> <li>Use the distribution of the parameters obtained from the resampled datasets to estimate properties of these distributions.</li> </ol> <p>Since we are resampling the original dataset, the bootstrapped distributions will represent the uncertainties and limitations of our dataset. Bootstrapping is an excellent way to test for biases, estimate uncertainties and confidence intervals, and extract properties for otherwise difficult to sample distributions.</p>"},{"location":"Python/ScientificPythonProgramming/#36-understanding-skewness-and-kurtosis","title":"3.6 Understanding Skewness and Kurtosis\u00b6","text":"<p>A lot of the time, we assume that data is normally distributed. Whether or not the data is actually Gaussian can impact how we use the data and whether or not a specific test or algorithm is valid for use!</p> <p>Skewness is defined as:</p> <p>$$ g_1 = \\frac{m_3}{m_2^{\\frac{3}{2}}} $$</p> <p>Where: $$ m_i = \\frac{1}{N} \\sum_{j=1}^{N} ( x[j] - \\bar{x})^i $$</p> <p>We can think of this as a measurement of the orientation and separation of the mean, median, and mode of a distribution: </p> <p>As a rule of thumb:</p> <ul> <li>-0.5 &lt; Skewness &lt; 0.5: Approximately symmetric.</li> <li>-1 &lt; Skewness &lt; -0.5 or 0.5 &lt; Skewness &lt; 1: Moderately skewed.</li> <li>Skewness &lt; -1 or Skewness &gt; 1: Highly skewed.</li> <li>Skewness &gt; 0: Right skewed.</li> <li>Skewness &lt; 0: Left skewed.</li> </ul> <p>We can see that the Skewness of the $\\tau$ is large and positive, suggesting that it is highly skewed to the right.</p> <p>Kurtosis tells us how much of the distribution is in the tails, compared to a normal distribution. This is defined as: $$ \\text{kurtosis} = \\frac{n(n+1)}{(n-1)(n-2)(n-3)} \\sum_{i=1}^n \\left( \\frac{x_i - \\mu}{\\sigma} \\right)^4 - \\frac{3(n-1)^2}{(n-2)(n-3)} $$</p> <p>It is common to report the \"excess kurtosis\", this is \"kurtosis - 3\" (with 3 being the kurtosis of a normal distribution). In <code>scipy.stats.kurtosis</code>, the default is to return the excess kurtosis, to override this pass <code>fisher=False</code>.</p> <p></p> <p>As a rule of thumb:</p> <ul> <li>Excess Kurtosis = 0 (Kurtosis = 3): The distribution has a normal level of tail weight.</li> <li>Excess Kurtosis &gt; 0 (Kurtosis &gt; 3): The distribution has heavier tails and a sharper peak (leptokurtic).</li> <li>Excess Kurtosis &lt; 0 (Kurtosis &lt; 3): The distribution has lighter tails and a flatter peak (platykurtic).</li> </ul> <p>We can see all distributions show heavier tails, suggesting a large number of outliers.</p>"},{"location":"Python/ScientificPythonProgramming/#37-reporting-fit-values","title":"3.7 Reporting Fit Values\u00b6","text":"<p>With the information about the Skewness and Kurtosis, we know that best-fit parameters aren't guassian. Symmetric error bars ($\\hat{\\theta} \\pm \\Delta\\theta$) don't accurately represent the uncertainty. Additional we know that some of the parameters have large tails with signficance outliers. It might be more approiate to report 90% containment rather than a $1\\sigma$ uncertainty region.</p> <p><code>numpy.quantile</code> is useful for measuring the quantiles of a dataset.</p>"},{"location":"Python/ScientificPythonProgramming/#40-assignment","title":"4.0 Assignment\u00b6","text":"<p>In this assignment, you will optimize a data fitting problem using NumPy and Numba. You are given a dataset generated from a Gaussian peak with a linear background, but with an additional cutoff in x, meaning any values where $ x &lt; C_{\\text{cutoff}} $ are set to zero.</p> <p>Your task is to:  Optimize the function which acts as a model for our data .</p> <p>At the end, you will use SciPy\u2019s <code>curve_fit</code> to fit the model parameters, including the cutoff.</p>"},{"location":"Python/ScientificPythonProgramming/#problem-statement","title":"Problem Statement\u00b6","text":"<p>A Gaussian peak with background is given by:</p> <p>$$ f(x) = \\begin{cases} A e^{-\\frac{(x - x_0)^2}{2\\sigma^2}} + Bx + C, &amp; \\text{if } x &gt; C_{\\text{cutoff}} \\\\ 0, &amp; \\text{otherwise} \\end{cases} $$</p> <p>where:</p> <ul> <li>$ A $ = Amplitude of the Gaussian peak</li> <li>$ x_0 $ = Center of the Gaussian peak</li> <li>$ \\sigma $ = Width of the Gaussian peak</li> <li>$ B $ = Slope of the linear background</li> <li>$ C $ = Offset of the linear background</li> <li>$ C_{\\text{cutoff}} $ = Cutoff point in ( x ) (to be fitted)</li> </ul> <p>The dataset contains noise, and your goal is to recover the original parameters by fitting this function to the data.</p> <p>The data is hidden, please only consult if you need additional hints.</p>"},{"location":"Python/ScientificPythonProgramming/#hints","title":"Hints\u00b6","text":"<ol> <li>Use <code>line_profile</code> to identify bottlenecks:</li> </ol> <pre><code>  %lprun -f function_name function_name(x, y, z)\n</code></pre> <ol> <li>Use <code>%time</code> and <code>%timeit</code> to measure performance</li> <li>Consider how evaluating logic within loops can effect vectorized (array-wise) operations.</li> <li>Consider pre-compiling!</li> </ol>"},{"location":"Python/ScientificPythonProgramming/#41-bonus-questions","title":"4.1 Bonus Questions:\u00b6","text":"<p>How can we report the uncertainties?</p>"},{"location":"Python/Threading_And_Multiprocessing/","title":"Threading And Multiprocessing","text":"In\u00a0[1]: Copied! <pre>import threading\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport multiprocessing as mp\n</pre> import threading import time import numpy as np import matplotlib.pyplot as plt import multiprocessing as mp <p>Let's define some expensive funtion with some randomness to it:</p> In\u00a0[2]: Copied! <pre>def expensive_function(n):\n    counter = 0\n    for i in range(n):\n        time.sleep(0.1)\n        counter += i\n    print (f\"sum of {n} = {counter}\")\n    return counter\n</pre> def expensive_function(n):     counter = 0     for i in range(n):         time.sleep(0.1)         counter += i     print (f\"sum of {n} = {counter}\")     return counter <p>We can span a thread using</p> <pre>t = threading.Thread(target = function_to_run, args = [arguments])\n</pre> <p>Let's create 16 threads:</p> In\u00a0[12]: Copied! <pre>n_threads = 16\nthreads = []\nfor i in range(n_threads):\n    r = np.random.randint(1,50)\n    threads.append( threading.Thread(target = expensive_function, args = [r]))\n</pre> n_threads = 16 threads = [] for i in range(n_threads):     r = np.random.randint(1,50)     threads.append( threading.Thread(target = expensive_function, args = [r]))  <p>The threads won't start running until we call:</p> <pre><code>t.start()\n</code></pre> <p>This will make the function run the a seperate thread. We can wait for a thread to finish by calling:</p> <pre><code>t.join()\n</code></pre> In\u00a0[13]: Copied! <pre>for t in threads:\n    t.start()\n\ntime.sleep(1)\nprint (\"This code will run while threads are running\")\n\n\nfor t in threads:\n    t.join()\n\nprint (\"This code will wait until the threads are finished\")\n</pre> for t in threads:     t.start()  time.sleep(1) print (\"This code will run while threads are running\")   for t in threads:     t.join()  print (\"This code will wait until the threads are finished\") <pre>sum of 6 = 15\nsum of 8 = 28\nThis code will run while threads are running\nsum of 10 = 45\nsum of 16 = 120\nsum of 18 = 153\nsum of 20 = 190\nsum of 22 = 231\nsum of 22 = 231\nsum of 25 = 300\nsum of 28 = 378\nsum of 29 = 406\nsum of 32 = 496\nsum of 35 = 595\nsum of 37 = 666\nsum of 41 = 820\nsum of 49 = 1176\nThis code will wait until the threads are finished\n</pre> <p>Let's perform vector addition using threading. We'll use <code>global</code> to ensure we're working on the global <code>a</code>, <code>b</code>, and <code>c</code> rather than a copy local to the function.</p> In\u00a0[59]: Copied! <pre>n = 200\na = [1] * n\nb = [2] * n\nc = [0] * n\n\ndef add_vectors(n):\n    global a, b, c\n    for i in range(n):\n        c[i] = a[i] + b[i]\n</pre> n = 200 a = [1] * n b = [2] * n c = [0] * n  def add_vectors(n):     global a, b, c     for i in range(n):         c[i] = a[i] + b[i]   In\u00a0[60]: Copied! <pre>%%timeit\nn = 200\na = [1] * n\nb = [2] * n\nc = [0] * n\n\nadd_vectors(n)\n</pre> %%timeit n = 200 a = [1] * n b = [2] * n c = [0] * n  add_vectors(n)  <pre>8.15 \u00b5s \u00b1 19.8 ns per loop (mean \u00b1 std. dev. of 7 runs, 100,000 loops each)\n</pre> In\u00a0[61]: Copied! <pre>print (c[:5])\n</pre> print (c[:5]) <pre>[3, 3, 3, 3, 3]\n</pre> <p>Let's define a function that will only apply the addition to a single element of the array:</p> In\u00a0[62]: Copied! <pre>n = 200\na = [1] * n\nb = [2] * n\nc = [0] * n\n\ndef add_vectors_element(i):\n    global a, b, c\n    c[i] = a[i] + b[i]\n\nadd_vectors_element(0)\nprint (c[:5])\n</pre> n = 200 a = [1] * n b = [2] * n c = [0] * n  def add_vectors_element(i):     global a, b, c     c[i] = a[i] + b[i]  add_vectors_element(0) print (c[:5])  <pre>[3, 0, 0, 0, 0]\n</pre> <p>Let's spawn <code>n</code> threads and allow each thread to operate on a single element</p> In\u00a0[19]: Copied! <pre>%%timeit\nn = 200\na = [1] * n\nb = [2] * n\nc = [0] * n\n\n\nthreads = []\nfor i in range(n):\n    threads.append(threading.Thread(target = add_vectors_element, args = [i]))\n\nfor t in threads:\n    t.start()\nfor t in threads:\n    t.join()\n</pre> %%timeit n = 200 a = [1] * n b = [2] * n c = [0] * n   threads = [] for i in range(n):     threads.append(threading.Thread(target = add_vectors_element, args = [i]))  for t in threads:     t.start() for t in threads:     t.join() <pre>7.18 ms \u00b1 241 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</pre> In\u00a0[20]: Copied! <pre>print (c[:5])\n</pre> print (c[:5]) <pre>[3, 3, 3, 3, 3]\n</pre> In\u00a0[24]: Copied! <pre>class MyObject:\n    def __init__(self):\n        self.property = 0\n\n    def write(self):\n        time.sleep(0.01)\n        self.property = 5\n    \n    def read(self):\n        time.sleep(0.01)\n        if self.property != 5:\n            print (\"reading...\")\n</pre> class MyObject:     def __init__(self):         self.property = 0      def write(self):         time.sleep(0.01)         self.property = 5          def read(self):         time.sleep(0.01)         if self.property != 5:             print (\"reading...\")  <p>What would happen if we took called <code>write</code> and <code>read</code> from two different threads? Would we see \"reading...\"?</p> In\u00a0[63]: Copied! <pre>obj = MyObject()\nt1 = threading.Thread(target = obj.write)\nt2 = threading.Thread(target = obj.read)\n\nt1.start()\nt2.start()\n\nt1.join()\nt2.join()\n</pre> obj = MyObject() t1 = threading.Thread(target = obj.write) t2 = threading.Thread(target = obj.read)  t1.start() t2.start()  t1.join() t2.join() <p>Is this always the case?</p> In\u00a0[66]: Copied! <pre>for i in range(100):\n    obj = MyObject()\n    t1 = threading.Thread(target = obj.write)\n    t2 = threading.Thread(target = obj.read)\n    \n    t1.start()\n    t2.start()\n    \n    t1.join()\n    t2.join()\n</pre> for i in range(100):     obj = MyObject()     t1 = threading.Thread(target = obj.write)     t2 = threading.Thread(target = obj.read)          t1.start()     t2.start()          t1.join()     t2.join() <pre>reading...\nreading...\nreading...\nreading...\nreading...\nreading...\nreading...\nreading...\nreading...\n</pre> <p>Here the small and variable overhead in creating thread and starting threads creates behaviour that is difficult to reproduce and therefore difficult to debug. In cases like this we should wrap dependent jobs within the same thread to ensure the execution order.</p> <p>No that we know what the GIL is, let's take a closer look at it's impact on our performance:</p> In\u00a0[29]: Copied! <pre>def countdown(n): \n    while n &gt; 0: \n        n -= 1\n</pre> def countdown(n):      while n &gt; 0:          n -= 1 In\u00a0[67]: Copied! <pre>large_number = 100_000_000\ntstart = time.time()\ncountdown(large_number)\ntelap = time.time() - tstart\n\nprint (f\"This took {telap : 0.3f} s\")\n</pre> large_number = 100_000_000 tstart = time.time() countdown(large_number) telap = time.time() - tstart  print (f\"This took {telap : 0.3f} s\") <pre>This took  2.389 s\n</pre> <p>It would make sense that if we were able to do half the work in two threads running at the same time, then it should take half the time to run. Is this the case?</p> In\u00a0[68]: Copied! <pre># run this in two seperate threads\nt1 = threading.Thread(target = countdown, args = [large_number // 2])\nt2 = threading.Thread(target = countdown, args = [large_number // 2])\n\ntstart = time.time()\nt1.start()\nt2.start()\nt1.join()\nt2.join()\n\ntelap = time.time() - tstart\n\nprint (f\"This took {telap : 0.3f} s\")\n</pre> # run this in two seperate threads t1 = threading.Thread(target = countdown, args = [large_number // 2]) t2 = threading.Thread(target = countdown, args = [large_number // 2])  tstart = time.time() t1.start() t2.start() t1.join() t2.join()  telap = time.time() - tstart  print (f\"This took {telap : 0.3f} s\") <pre>This took  2.740 s\n</pre> <p>It actually takes about the same time to run, if not longer! This is because the GIL is making it such that only thread can run at a time.</p> <p>How can we get around this?</p> In\u00a0[69]: Copied! <pre># run this in two seperate processes\np1 = mp.Process(target = countdown, args = [large_number // 2])\np2 = mp.Process(target = countdown, args = [large_number // 2])\n\ntstart = time.time()\np1.start()\np2.start()\np1.join()\np2.join()\n\ntelap = time.time() - tstart\n\nprint (f\"This took {telap : 0.3f} s\")\n</pre> # run this in two seperate processes p1 = mp.Process(target = countdown, args = [large_number // 2]) p2 = mp.Process(target = countdown, args = [large_number // 2])  tstart = time.time() p1.start() p2.start() p1.join() p2.join()  telap = time.time() - tstart  print (f\"This took {telap : 0.3f} s\") <pre>This took  1.410 s\n</pre> In\u00a0[52]: Copied! <pre>def track_live_time(x):\n    t = []\n    for i in range(10**6):\n        # log the current time\n        t.append(time.time())\n    # return array of times\n    return np.array(t)\n</pre> def track_live_time(x):     t = []     for i in range(10**6):         # log the current time         t.append(time.time())     # return array of times     return np.array(t) <p>For this example I'll use <code>concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor</code> because they have a very similar API.</p> In\u00a0[70]: Copied! <pre>from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n\n\ndef multi_threading(func, args, workers):\n    with ThreadPoolExecutor(workers) as ex:\n        res = ex.map(func, args)\n    return list(res)\n\ndef multi_processing(func, args, workers):\n    with ProcessPoolExecutor(workers) as ex:\n        res = ex.map(func, args)\n    return list(res)\n</pre> from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor   def multi_threading(func, args, workers):     with ThreadPoolExecutor(workers) as ex:         res = ex.map(func, args)     return list(res)  def multi_processing(func, args, workers):     with ProcessPoolExecutor(workers) as ex:         res = ex.map(func, args)     return list(res) <p>let's run 10 tasks across 4 workers.</p> <p>Here the workers are the number of threads or processes we're using:</p> In\u00a0[71]: Copied! <pre>n = 10\nworkers = 4\nr = np.random.randint(1, 10, size = n)\n\ntstart = time.time()\n[track_live_time(r) for i in range(n)]\ntelap = time.time() - tstart \nprint(f\"Serial took: {telap:0.5} s\")\n\ntstart = time.time()\nlive_times_threading = multi_threading(track_live_time, r, workers)\ntelap = time.time() - tstart \nprint(f\"Multithreading took: {telap:0.5} s\")\n\ntstart = time.time()\nlive_times_processing = multi_processing(track_live_time, r, workers)\ntelap = time.time() - tstart \nprint(f\"Multiprocessing took: {telap:0.5} s\")\n</pre> n = 10 workers = 4 r = np.random.randint(1, 10, size = n)  tstart = time.time() [track_live_time(r) for i in range(n)] telap = time.time() - tstart  print(f\"Serial took: {telap:0.5} s\")  tstart = time.time() live_times_threading = multi_threading(track_live_time, r, workers) telap = time.time() - tstart  print(f\"Multithreading took: {telap:0.5} s\")  tstart = time.time() live_times_processing = multi_processing(track_live_time, r, workers) telap = time.time() - tstart  print(f\"Multiprocessing took: {telap:0.5} s\")  <pre>Serial took: 1.3956 s\nMultithreading took: 1.3856 s\nMultiprocessing took: 0.76431 s\n</pre> <p>We can see that the multithreaded example takes about as long as the serial example. But the multiprocessing example takes about half the time.</p> <p>Let's plot the times logged by each:</p> In\u00a0[72]: Copied! <pre>fig, axs = plt.subplots(1,2, figsize = (12,4), sharey= True)\nfor i in range(len(live_times_threading)):\n    axs[0].plot(\n        live_times_threading[i] - np.min(live_times_threading),\n        np.ones(len(live_times_threading[i])) + i,\n        \"o\",\n        ms = 0.5)\n    \n    axs[1].plot(\n        live_times_processing[i] - np.min(live_times_processing),\n        np.ones(len(live_times_processing[i])) + i,\n        \"o\",\n        ms = 0.5)\nfor ax in axs:\n    ax.grid()\n    ax.set_xlabel(\"Time Elapsed [s]\")\naxs[0].set_ylabel(\"Job Number\")\n\naxs[0].set_title(\"Multi-Threading\")\naxs[1].set_title(\"Multi-Processing\")\n\nfig.tight_layout()\nfig.savefig(\"multi.png\")\n</pre> fig, axs = plt.subplots(1,2, figsize = (12,4), sharey= True) for i in range(len(live_times_threading)):     axs[0].plot(         live_times_threading[i] - np.min(live_times_threading),         np.ones(len(live_times_threading[i])) + i,         \"o\",         ms = 0.5)          axs[1].plot(         live_times_processing[i] - np.min(live_times_processing),         np.ones(len(live_times_processing[i])) + i,         \"o\",         ms = 0.5) for ax in axs:     ax.grid()     ax.set_xlabel(\"Time Elapsed [s]\") axs[0].set_ylabel(\"Job Number\")  axs[0].set_title(\"Multi-Threading\") axs[1].set_title(\"Multi-Processing\")  fig.tight_layout() fig.savefig(\"multi.png\") <p>We see gaps in the multi-threaded example. This is because we are switching between threads as the code is ran. However in the multi-processing example we are running 4 instances at the same time.</p> In\u00a0[73]: Copied! <pre>#%%timeit\nn = 200\na = [1] * n\nb = [2] * n\nc = [0] * n\n\nwith mp.Pool(processes = 16) as pool:\n    pool.map(add_vectors_element, range(n))\n</pre> #%%timeit n = 200 a = [1] * n b = [2] * n c = [0] * n  with mp.Pool(processes = 16) as pool:     pool.map(add_vectors_element, range(n)) In\u00a0[74]: Copied! <pre>print (c[:4])\n</pre> print (c[:4]) <pre>[0, 0, 0, 0]\n</pre> <p>What happened here? <code>c</code> should be a list with 200 values of <code>3</code>. We specifically used <code>global</code> in <code>add_vectors_element</code> to specify we're using the global value of <code>c</code> not the local value.</p> <pre>def add_vectors_element(i):\n    global a, b, c\n    c[i] = a[i] + b[i]\n</pre> <p>It looks like the global value of <code>c</code> wasn't modified by multiprocessing. This is because we've created entirely new instances of python to run our process through. This includes a new <code>global c</code> which is local to the child process and not the main process.</p> <p>By copying the variables that are need from the main to the child process and running as a new python process, multiprocessing circumvents the GIL. Each process will have its own local variables and its own GIL. This means a \"lock\" will be issued when each process is modifying their own instance of <code>c</code>, but since these aren't shared, there is no mutual lock preventing the processes from running at the same time.</p> <p>How could we rewrite this to get our desired output?</p> In\u00a0[75]: Copied! <pre>from multiprocessing import shared_memory\nimport sys\nc_shared = shared_memory.SharedMemory(create=True, size=sys.getsizeof(c))\n</pre> from multiprocessing import shared_memory import sys c_shared = shared_memory.SharedMemory(create=True, size=sys.getsizeof(c))  In\u00a0[76]: Copied! <pre>def add_vectors_element_shared(i):\n    global a, b, c_shared\n    c_shared.buf[i] = a[i] + b[i]\n</pre> def add_vectors_element_shared(i):     global a, b, c_shared     c_shared.buf[i] = a[i] + b[i] In\u00a0[77]: Copied! <pre>with mp.Pool(processes = 16) as pool:\n    pool.map(add_vectors_element_shared, range(n))\n</pre> with mp.Pool(processes = 16) as pool:     pool.map(add_vectors_element_shared, range(n)) In\u00a0[78]: Copied! <pre>[c_shared.buf[i] for i in range(4)]\n</pre> [c_shared.buf[i] for i in range(4)]  Out[78]: <pre>[3, 3, 3, 3]</pre> In\u00a0[79]: Copied! <pre>c_shared.close()\nc_shared.unlink()\n</pre> c_shared.close() c_shared.unlink() <p>Another way could be to split the task into chunk and then combine the chunks from each task.</p> In\u00a0[80]: Copied! <pre>def add_vectors_parallel(n, chunk):\n    global a, b\n    c = []\n    for i in range(n * chunk, (n+1) * chunk):\n        if i &gt;= len(a):\n            return c\n        c.append(a[i] + b[i])\n    return c\n</pre> def add_vectors_parallel(n, chunk):     global a, b     c = []     for i in range(n * chunk, (n+1) * chunk):         if i &gt;= len(a):             return c         c.append(a[i] + b[i])     return c  In\u00a0[81]: Copied! <pre>n = 200\na = [1] * n\nb = [2] * n\nc = [0] * n\n\nres = add_vectors_parallel(0, 3)\nprint (res[:5])\n</pre> n = 200 a = [1] * n b = [2] * n c = [0] * n  res = add_vectors_parallel(0, 3) print (res[:5]) <pre>[3, 3, 3]\n</pre> In\u00a0[82]: Copied! <pre># %%timeit\nn_proc = 8\nn = 200\n# Adding a little over subscription to ensure we fill all elements\nover_sub  = 0.3\nchunk = int(n // (n_proc - 1))\n\na = [1] * n\nb = [2] * n\nwith mp.Pool(n_proc) as pool:\n    res = pool.starmap(add_vectors_parallel, [[i, chunk] for i in range(n_proc)])\n</pre> # %%timeit n_proc = 8 n = 200 # Adding a little over subscription to ensure we fill all elements over_sub  = 0.3 chunk = int(n // (n_proc - 1))  a = [1] * n b = [2] * n with mp.Pool(n_proc) as pool:     res = pool.starmap(add_vectors_parallel, [[i, chunk] for i in range(n_proc)])    In\u00a0[83]: Copied! <pre>final = np.array([])\nfor r in res:\n    print (len(r))\n    final = np.append(final, r)\nprint (final[:5])\nprint (final.shape)\nprint (final.sum() / n)\n</pre> final = np.array([]) for r in res:     print (len(r))     final = np.append(final, r) print (final[:5]) print (final.shape) print (final.sum() / n) <pre>28\n28\n28\n28\n28\n28\n28\n4\n[3. 3. 3. 3. 3.]\n(200,)\n3.0\n</pre> In\u00a0[89]: Copied! <pre>def parallel_task(i):\n    time.sleep(0.1)\n    print(f\"Running task: {i}\")\n    return i*i\n</pre> def parallel_task(i):     time.sleep(0.1)     print(f\"Running task: {i}\")     return i*i In\u00a0[91]: Copied! <pre>with mp.Pool(processes=4) as pool:\n    res = pool.map(parallel_task, range(10))\nprint (res)\n</pre> with mp.Pool(processes=4) as pool:     res = pool.map(parallel_task, range(10)) print (res) <pre>Running task: 0Running task: 3\n\nRunning task: 2\nRunning task: 1\nRunning task: 4Running task: 5\n\nRunning task: 6\nRunning task: 7\nRunning task: 8Running task: 9\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n</pre> <p>How do we pass multiple arguments?</p> <p>We can use starmap!</p> In\u00a0[98]: Copied! <pre>def parallel_task_with_arg(i, number):\n    time.sleep(0.1)\n    print(f\"Running task: {i}, with number {number}\")\n    return i*number\n</pre> def parallel_task_with_arg(i, number):     time.sleep(0.1)     print(f\"Running task: {i}, with number {number}\")     return i*number In\u00a0[101]: Copied! <pre>with mp.Pool(processes=4) as pool:\n    arguments = [(i, np.random.random()) for i in range(10)]\n    res = pool.starmap(parallel_task_with_arg, arguments)\nprint (res)\n</pre> with mp.Pool(processes=4) as pool:     arguments = [(i, np.random.random()) for i in range(10)]     res = pool.starmap(parallel_task_with_arg, arguments) print (res) <pre>Running task: 0, with number 0.9485979316997868Running task: 1, with number 0.5395337513089026Running task: 2, with number 0.28013889341958087Running task: 3, with number 0.5752552495120759\n\n\n\nRunning task: 4, with number 0.19117026413827765Running task: 5, with number 0.8768078872759388Running task: 6, with number 0.8529407224083398Running task: 7, with number 0.644406631619091\n\n\n\nRunning task: 8, with number 0.08242269827131332Running task: 9, with number 0.7439961150318876\n\n[0.0, 0.5395337513089026, 0.5602777868391617, 1.7257657485362277, 0.7646810565531106, 4.384039436379695, 5.1176443344500395, 4.510846421333636, 0.6593815861705066, 6.695965035286988]\n</pre> In\u00a0[137]: Copied! <pre>def large_sum(n):\n    counter = 0\n    for i in range(n):\n        counter +=1\n    return counter\n</pre> def large_sum(n):     counter = 0     for i in range(n):         counter +=1     return counter  In\u00a0[138]: Copied! <pre>%%time \nres = large_sum(10**5)\nprint (res)\n</pre> %%time  res = large_sum(10**5) print (res) <pre>100000\nCPU times: user 3.5 ms, sys: 0 ns, total: 3.5 ms\nWall time: 3.48 ms\n</pre> In\u00a0[139]: Copied! <pre>%timeit large_sum(10**5)\n</pre> %timeit large_sum(10**5) <pre>2.58 ms \u00b1 84.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</pre> In\u00a0[140]: Copied! <pre>from numba import jit\n@jit\ndef large_sum_jit(n):\n    counter = 0\n    for i in range(n):\n        counter +=1\n    return counter\n</pre> from numba import jit @jit def large_sum_jit(n):     counter = 0     for i in range(n):         counter +=1     return counter In\u00a0[141]: Copied! <pre>%time large_sum_jit(10**5)\n</pre> %time large_sum_jit(10**5) <pre>CPU times: user 34.2 ms, sys: 170 \u00b5s, total: 34.4 ms\nWall time: 33.5 ms\n</pre> Out[141]: <pre>100000</pre> <p>This took longer.... What happened?</p> <p>The first time we run a <code>jit</code>-ted function it is compiled! We will aways get poor performance the first time we run it!</p> In\u00a0[142]: Copied! <pre>%%time \nres = large_sum_jit(10**5)\nprint (res)\n</pre> %%time  res = large_sum_jit(10**5) print (res) <pre>100000\nCPU times: user 49 \u00b5s, sys: 0 ns, total: 49 \u00b5s\nWall time: 46.7 \u00b5s\n</pre> In\u00a0[143]: Copied! <pre>%timeit large_sum_jit(10**5)\n</pre> %timeit large_sum_jit(10**5) <pre>98.4 ns \u00b1 3.81 ns per loop (mean \u00b1 std. dev. of 7 runs, 10,000,000 loops each)\n</pre> In\u00a0[144]: Copied! <pre>def vector_addition_loop(x,y):\n    z = np.zeros(x.shape)\n    n = z.shape[0]\n\n    for i in range(n):\n        z[i] = x[i] + y[i]\n    return z\n</pre> def vector_addition_loop(x,y):     z = np.zeros(x.shape)     n = z.shape[0]      for i in range(n):         z[i] = x[i] + y[i]     return z In\u00a0[145]: Copied! <pre>x = np.random.random(size = 10**5)\ny = np.random.random(size = 10**5)\n</pre> x = np.random.random(size = 10**5) y = np.random.random(size = 10**5) In\u00a0[146]: Copied! <pre>%timeit _ = vector_addition_loop(x,y)\n</pre> %timeit _ = vector_addition_loop(x,y) <pre>16.1 ms \u00b1 465 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</pre> In\u00a0[147]: Copied! <pre>@jit\ndef vector_addition_loop_jit(x,y):\n    z = np.zeros(x.shape)\n    n = z.shape[0]\n\n    for i in range(n):\n        z[i] = x[i] + y[i]\n    return z\n</pre> @jit def vector_addition_loop_jit(x,y):     z = np.zeros(x.shape)     n = z.shape[0]      for i in range(n):         z[i] = x[i] + y[i]     return z In\u00a0[148]: Copied! <pre>_ = vector_addition_loop_jit(x,y)\n</pre> _ = vector_addition_loop_jit(x,y) In\u00a0[149]: Copied! <pre>%timeit _ = vector_addition_loop_jit(x,y)\n</pre> %timeit _ = vector_addition_loop_jit(x,y) <pre>36.9 \u00b5s \u00b1 980 ns per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each)\n</pre> <p>Automagic parallelization with <code>numba</code>. We can use <code>prange</code> to explicitly tell <code>numba</code> that a loop is to be ran in parallel across all available CPUs. We also ass <code>parallel = True</code> to the <code>@jit</code> decorator. Without <code>parallel = True</code> <code>prange</code> will behave the same as <code>range</code>.</p> <p><code>Numba</code> can parallelize a lot of common element/bitwise operations (see here):</p> <ul> <li>unary operators: + - ~</li> <li>binary operators: + - * / /? % | &gt;&gt; ^ &lt;&lt; &amp; ** //</li> <li>comparison operators: == != &lt; &lt;= &gt; &gt;=</li> <li>Numpy ufuncs that are supported in nopython mode.</li> <li>User defined DUFunc through vectorize().</li> </ul> In\u00a0[150]: Copied! <pre>from numba import prange\n@jit(parallel= True)\ndef vector_addition_loop_jit_parallel(x,y):\n    z = np.zeros(x.shape)\n    n = z.shape[0]\n\n    # Use prange to explicitly run a parallel loop\n    for i in prange(n):\n        z[i] = x[i] + y[i]\n    return z\n</pre> from numba import prange @jit(parallel= True) def vector_addition_loop_jit_parallel(x,y):     z = np.zeros(x.shape)     n = z.shape[0]      # Use prange to explicitly run a parallel loop     for i in prange(n):         z[i] = x[i] + y[i]     return z In\u00a0[151]: Copied! <pre>_ = vector_addition_loop_jit_parallel(x,y)\n</pre> _ = vector_addition_loop_jit_parallel(x,y) In\u00a0[152]: Copied! <pre>x = np.random.random(size = 10**7)\ny = np.random.random(size = 10**7)\n</pre> x = np.random.random(size = 10**7) y = np.random.random(size = 10**7) In\u00a0[153]: Copied! <pre>%timeit _ = vector_addition_loop_jit_parallel(x,y)\n</pre> %timeit _ = vector_addition_loop_jit_parallel(x,y) <pre>14.8 ms \u00b1 908 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</pre> In\u00a0[154]: Copied! <pre>%timeit _ = vector_addition_loop_jit(x,y)\n</pre> %timeit _ = vector_addition_loop_jit(x,y) <pre>16.3 ms \u00b1 201 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</pre>"},{"location":"Python/Threading_And_Multiprocessing/#what-is-concurrent-programming","title":"What is Concurrent Programming\u00b6","text":"<p>Concurrent programming is when a number of processes, tasks or instructions happen at the same time. This is similar but not nessicarily the same of parallel programming. Concurrent programming can be strictly logical, meaning that multiple processes are logically running at the same time, however they only utilize the same processor. In this case the two proceses share the processor with only one process being active at a given time.</p>"},{"location":"Python/Threading_And_Multiprocessing/#what-is-threading","title":"What is Threading\u00b6","text":"<p>Threading is where we have multiple processes running concurrently, while utilizing a single processor<sup>[1]</sup> . Threading gives off the appearance of parallel processing by switching between tasks such that only one task is ever running at a given time. In this case the processes are \"logically happening at the same time\", but not parallel.</p> <p>A example of this could be a web browser with multiple tabs open. In this analogy each tab is a seperate thread, but we're only ever looking at a single tab, therefore only one thread is ever executed at a time.</p> <p>In programming, threading can be useful for expensive unrelated tasks such as downloading a file. Consider the following example:</p> Time Step Main Child 1 Child 2 1 spawns child 1 2 determine background criteria downloading file 3 spawns child 2 downloading file 4 look up existing files downloading file Background calculation 5 organise files downloading file  Background complete  6 select files of interest  Download complete   Background complete  7 close threads  Download complete   Background complete  8 perform analysis <p>1. ^ Note this is specifically the case in Python and may not be the case in other langugaes.</p>"},{"location":"Python/Threading_And_Multiprocessing/#what-is-this-slower","title":"What is this slower?\u00b6","text":"<p>There are a few reasons why. Firstly, spawning and waiting for threads to finish introduces and additional overhead, but it's not likely the major problem.</p> <p>It is likely we are being blocked by the Global interpreter lock.</p>"},{"location":"Python/Threading_And_Multiprocessing/#what-is-the-gil","title":"What is the GIL\u00b6","text":"<p>The GIL (Global interpreter lock) is a mechinism in CPython which allows only one instance of bytecode to be executed at a time. Essentially, to prevent data races.</p>"},{"location":"Python/Threading_And_Multiprocessing/#what-is-a-data-racerace-condition","title":"What is a data race/race condition\u00b6","text":"<p>A data race or a race condition occurs when multiple processes try to modify and/or read the same piece of memory at the same time.</p> <p>This can result in the output of the code being dependent on the order in which the code is executed.</p> <p>Consider two functions that act on the same object:</p>"},{"location":"Python/Threading_And_Multiprocessing/#multiprocessing","title":"Multiprocessing\u00b6","text":"<p>Multiprocessing  is a method where instead of spawning a new thread, we spawn an entirely new process. This essentially means that we start a new instance of python, load what is needed into that instance and execute the code we want.</p> <p>The GIL will still be enforce, however each process will have it's own GIL, meaning the two can operate independently.</p>"},{"location":"Python/Threading_And_Multiprocessing/#what-is-parallel-programming","title":"What is Parallel Programming?\u00b6","text":"<p>Parallel programming is when a number of processes, tasks or instructions are executed in parallel to each other. This can be obtained using a single CPU by threading or using multiple CPUs by multiprocessing, or some combination of both. In Python, threading will only give us logically parallel programming as the GIL limits true parallelization using threading. This is not the case in languages like C++.</p> <p></p>"},{"location":"Python/Threading_And_Multiprocessing/#what-is-the-difference-between-threading-and-multiprocessing","title":"What is the Difference between Threading and Multiprocessing?\u00b6","text":"<p>As we've seen, Threading, in Python, is limited to one task at a time by the GIL. Threading achieves the appearance of true parallelization by switching between tasks.</p> <p>Multiprocessing explicitly creates a new python process. As a result, we copy data into the new instant. This allows us to get around the GIL and obtain true parallelization. However, since we are required to copy memory into the instance, we cannot, by default, share memory across different threads. We'll look into how to do this later...</p> <p>Consider the following example of a function that logs the time inbetween interations of a loop</p>"},{"location":"Python/Threading_And_Multiprocessing/#returning-to-the-vector-addition","title":"Returning to the vector addition:\u00b6","text":"<p>Let's return to the vector example and try to run it with multiprocessing</p>"},{"location":"Python/Threading_And_Multiprocessing/#taking-a-closer-look-at-multiprocessing","title":"Taking a closer look at multiprocessing\u00b6","text":"<p>Multiprocessing has different ways to execute in parallel. Let's take alook at:</p> <ul> <li>map</li> <li>starmap</li> </ul>"},{"location":"Python/Threading_And_Multiprocessing/#parallelizing-with-numba","title":"Parallelizing with Numba\u00b6","text":"<p>Numba is an awesome library that can really help turbo charge our code. Numba allows for automagic parallelization and just in time (JIT) compiling. JIT allows us to compile the Python code we write to machine code, often providing significant performace improvements.</p>"},{"location":"Python/python-tutorial-empty/","title":"Classes in Python","text":"In\u00a0[2]: Copied! <pre># Defining a bool\nmy_variable = True\n# Redefining as a float\nmy_variable = 3.14\n# Redefining as a string\nmy_variable = \"hello\"\nmy_variable\n</pre> # Defining a bool my_variable = True # Redefining as a float my_variable = 3.14 # Redefining as a string my_variable = \"hello\" my_variable Out[2]: <pre>'hello'</pre> <p>In python we can print output using the <code>print()</code> function:</p> In\u00a0[3]: Copied! <pre># Defining as a string\nmy_variable = \"hello\"\nprint (my_variable)\n# Redefining a bool\nmy_variable = False\nprint (my_variable)\n# Redefining as a float\nmy_variable = 3.14\nprint (my_variable)\nprint (\"Goodbye, World!\")\n</pre> # Defining as a string my_variable = \"hello\" print (my_variable) # Redefining a bool my_variable = False print (my_variable) # Redefining as a float my_variable = 3.14 print (my_variable) print (\"Goodbye, World!\")  <pre>hello\nFalse\n3.14\nGoodbye, World!\n</pre> <p>when printing we can format strings using <code>fstrings</code>:</p> In\u00a0[10]: Copied! <pre>pi = 3.14159265359\nprint (f\"Pi to 5 digits = {pi : 0.5f}\")\nprint (f\"Pi to 3 digits = {pi: 0.3f}\")\nprint (f\"Pi to 4 digits in scientific notation = {pi : 0.2e}\")\nprint (f\"Pi as an integer = {pi: 0.0f}, minus pi to 1 digit {-pi :0.1f}\")\n</pre> pi = 3.14159265359 print (f\"Pi to 5 digits = {pi : 0.5f}\") print (f\"Pi to 3 digits = {pi: 0.3f}\") print (f\"Pi to 4 digits in scientific notation = {pi : 0.2e}\") print (f\"Pi as an integer = {pi: 0.0f}, minus pi to 1 digit {-pi :0.1f}\") <pre>Pi to 5 digits =  3.14159\nPi to 3 digits =  3.142\nPi to 4 digits in scientific notation =  3.14e+00\nPi as an integer =  3, minus pi to 1 digit -3.1\n</pre> <p>We can also define strings to format later:</p> In\u00a0[12]: Copied! <pre>my_string = \"My name is {name}, my favorite number is {number}\"\n\nformatted = my_string.format( name = \"Ste\", number = 42)\nprint(formatted)\nprint(my_string)\n</pre> my_string = \"My name is {name}, my favorite number is {number}\"  formatted = my_string.format( name = \"Ste\", number = 42) print(formatted) print(my_string)  <pre>My name is Ste, my favorite number is 42\nMy name is {name}, my favorite number is {number}\n</pre> <p>And we can add strings together and take slices of string:</p> In\u00a0[14]: Copied! <pre># Adding to a string\nextended = formatted + \" and I like pie\"\nprint (extended)\n# Taking up to the last 6 elements and adding \"pi\"\nprint ( extended[:-6] + \"pi\")\n</pre> # Adding to a string extended = formatted + \" and I like pie\" print (extended) # Taking up to the last 6 elements and adding \"pi\" print ( extended[:-6] + \"pi\") <pre>My name is Ste, my favorite number is 42 and I like pie\nMy name is Ste, my favorite number is 42 and I lipi\n</pre> In\u00a0[15]: Copied! <pre>a = 7\nb = 2.2\n# Normal division\nc = a / b\n# Integer division\nd = a // b\n# Modulus (remainder)\ne = a % b\n\nprint(f\"{a} / {b} = {c}\")\nprint(f\"{a} // {b} = {d}\")\nprint(f\"{a} % {b} = {e}\")\n\nprint (f\"a &gt; 2: {a &gt; 2}\")\n</pre> a = 7 b = 2.2 # Normal division c = a / b # Integer division d = a // b # Modulus (remainder) e = a % b  print(f\"{a} / {b} = {c}\") print(f\"{a} // {b} = {d}\") print(f\"{a} % {b} = {e}\")  print (f\"a &gt; 2: {a &gt; 2}\") <pre>7 / 2.2 = 3.1818181818181817\n7 // 2.2 = 3.0\n7 % 2.2 = 0.39999999999999947\na &gt; 2: True\n</pre> In\u00a0[17]: Copied! <pre>x = 10\ny = float(x)\nz = str(x)\nv = bool(x)\nprint (x,y,z,v)\nprint (type(x), type(y), type(z), type(v))\n</pre> x = 10 y = float(x) z = str(x) v = bool(x) print (x,y,z,v) print (type(x), type(y), type(z), type(v))  <pre>10 10.0 10 True\n&lt;class 'int'&gt; &lt;class 'float'&gt; &lt;class 'str'&gt; &lt;class 'bool'&gt;\n</pre> In\u00a0[24]: Copied! <pre># Create a list\nmy_list = [1,2,3,4,'apple']\nprint (my_list)\n\n# Add a new element to the end\nmy_list.append('jeudi')\nprint (my_list)\n\n# \"Pop\" out the last element\nelement = my_list.pop()\nprint (element, my_list)\n\nmy_first = my_list.pop(0)\nprint (my_first, my_list)\n\n# Reasign a value\nmy_list[-1] = 58.3\nprint (my_list)\n\n\n# Lists can include multiple data types\nmy_list[2] = \"hello\"\nprint(my_list)\n</pre> # Create a list my_list = [1,2,3,4,'apple'] print (my_list)  # Add a new element to the end my_list.append('jeudi') print (my_list)  # \"Pop\" out the last element element = my_list.pop() print (element, my_list)  my_first = my_list.pop(0) print (my_first, my_list)  # Reasign a value my_list[-1] = 58.3 print (my_list)   # Lists can include multiple data types my_list[2] = \"hello\" print(my_list) <pre>[1, 2, 3, 4, 'apple']\n[1, 2, 3, 4, 'apple', 'jeudi']\njeudi [1, 2, 3, 4, 'apple']\n1 [2, 3, 4, 'apple']\n[2, 3, 4, 58.3]\n[2, 3, 'hello', 58.3]\n</pre> In\u00a0[27]: Copied! <pre># Define a new list\nmy_list = [1,2,3,4,5,6,7,8,9,1,2,3,4]\nprint (my_list)\n\n# Create a slice excluding the first and last\nmy_sub_list = my_list[1:-1]\nprint(my_sub_list)\n\n# get the length of the list\nprint (f\"The list is {len(my_sub_list)} elements long\")\n</pre> # Define a new list my_list = [1,2,3,4,5,6,7,8,9,1,2,3,4] print (my_list)  # Create a slice excluding the first and last my_sub_list = my_list[1:-1] print(my_sub_list)  # get the length of the list print (f\"The list is {len(my_sub_list)} elements long\")     <pre>[1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4]\n[2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3]\nThe list is 11 elements long\n</pre> In\u00a0[32]: Copied! <pre># Create a set using {}\nfirst_set = {1,2,3,4,5,7,5,2,1}\nprint (first_set)\n\n# create a set from the list\nmy_set = set(my_list)\nprint (my_set)\n\n# Add values to the set\nmy_set.add(13)\nmy_set.add(1)\nprint (my_set)\n\n# Sets can have multiple data types\nmy_set.add(\"hello\")\nprint (my_set)\n</pre> # Create a set using {} first_set = {1,2,3,4,5,7,5,2,1} print (first_set)  # create a set from the list my_set = set(my_list) print (my_set)  # Add values to the set my_set.add(13) my_set.add(1) print (my_set)  # Sets can have multiple data types my_set.add(\"hello\") print (my_set) <pre>{1, 2, 3, 4, 5, 7}\n{1, 2, 3, 4, 5, 6, 7, 8, 9}\n{1, 2, 3, 4, 5, 6, 7, 8, 9, 13}\n{1, 2, 3, 4, 5, 6, 7, 8, 9, 'hello', 13}\n</pre> In\u00a0[33]: Copied! <pre>my_tup = (4, 5, 6, 'dog', 'cat', 'rabbit')\nprint (my_tup)\nmy_tup[0] = -2\nprint (my_tup)\n</pre> my_tup = (4, 5, 6, 'dog', 'cat', 'rabbit') print (my_tup) my_tup[0] = -2 print (my_tup) <pre>(4, 5, 6, 'dog', 'cat', 'rabbit')\n</pre> <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[33], line 3\n      1 my_tup = (4, 5, 6, 'dog', 'cat', 'rabbit')\n      2 print (my_tup)\n----&gt; 3 my_tup[0] = -2\n      4 print (my_tup)\n\nTypeError: 'tuple' object does not support item assignment</pre> In\u00a0[39]: Copied! <pre>my_dict = {}\n\nfmt_string = \"entry_{entry}\"\nmy_dict[fmt_string.format(entry = 9)] = \"Hello\"\nprint (my_dict[\"entry_9\"])\n\nif \"entry_9\" in my_dict:\n    print (\"Key exists\")\n\nmy_dict[\"workshop\"] = [1,2,3,4]\n\nif  \"workshop\" in my_dict:\n    print (\"Key exists: \", my_dict[\"workshop\"][2])\n</pre> my_dict = {}  fmt_string = \"entry_{entry}\" my_dict[fmt_string.format(entry = 9)] = \"Hello\" print (my_dict[\"entry_9\"])  if \"entry_9\" in my_dict:     print (\"Key exists\")  my_dict[\"workshop\"] = [1,2,3,4]  if  \"workshop\" in my_dict:     print (\"Key exists: \", my_dict[\"workshop\"][2])   <pre>Hello\nKey exists\nKey exists:  3\n</pre> In\u00a0[41]: Copied! <pre># Create an empty list\nx = []\n\n# range(n) will return an iteratable type which goes from 0-10 exclusive (0,1,...,9)\n# Write a loop taht will append i to the list x\nfor i in range(10):\n    x.append(i)\n\nprint (x)\n\n# The list x is also iterable\n# write a list to square each element and print to the screen\nfor element in x:\n    print (element**2)\n</pre> # Create an empty list x = []  # range(n) will return an iteratable type which goes from 0-10 exclusive (0,1,...,9) # Write a loop taht will append i to the list x for i in range(10):     x.append(i)  print (x)  # The list x is also iterable # write a list to square each element and print to the screen for element in x:     print (element**2) <pre>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n0\n1\n4\n9\n16\n25\n36\n49\n64\n81\n</pre> <p>We can also use \"list comprehension\" to generate a list from simple loops. This takes the format:</p> <pre>array = [ some_funtion(i) for i in some_loop() ]\n</pre> <p>We can also use list comprehension to do some filtering:</p> <pre>array = [ some_function(i) for i in some_loop() if some_condition(i) ]\n</pre> <p>The if statement can go before or after the <code>for</code> loop:</p> <pre>array = [ some_function(i) if some_condition(i) for i in some_loop()  ]\n</pre> <p>We can also add in an else condition:</p> <pre>array = [ some_function(i) if some_condition(i) else 0 for i in some_loop() ]\n</pre> In\u00a0[47]: Copied! <pre># Create a list with numbers between 0 and 100\nmy_list = [ i for i in range(100) ]\n# Create a list with only even numbers between 0 and 100\nmy_even_list = [ i for i in range(100) if i % 2 == 0  ]\n# Create a list with 0 for even indices and 1 for odd indicies\n# Between 0 and 100\nmy_conditional_list = [ 0  if i % 2 == 0 else 1 for i in range(100)  ]\n\nprint (my_list[:5])\nprint (my_even_list[:5])\nprint (my_conditional_list[:5])\n</pre> # Create a list with numbers between 0 and 100 my_list = [ i for i in range(100) ] # Create a list with only even numbers between 0 and 100 my_even_list = [ i for i in range(100) if i % 2 == 0  ] # Create a list with 0 for even indices and 1 for odd indicies # Between 0 and 100 my_conditional_list = [ 0  if i % 2 == 0 else 1 for i in range(100)  ]  print (my_list[:5]) print (my_even_list[:5]) print (my_conditional_list[:5])  <pre>[0, 1, 2, 3, 4]\n[0, 2, 4, 6, 8]\n[0, 1, 0, 1, 0]\n</pre> In\u00a0[48]: Copied! <pre>i = 0\n# This will not run\n# Write a while loop that won't run\nwhile None:\n    i = i + 1\n    if i &gt; 5:\n        break\n</pre> i = 0 # This will not run # Write a while loop that won't run while None:     i = i + 1     if i &gt; 5:         break In\u00a0[50]: Copied! <pre>i = 0\n# Use while loop to print the numbers up to 10\nwhile i &lt; 10:\n    print (i)\n    i = i +1\n</pre> i = 0 # Use while loop to print the numbers up to 10 while i &lt; 10:     print (i)     i = i +1  <pre>0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n</pre> In\u00a0[54]: Copied! <pre>i = 0\n# Use an if condition to break this infinite loop after the 5th iteration\nwhile True:\n    if i &gt;= 5:\n        break\n    i = i +1\n    print (i)\n</pre> i = 0 # Use an if condition to break this infinite loop after the 5th iteration while True:     if i &gt;= 5:         break     i = i +1     print (i)       <pre>1\n2\n3\n4\n5\n</pre> <p>Without the if statement here we would have an infinite loop!  We can exit out of a loop with a <code>break</code> command or we can skip to the end of the current iteration using the <code>continue</code> command.</p> <p>Let's use <code>if</code> statements to see how these work.</p> In\u00a0[\u00a0]: Copied! <pre>even_sum = 0\nodd_sum = 0\n\n# Write a loop over 0-100\n\n    # Exit the loop when i goes above or equal to 10\n\n    # Skip the i = 3 or the i = 0 iteration\n\n    # otherwise sum the odd and event values\n</pre> even_sum = 0 odd_sum = 0  # Write a loop over 0-100      # Exit the loop when i goes above or equal to 10      # Skip the i = 3 or the i = 0 iteration      # otherwise sum the odd and event values      In\u00a0[66]: Copied! <pre># Simple function to print hello\ndef print_hello():\n    print (\"Hello\")\n\n# We can take in arguments\ndef print_message(msg):\n    print (msg)\n\n# Simple function to take in two arguments and add them togeter and return the sum\ndef add_numbers(a, b, subtract = False ):\n    \n    if subtract:\n        print (a - b)\n        return a - b\n    else:\n        print (a+b)\n        return a + b\n\n# We can also pass a function to a function\ndef repeat(func, n, args, kwargs):\n    for i in range(n):\n        func(*args, **kwargs)\n        # func(args[0], args[1])\nprint_hello()\nprint_message(42)\n\nc = add_numbers(1.3, 5)\n\n# We can specify which variable is which by specificing the argument name/\nrepeat(func = add_numbers, n = 1, args=(1.3,2.1), kwargs={\"subtract\" : True})\n</pre> # Simple function to print hello def print_hello():     print (\"Hello\")  # We can take in arguments def print_message(msg):     print (msg)  # Simple function to take in two arguments and add them togeter and return the sum def add_numbers(a, b, subtract = False ):          if subtract:         print (a - b)         return a - b     else:         print (a+b)         return a + b  # We can also pass a function to a function def repeat(func, n, args, kwargs):     for i in range(n):         func(*args, **kwargs)         # func(args[0], args[1]) print_hello() print_message(42)  c = add_numbers(1.3, 5)  # We can specify which variable is which by specificing the argument name/ repeat(func = add_numbers, n = 1, args=(1.3,2.1), kwargs={\"subtract\" : True}) <pre>Hello\n42\n6.3\n-0.8\n</pre> <p>We can also write <code>lambda</code> functions which are short inline functions:</p> In\u00a0[73]: Copied! <pre># Lambda function to get square root\nmy_function = lambda x : x ** 0.5\n\n# Lambda function to act as a wrapper\ndef larger_function(x, y):\n    return x**2 / y**3\n\n# # Lambda function which calls for y = 1.5\nfor i in range(10):\n    const = 1.5 + i\n    my_wrapper = lambda x : larger_function(x, const)\n    print (my_wrapper(3))\n\n\n# print (my_function(4))\n# print (larger_function(3,4))-0.8\n</pre> # Lambda function to get square root my_function = lambda x : x ** 0.5  # Lambda function to act as a wrapper def larger_function(x, y):     return x**2 / y**3  # # Lambda function which calls for y = 1.5 for i in range(10):     const = 1.5 + i     my_wrapper = lambda x : larger_function(x, const)     print (my_wrapper(3))   # print (my_function(4)) # print (larger_function(3,4))-0.8  <pre>2.6666666666666665\n0.576\n0.2099125364431487\n0.09876543209876543\n0.054094665664913597\n0.032771961766044605\n0.021333333333333333\n0.014654996946875637\n0.010497157019973757\n0.007774538386783284\n</pre> In\u00a0[74]: Copied! <pre>def calc(x, b, i):\n    x[i] = x[i] / b\n    return x\n</pre> def calc(x, b, i):     x[i] = x[i] / b     return x <p>The function scales an element in <code>x</code> by <code>1/b</code>. We can choose a name to better describe what the function does.</p> In\u00a0[75]: Copied! <pre>def scale_element(x, b, i):\n    x[i] = x[i] / b\n    return x\n</pre> def scale_element(x, b, i):     x[i] = x[i] / b     return x <p>The use now knows that the function will scale an element of the array, but the user still doesn't know what the arguments are or what is returned. We can add a doc string to help with this.</p> In\u00a0[76]: Copied! <pre>help(scale_element)\n</pre> help(scale_element) <pre>Help on function scale_element in module __main__:\n\nscale_element(x, b, i)\n\n</pre> <p>This help message is automatically generated from the \"docstring\" of the function. The docstring is a small description of a function that we write at the state of the function.</p> <p>Following the Google Python Style Guide, a good template for your docstring is:</p> <pre><code>def function(x,y,z):\n    \"\"\"One line summary of my function\n\n    More detailed description of my function, potentially showing\n    some math relation:\n    $\\frac{dy}{dx} = x^2$\n\n    Args:\n        x: description of x\n        y: description of y\n        z: description of z\n\n    Returns:\n        description of what is returned\n\n    Examples:\n        Some example\n        &gt;&gt;&gt; function (x,y,z)\n        return_value\n\n    Raises:\n        Error: Error raised and description of that error\n    \"\"\"\n</code></pre> <p>This is quite verbose but has huge benifits for the user and allows us to self document our code.</p> In\u00a0[78]: Copied! <pre>def scale_element(x, b, i):\n    \"\"\"Scale element i in x by 1/b\n\n\n    Args:\n        x: list of value\n        b: some constant\n        i: index to scale\n\n    Returns:\n        return x with element i = x[i]/b\n\n    Examples:\n        &gt;&gt;&gt; scale_element([1,2,3,4], 2, 1)\n        [1,1.0,3,4]\n    \"\"\"\n    x[i] = x[i] / b\n    return x\n</pre> def scale_element(x, b, i):     \"\"\"Scale element i in x by 1/b       Args:         x: list of value         b: some constant         i: index to scale      Returns:         return x with element i = x[i]/b      Examples:         &gt;&gt;&gt; scale_element([1,2,3,4], 2, 1)         [1,1.0,3,4]     \"\"\"     x[i] = x[i] / b     return x <p>Let's break down the sections here:</p> <pre><code>    \"\"\"Scale an element of the input array\n</code></pre> <p>We start off with a short 1 sentence description of the function</p> <pre><code>    Scale an element of the array by a constant\n</code></pre> <p>We then use a more detailed description of the function and how to use it</p> <pre><code>    Args:\n        x: list of values\n        b: constant to scale by\n        i: index of element to scale\n</code></pre> <p>We list the arguments by name and what they are.</p> <pre><code>\n    Returns:\n        Scaled a list with the element i scaled by 1/b\n</code></pre> <p>We list what is returned by the function and what they are.</p> <pre><code>    Examples:\n        &gt;&gt;&gt; scale_element([1,2,3,4], 2, 1)\n        [1,1.0,3,4]\n\n    \"\"\"\n</code></pre> <p>We give an usage example of the functions and the expected output. The user can then access this helpful message anything using:</p> In\u00a0[79]: Copied! <pre>help(scale_element)\n</pre> help(scale_element) <pre>Help on function scale_element in module __main__:\n\nscale_element(x, b, i)\n    Scale element i in x by 1/b\n    \n    \n    Args:\n        x: list of value\n        b: some constant\n        i: index to scale\n    \n    Returns:\n        return x with element i = x[i]/b\n    \n    Examples:\n        &gt;&gt;&gt; scale_element([1,2,3,4], 2, 1)\n        [1,1.0,3,4]\n\n</pre> <p>We can do better still. The user doesn't know what type of data to pass. For example if x is just a float rather than an array, this will fail. We can do this with \"type-hinting\". Type hinting is an optional feature in Python where we tell the expect type of the data that a fuction is expecting.</p> <p>For example:</p> In\u00a0[80]: Copied! <pre>def multiply(a, b):\n    return a * b\n\n# what happens when we pass two floats to this function?\nmultiply(1.5, 2.3)\n</pre> def multiply(a, b):     return a * b  # what happens when we pass two floats to this function? multiply(1.5, 2.3) Out[80]: <pre>3.4499999999999997</pre> In\u00a0[81]: Copied! <pre># What happens when we pass a string and an int?\nmultiply(\"apple\", 2)\n</pre> # What happens when we pass a string and an int? multiply(\"apple\", 2) Out[81]: <pre>'appleapple'</pre> <p>We've originally defined our function to ints or floats but we behaviour we weren't expecting when passing a string. We can use type hinting to be explicit about what can be passed to the function.</p> In\u00a0[82]: Copied! <pre>def multiply(a : float, b : float ) -&gt; float:\n    return a * b\n\n# what happens when we pass two floats to this function?\nmultiply(1.5, 2.3)\nhelp(multiply)\n</pre> def multiply(a : float, b : float ) -&gt; float:     return a * b  # what happens when we pass two floats to this function? multiply(1.5, 2.3) help(multiply) <pre>Help on function multiply in module __main__:\n\nmultiply(a: float, b: float) -&gt; float\n\n</pre> In\u00a0[83]: Copied! <pre># What happens when we pass a string and an int?\nmultiply(\"apple\", 2)\n</pre> # What happens when we pass a string and an int? multiply(\"apple\", 2) Out[83]: <pre>'appleapple'</pre> <p>This doesn't stop us from calling the function with a string and int, but it does provide additional information to the <code>help()</code> function.</p> In\u00a0[84]: Copied! <pre>help(multiply)\n</pre> help(multiply) <pre>Help on function multiply in module __main__:\n\nmultiply(a: float, b: float) -&gt; float\n\n</pre> In\u00a0[85]: Copied! <pre>def scale_element(x : list, b : float, i : int) -&gt; list:\n    \"\"\"Scale element i in x by 1/b\n\n    Args:\n        x: list of value\n        b: some constant\n        i: index to scale\n\n    Returns:\n        return x with element i = x[i]/b\n\n    Examples:\n        &gt;&gt;&gt; scale_element([1,2,3,4], 2, 1)\n        [1,1.0,3,4]\n    \"\"\"\n    x[i] = x[i] / b\n    return x\n</pre> def scale_element(x : list, b : float, i : int) -&gt; list:     \"\"\"Scale element i in x by 1/b      Args:         x: list of value         b: some constant         i: index to scale      Returns:         return x with element i = x[i]/b      Examples:         &gt;&gt;&gt; scale_element([1,2,3,4], 2, 1)         [1,1.0,3,4]     \"\"\"     x[i] = x[i] / b     return x <p>From the first line we can see:</p> <pre><code>def scale_element(x : list, b : float, i : int) -&gt; list:\n</code></pre> <p>That <code>x</code> is expected to be a <code>list</code>, <code>b</code> is expected to be a <code>float</code>, <code>i</code> is expected to be a <code>int</code> and that the function will return a <code>list</code>.</p> In\u00a0[86]: Copied! <pre>help(scale_element)\n</pre> help(scale_element) <pre>Help on function scale_element in module __main__:\n\nscale_element(x: list, b: float, i: int) -&gt; list\n    Scale element i in x by 1/b\n    \n    Args:\n        x: list of value\n        b: some constant\n        i: index to scale\n    \n    Returns:\n        return x with element i = x[i]/b\n    \n    Examples:\n        &gt;&gt;&gt; scale_element([1,2,3,4], 2, 1)\n        [1,1.0,3,4]\n\n</pre> <p>This is better but let's try and anticipate potential errors</p> In\u00a0[87]: Copied! <pre>scale_element(3, 2, 1)\n</pre> scale_element(3, 2, 1) <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[87], line 1\n----&gt; 1 scale_element(3, 2, 1)\n\nCell In[85], line 16, in scale_element(x, b, i)\n      1 def scale_element(x : list, b : float, i : int) -&gt; list:\n      2     \"\"\"Scale element i in x by 1/b\n      3 \n      4     Args:\n   (...)\n     14         [1,1.0,3,4]\n     15     \"\"\"\n---&gt; 16     x[i] = x[i] / b\n     17     return x\n\nTypeError: 'int' object is not subscriptable</pre> <p>This error message isn't too helpful... Let's write our own</p> In\u00a0[92]: Copied! <pre>def scale_element(x : list, b : float, i : int) -&gt; list:\n    \"\"\"Scale element i in x by 1/b\n\n    Args:\n        x: list of value\n        b: some constant\n        i: index to scale\n\n    Returns:\n        return x with element i = x[i]/b\n\n    Examples:\n        &gt;&gt;&gt; scale_element([1,2,3,4], 2, 1)\n        [1, 1.0, 3, 4]\n\n    Raises:\n        TypeError : if x isn't a list\n    \"\"\"\n    if not isinstance(x, list):\n        raise TypeError(f\"Expected a list, instead received {type(x)}\")\n    x[i] = x[i] / b\n    return x\n</pre> def scale_element(x : list, b : float, i : int) -&gt; list:     \"\"\"Scale element i in x by 1/b      Args:         x: list of value         b: some constant         i: index to scale      Returns:         return x with element i = x[i]/b      Examples:         &gt;&gt;&gt; scale_element([1,2,3,4], 2, 1)         [1, 1.0, 3, 4]      Raises:         TypeError : if x isn't a list     \"\"\"     if not isinstance(x, list):         raise TypeError(f\"Expected a list, instead received {type(x)}\")     x[i] = x[i] / b     return x In\u00a0[93]: Copied! <pre>scale_element(3, 2, 1)\n</pre> scale_element(3, 2, 1) <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[93], line 1\n----&gt; 1 scale_element(3, 2, 1)\n\nCell In[92], line 20, in scale_element(x, b, i)\n      2 \"\"\"Scale element i in x by 1/b\n      3 \n      4 Args:\n   (...)\n     17     TypeError : if x isn't a list\n     18 \"\"\"\n     19 if not isinstance(x, list):\n---&gt; 20     raise TypeError(f\"Expected a list, instead received {type(x)}\")\n     21 x[i] = x[i] / b\n     22 return x\n\nTypeError: Expected a list, instead received &lt;class 'int'&gt;</pre> In\u00a0[94]: Copied! <pre>scale_element([3,2,2], 2, 5)\n</pre> scale_element([3,2,2], 2, 5) <pre>\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[94], line 1\n----&gt; 1 scale_element([3,2,2], 2, 5)\n\nCell In[92], line 21, in scale_element(x, b, i)\n     19 if not isinstance(x, list):\n     20     raise TypeError(f\"Expected a list, instead received {type(x)}\")\n---&gt; 21 x[i] = x[i] / b\n     22 return x\n\nIndexError: list index out of range</pre> In\u00a0[95]: Copied! <pre>help(scale_element)\n</pre> help(scale_element) <pre>Help on function scale_element in module __main__:\n\nscale_element(x: list, b: float, i: int) -&gt; list\n    Scale element i in x by 1/b\n    \n    Args:\n        x: list of value\n        b: some constant\n        i: index to scale\n    \n    Returns:\n        return x with element i = x[i]/b\n    \n    Examples:\n        &gt;&gt;&gt; scale_element([1,2,3,4], 2, 1)\n        [1, 1.0, 3, 4]\n    \n    Raises:\n        TypeError : if x isn't a list\n\n</pre> In\u00a0[96]: Copied! <pre>import doctest\ndoctest.testmod(verbose=True)\n</pre> import doctest doctest.testmod(verbose=True)  <pre>Trying:\n    scale_element([1,2,3,4], 2, 1)\nExpecting:\n    [1, 1.0, 3, 4]\nok\n10 items had no tests:\n    __main__\n    __main__.add_numbers\n    __main__.calc\n    __main__.larger_function\n    __main__.multiply\n    __main__.my_function\n    __main__.my_wrapper\n    __main__.print_hello\n    __main__.print_message\n    __main__.repeat\n1 items passed all tests:\n   1 tests in __main__.scale_element\n1 tests in 11 items.\n1 passed and 0 failed.\nTest passed.\n</pre> Out[96]: <pre>TestResults(failed=0, attempted=1)</pre> In\u00a0[97]: Copied! <pre>import numpy as np\nimport math as m\n\nprint(np.sin(0))\nprint(m.sin(0))\n</pre> import numpy as np import math as m  print(np.sin(0)) print(m.sin(0)) <pre>0.0\n0.0\n</pre> <p>Here we have imported <code>numpy</code> using the alias <code>np</code> and <code>math</code> using the alias <code>m</code>. Then, we call the <code>sin</code> function from both packages, specifying which version of the <code>sin</code> function we want to invoke.</p> <p>Numpy is a crucial package in scientific programming, and we'll delve deeper into its functionalities shortly.</p> <p>We can also import only a section of a package. For example:</p> In\u00a0[98]: Copied! <pre>import matplotlib.pyplot as plt\nfrom scipy.stats import chi\n</pre> import matplotlib.pyplot as plt from scipy.stats import chi <p>Here, we import the <code>pyplot</code> sub-package from the larger <code>matplotlib</code> package and assign it the alias <code>plt</code>. Additionally, we import <code>chi</code> from the <code>stats</code> sub-package of the <code>scipy</code> package.</p> <p>Both packages hold significant importance:</p> <ul> <li><p>Matplotlib is an extensive library enabling the creation of static, animated, and interactive visualizations in Python. It offers a plethora of tools for various types of plots, charts, and graphical representations.</p> </li> <li><p>SciPy encompasses a wide range of scientific computing tools, providing algorithms for optimization, integration, interpolation, solving eigenvalue problems, handling algebraic and differential equations, statistical computations, and more. It's a fundamental package for scientific and technical computing in Python.</p> </li> </ul> In\u00a0[101]: Copied! <pre># Define an array\nx = np.array([0,1,2,3,4])\ny = x**2\nprint (x)\nprint (y)\n\ndef add_2(arr : np.ndarray ) -&gt; np.ndarray:\n    return x + 2\n\nz = add_2(x)\nprint (z)\n</pre> # Define an array x = np.array([0,1,2,3,4]) y = x**2 print (x) print (y)  def add_2(arr : np.ndarray ) -&gt; np.ndarray:     return x + 2  z = add_2(x) print (z) <pre>[0 1 2 3 4]\n[ 0  1  4  9 16]\n[2 3 4 5 6]\n</pre> In\u00a0[105]: Copied! <pre>def subtract_and_add(x : np.array) -&gt; np.array:\n    z = x\n    z += 2 # z = z + 2\n    z -= 5\n    return z\n\n\nx_data = np.arange(0,10,1)\nprint (x_data)\ny_data = subtract_and_add(x_data)\n\n# What happened here!\nprint (x_data)\n\nprint (y_data)\n</pre> def subtract_and_add(x : np.array) -&gt; np.array:     z = x     z += 2 # z = z + 2     z -= 5     return z   x_data = np.arange(0,10,1) print (x_data) y_data = subtract_and_add(x_data)  # What happened here! print (x_data)  print (y_data) <pre>[0 1 2 3 4 5 6 7 8 9]\n[-3 -2 -1  0  1  2  3  4  5  6]\n[-3 -2 -1  0  1  2  3  4  5  6]\n</pre> In\u00a0[106]: Copied! <pre>def subtract_and_add_copy(x):\n    z = x.copy()\n    z += 2 # z = z + 2\n    z -= 5\n    return z\n\n\n\nx_data = np.arange(0,10,1)\nprint (x_data)\ny_data = subtract_and_add_copy(x_data)\n\n# What happened here!\nprint (x_data)\n\nprint (y_data)\n</pre> def subtract_and_add_copy(x):     z = x.copy()     z += 2 # z = z + 2     z -= 5     return z    x_data = np.arange(0,10,1) print (x_data) y_data = subtract_and_add_copy(x_data)  # What happened here! print (x_data)  print (y_data) <pre>[0 1 2 3 4 5 6 7 8 9]\n[0 1 2 3 4 5 6 7 8 9]\n[-3 -2 -1  0  1  2  3  4  5  6]\n</pre> <p>Numpy arrays allow us to filter them using an array mask. We can pass an array of equal size to the array with a binary mask to select the items we want.</p> In\u00a0[110]: Copied! <pre>x = np.array([1,2,3,4])\nx_mask = np.array([True, False, True, True])\nprint (x[x_mask])\n\n# We can select by indexing by the mask\n\n# We can invert the selection using ~\nprint (x[~x_mask])\n</pre> x = np.array([1,2,3,4]) x_mask = np.array([True, False, True, True]) print (x[x_mask])  # We can select by indexing by the mask  # We can invert the selection using ~ print (x[~x_mask])  <pre>[1 3 4]\n[2]\n</pre> In\u00a0[120]: Copied! <pre># We can mask and filter numpy arrays too\nx_data = np.arange(0,10)\n# print (x_data)\n\n# Get the odd numbers greater than 4\n# mask = (x_data &gt; 4) &amp; (x_data % 2 == 0) \nx_data = np.arange(0,10)\n\nmask_gt_4 = x_data &gt; 4\nmask_even = x_data % 2 == 0 \nmask_total = mask_gt_4\nmask_total &amp;= mask_even\nprint (x_data[x_data &gt; 4])\nprint (x_data[mask_even])\nprint (x_data[mask_total])\n</pre> # We can mask and filter numpy arrays too x_data = np.arange(0,10) # print (x_data)  # Get the odd numbers greater than 4 # mask = (x_data &gt; 4) &amp; (x_data % 2 == 0)  x_data = np.arange(0,10)  mask_gt_4 = x_data &gt; 4 mask_even = x_data % 2 == 0  mask_total = mask_gt_4 mask_total &amp;= mask_even print (x_data[x_data &gt; 4]) print (x_data[mask_even]) print (x_data[mask_total])  <pre>[5 6 7 8 9]\n[0 2 4 6 8]\n[6 8]\n</pre> In\u00a0[\u00a0]: Copied! <pre># use numpy's random number generator to get normal random numbers:\nx_rnd = np.random.normal(loc = 0, scale = 1, size = 1000)\ngreat_that_0 = x_rnd &gt; 0\n\n\n# Use plt.hist to create histograms of the values\n</pre> # use numpy's random number generator to get normal random numbers: x_rnd = np.random.normal(loc = 0, scale = 1, size = 1000) great_that_0 = x_rnd &gt; 0   # Use plt.hist to create histograms of the values   In\u00a0[\u00a0]: Copied! <pre># alpha = transparancy of the histogram\n# color = color of the histogram\n# bins = binning to use\n# hatch = fill style for the histogram\n\n# linspace linearly paced numbers\n# min, max, n\nbinning = np.linspace(-5,5, 20)\nplt.hist(x_rnd, bins= binning, \n         alpha = 0.5, color = \"magenta\", label = \"All\", hatch = \"/\")\nplt.hist(x_rnd[great_that_0], bins= binning, \n         alpha = 0.5, color = \"black\", label = \"X&gt;0\", hatch = \"o\")\nplt.hist(x_rnd[~great_that_0], bins= binning, \n         alpha = 0.5, color = \"darkorange\", label = \"$X \\leq 0$\",hatch = \"8\")\nplt.xlabel(\"X Value\")\nplt.ylabel(\"dN/dX\")\nplt.grid()\nplt.legend()\n</pre> # alpha = transparancy of the histogram # color = color of the histogram # bins = binning to use # hatch = fill style for the histogram  # linspace linearly paced numbers # min, max, n binning = np.linspace(-5,5, 20) plt.hist(x_rnd, bins= binning,           alpha = 0.5, color = \"magenta\", label = \"All\", hatch = \"/\") plt.hist(x_rnd[great_that_0], bins= binning,           alpha = 0.5, color = \"black\", label = \"X&gt;0\", hatch = \"o\") plt.hist(x_rnd[~great_that_0], bins= binning,           alpha = 0.5, color = \"darkorange\", label = \"$X \\leq 0$\",hatch = \"8\") plt.xlabel(\"X Value\") plt.ylabel(\"dN/dX\") plt.grid() plt.legend() <p>We can define functions which to operate on numpy arrays</p> In\u00a0[121]: Copied! <pre>def sqrt(x):\n    return np.sqrt(x)\n</pre> def sqrt(x):     return np.sqrt(x) In\u00a0[122]: Copied! <pre>my_values = np.linspace(0,100)\nmy_sqrts = sqrt(my_values)\nprint (my_sqrts[:5])\n</pre> my_values = np.linspace(0,100) my_sqrts = sqrt(my_values) print (my_sqrts[:5]) <pre>[0.         1.42857143 2.02030509 2.4743583  2.85714286]\n</pre> <p>However we do need to be careful on how we write our functions:</p> In\u00a0[123]: Copied! <pre>def capped_sqrt(x):\n    if x &lt; 0:\n        return 0\n    else:\n        return np.sqrt(x)\n</pre> def capped_sqrt(x):     if x &lt; 0:         return 0     else:         return np.sqrt(x) In\u00a0[125]: Copied! <pre>capped_sqrt(9)\n</pre> capped_sqrt(9) Out[125]: <pre>3.0</pre> In\u00a0[126]: Copied! <pre>my_values = np.linspace(-10,10, 100)\nmy_sqrts = capped_sqrt(my_values)\nprint (my_sqrts[:5])\n</pre> my_values = np.linspace(-10,10, 100) my_sqrts = capped_sqrt(my_values) print (my_sqrts[:5]) <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[126], line 2\n      1 my_values = np.linspace(-10,10, 100)\n----&gt; 2 my_sqrts = capped_sqrt(my_values)\n      3 print (my_sqrts[:5])\n\nCell In[123], line 2, in capped_sqrt(x)\n      1 def capped_sqrt(x):\n----&gt; 2     if x &lt; 0:\n      3         return 0\n      4     else:\n\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()</pre> <p>We get around this by vectorizing functions. This allows us to run the function on the entire array without needing to loop over the elements.</p> In\u00a0[131]: Copied! <pre>@np.vectorize\ndef capped_sqrt(x):\n    if x &lt; 0:\n        return 0.\n    else:\n        return np.sqrt(x)\n</pre> @np.vectorize def capped_sqrt(x):     if x &lt; 0:         return 0.     else:         return np.sqrt(x) In\u00a0[132]: Copied! <pre>my_values = np.arange(-1,10)\nmy_sqrts = capped_sqrt(my_values)\nprint (my_values[:5])\nprint (my_sqrts[:5])\n</pre> my_values = np.arange(-1,10) my_sqrts = capped_sqrt(my_values) print (my_values[:5]) print (my_sqrts[:5]) <pre>[-1  0  1  2  3]\n[0.         0.         1.         1.41421356 1.73205081]\n</pre> In\u00a0[133]: Copied! <pre>def my_logger(func):\n    \n    def wrapper(*args, **kwargs):\n        print (f\"Function: {func.__name__} called with: \\n\\t args: {args} \\n\\t kwargs {kwargs}\")\n        ret = func(*args,  **kwargs)\n        print (f\"Return: {ret}\")\n        return ret\n   \n    return wrapper\n</pre> def my_logger(func):          def wrapper(*args, **kwargs):         print (f\"Function: {func.__name__} called with: \\n\\t args: {args} \\n\\t kwargs {kwargs}\")         ret = func(*args,  **kwargs)         print (f\"Return: {ret}\")         return ret         return wrapper In\u00a0[136]: Copied! <pre>@my_logger\ndef sqrt(x, is_capped = False):\n    if is_capped:\n        if x &gt; 0:\n            return np.sqrt(x)\n        else :\n            return 0\n    return np.sqrt(x)\n</pre> @my_logger def sqrt(x, is_capped = False):     if is_capped:         if x &gt; 0:             return np.sqrt(x)         else :             return 0     return np.sqrt(x) In\u00a0[138]: Copied! <pre>sqrt(4, is_capped = True)\n</pre> sqrt(4, is_capped = True) <pre>Function: sqrt called with: \n\t args: (4,) \n\t kwargs {'is_capped': True}\nReturn: 2.0\n</pre> Out[138]: <pre>2.0</pre> In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n\n# Creating a Pandas DataFrame\ndata = {\n\n}\n\ndf = pd.DataFrame(data)\nprint(\"Pandas DataFrame:\")\nprint(df)\n</pre> import pandas as pd  # Creating a Pandas DataFrame data = {  }  df = pd.DataFrame(data) print(\"Pandas DataFrame:\") print(df) In\u00a0[\u00a0]: Copied! <pre>help(np.random.randn)\n</pre> help(np.random.randn) In\u00a0[\u00a0]: Copied! <pre># Accessing the underlying NumPy array of column 'A'\nnumpy_array = df['A'].values\nprint(\"Numpy array from Pandas DataFrame:\")\nprint(numpy_array)\n</pre> # Accessing the underlying NumPy array of column 'A' numpy_array = df['A'].values print(\"Numpy array from Pandas DataFrame:\") print(numpy_array)  In\u00a0[\u00a0]: Copied! <pre># Loading a csv file using pandas\nurl=\"https://r2.datahub.io/clt98lqg6000el708ja5zbtz0/master/raw/data/monthly.csv\"\n</pre> # Loading a csv file using pandas url=\"https://r2.datahub.io/clt98lqg6000el708ja5zbtz0/master/raw/data/monthly.csv\"  In\u00a0[\u00a0]: Copied! <pre>df.tail()\n</pre> df.tail() In\u00a0[\u00a0]: Copied! <pre>df.plot(x = \"Date\", y = \"Mean\")\n</pre> df.plot(x = \"Date\", y = \"Mean\") In\u00a0[140]: Copied! <pre>help(np.random.random)\n</pre> help(np.random.random) <pre>Help on built-in function random:\n\nrandom(...) method of numpy.random.mtrand.RandomState instance\n    random(size=None)\n    \n    Return random floats in the half-open interval [0.0, 1.0). Alias for\n    `random_sample` to ease forward-porting to the new random API.\n\n</pre> In\u00a0[141]: Copied! <pre># Let define the true model\ndef model(x, p0, p1, p2):\n    # ax^2 + bx + c\n    return p0*x**2 + p1 * x + p2\n\n# Set the true parameters\np_true = [0.02, 0.1, -2.5]\n\n# Let the x points be random floats between 0-10\nx = 10 * np.random.random(size = 100)\ny = model(x, *p_true)\n\n# let's add some gaussian noise\ny_noisey = y + np.random.normal(loc = 0, scale = 0.2, size = 100)\n# define our y error as 0.2\ny_err = 0.2 * np.ones(x.shape)\n\n# Plot the data\nplt.errorbar(x, y_noisey, yerr = y_err , fmt = \"C0o\", label = \"Measured\")\nplt.ylabel(\"Y Values\")\nplt.xlabel(\"X Values\")\nplt.grid()\n</pre> # Let define the true model def model(x, p0, p1, p2):     # ax^2 + bx + c     return p0*x**2 + p1 * x + p2  # Set the true parameters p_true = [0.02, 0.1, -2.5]  # Let the x points be random floats between 0-10 x = 10 * np.random.random(size = 100) y = model(x, *p_true)  # let's add some gaussian noise y_noisey = y + np.random.normal(loc = 0, scale = 0.2, size = 100) # define our y error as 0.2 y_err = 0.2 * np.ones(x.shape)  # Plot the data plt.errorbar(x, y_noisey, yerr = y_err , fmt = \"C0o\", label = \"Measured\") plt.ylabel(\"Y Values\") plt.xlabel(\"X Values\") plt.grid()  <p>Let's use <code>scipy.optimize.curve_fit</code></p> <p><code>curve_fit</code> will perform a lease-squares minimization: $$ (\\vec{y} - y_{model}(\\vec{x}, \\theta))^2 $$</p> <p>If errors are provided than it will perform a $\\chi^2$-minimization $$ \\frac{(\\vec{y} - y_{model}(\\vec{x}, \\theta))^2}{\\vec{\\Delta y}^2} $$</p> <p><code>curve_fit</code> returns the optimal parameters and the correlation matrix for the minimization allowing us to easily extract an uncertainty.</p> In\u00a0[144]: Copied! <pre># Use scipy curve_fit to perform a fit\nfrom scipy.optimize import curve_fit\n\n# Returns optimal (popt) and correation matrix (pcov)\npopt, pcov = curve_fit(\n    model, # Function we want to fit\n    x,     # x data\n    y_noisey,  # y data\n    p0 = [-0.2, 1, 5],  # initial guess\n    sigma=y_err   # error on y\n)\n\nx_plot = np.linspace(0,10)\nplt.errorbar(x, y_noisey, yerr = y_err , fmt = \"C0o\", label = \"Measured\")\nplt.plot(x_plot, model(x_plot, *popt), \"r--\", label = \"Best fit\")\nplt.ylabel(\"Y Values\")\nplt.xlabel(\"X Values\")\nplt.legend()\nplt.grid()\n\nparameter_errors = np.sqrt(np.diag(pcov))\nprint (p_true)\nprint (popt)\nprint (parameter_errors)\n</pre> # Use scipy curve_fit to perform a fit from scipy.optimize import curve_fit  # Returns optimal (popt) and correation matrix (pcov) popt, pcov = curve_fit(     model, # Function we want to fit     x,     # x data     y_noisey,  # y data     p0 = [-0.2, 1, 5],  # initial guess     sigma=y_err   # error on y )  x_plot = np.linspace(0,10) plt.errorbar(x, y_noisey, yerr = y_err , fmt = \"C0o\", label = \"Measured\") plt.plot(x_plot, model(x_plot, *popt), \"r--\", label = \"Best fit\") plt.ylabel(\"Y Values\") plt.xlabel(\"X Values\") plt.legend() plt.grid()  parameter_errors = np.sqrt(np.diag(pcov)) print (p_true) print (popt) print (parameter_errors) <pre>[0.02, 0.1, -2.5]\n[ 0.02336569  0.07350938 -2.46847516]\n[0.00277373 0.02722641 0.05462685]\n</pre> In\u00a0[146]: Copied! <pre># bootstrapping\nsamples = []\n\nfor i in range(100):\n    # Get random indices\n    # replace = True allows us to reuse indicies\n    # So we could be drawing an estimate from the [0th, 11th, 81st, 0th] elements of our array\n    rnd_int = np.random.choice(np.arange(len(x)), size=len(x), replace=True)\n    # Extract the corresponding values\n    x_samp = x[rnd_int]\n    y_samp = y_noisey[rnd_int]\n    y_samp_err = y_err[rnd_int]\n\n    # Apply fit\n    p, _ = curve_fit(model, x_samp, y_samp, sigma = y_samp_err)\n    # Store Result\n    samples.append(p)\nsamples = np.array(samples)\n</pre>  # bootstrapping samples = []  for i in range(100):     # Get random indices     # replace = True allows us to reuse indicies     # So we could be drawing an estimate from the [0th, 11th, 81st, 0th] elements of our array     rnd_int = np.random.choice(np.arange(len(x)), size=len(x), replace=True)     # Extract the corresponding values     x_samp = x[rnd_int]     y_samp = y_noisey[rnd_int]     y_samp_err = y_err[rnd_int]      # Apply fit     p, _ = curve_fit(model, x_samp, y_samp, sigma = y_samp_err)     # Store Result     samples.append(p) samples = np.array(samples) In\u00a0[147]: Copied! <pre>fig, axs = plt.subplots(1,3, figsize = (18,6))\nfor i in range(3):\n    mean = np.mean(samples[:,i])\n    std = np.std(samples[:,i])\n    \n    axs[i].hist(samples[:,i], alpha = 0.5)\n    axs[i].axvline(popt[i], color = \"r\", label = \"From Fit\")\n    axs[i].axvline(popt[i] - parameter_errors[i], ls = \"--\", color = \"r\")\n    axs[i].axvline(popt[i] + parameter_errors[i], ls = \"--\", color = \"r\")\n\n    axs[i].axvline(mean, color = \"C4\", label = \"Bootstrap\")\n    axs[i].axvline(mean - std, ls = \"--\", color = \"C4\")\n    axs[i].axvline(mean + std, ls = \"--\", color = \"C4\")\n    axs[i].axvline(p_true[i], color = \"k\", label = \"True\")\n    axs[i].grid()\n    axs[i].legend()\n</pre> fig, axs = plt.subplots(1,3, figsize = (18,6)) for i in range(3):     mean = np.mean(samples[:,i])     std = np.std(samples[:,i])          axs[i].hist(samples[:,i], alpha = 0.5)     axs[i].axvline(popt[i], color = \"r\", label = \"From Fit\")     axs[i].axvline(popt[i] - parameter_errors[i], ls = \"--\", color = \"r\")     axs[i].axvline(popt[i] + parameter_errors[i], ls = \"--\", color = \"r\")      axs[i].axvline(mean, color = \"C4\", label = \"Bootstrap\")     axs[i].axvline(mean - std, ls = \"--\", color = \"C4\")     axs[i].axvline(mean + std, ls = \"--\", color = \"C4\")     axs[i].axvline(p_true[i], color = \"k\", label = \"True\")     axs[i].grid()     axs[i].legend()      In\u00a0[\u00a0]: Copied! <pre>### What if we under estimate our errors?\n</pre> ### What if we under estimate our errors? In\u00a0[156]: Copied! <pre># Let the x points be random floats between 0-10\nx = 10 * np.random.random(size = 100)\ny = model(x, *p_true)\n\n# let's add some gaussian noise\ny_noisey = y + np.random.normal(loc = 0, scale = 0.3, size = 100)\n# define our y error as 0.2\ny_err = 0.1 * np.ones(x.shape)\n</pre>   # Let the x points be random floats between 0-10 x = 10 * np.random.random(size = 100) y = model(x, *p_true)  # let's add some gaussian noise y_noisey = y + np.random.normal(loc = 0, scale = 0.3, size = 100) # define our y error as 0.2 y_err = 0.1 * np.ones(x.shape)  In\u00a0[157]: Copied! <pre># Returns optimal (popt) and correation matrix (pcov)\npopt, pcov = curve_fit(\n    model, # Function we want to fit\n    x,     # x data\n    y_noisey,  # y data\n    p0 = [-0.2, 1, 5],  # initial guess\n    sigma=y_err   # error on y\n)\n\nx_plot = np.linspace(0,10)\nplt.errorbar(x, y_noisey, yerr = y_err , fmt = \"C0o\", label = \"Measured\")\nplt.plot(x_plot, model(x_plot, *popt), \"r--\", label = \"Best fit\")\nplt.ylabel(\"Y Values\")\nplt.xlabel(\"X Values\")\nplt.legend()\nplt.grid()\n</pre> # Returns optimal (popt) and correation matrix (pcov) popt, pcov = curve_fit(     model, # Function we want to fit     x,     # x data     y_noisey,  # y data     p0 = [-0.2, 1, 5],  # initial guess     sigma=y_err   # error on y )  x_plot = np.linspace(0,10) plt.errorbar(x, y_noisey, yerr = y_err , fmt = \"C0o\", label = \"Measured\") plt.plot(x_plot, model(x_plot, *popt), \"r--\", label = \"Best fit\") plt.ylabel(\"Y Values\") plt.xlabel(\"X Values\") plt.legend() plt.grid()  In\u00a0[158]: Copied! <pre># bootstrapping\nsamples = []\nfor i in range(100):\n    rnd_int = np.random.choice(np.arange(len(x)), size=len(x), replace=True)\n    x_samp = x[rnd_int]\n    y_samp = y_noisey[rnd_int]\n    y_samp_err = y_err[rnd_int]\n\n    p, _ = curve_fit(model, x_samp, y_samp, sigma = y_samp_err)\n    samples.append(p)\nsamples = np.array(samples)\n</pre> # bootstrapping samples = [] for i in range(100):     rnd_int = np.random.choice(np.arange(len(x)), size=len(x), replace=True)     x_samp = x[rnd_int]     y_samp = y_noisey[rnd_int]     y_samp_err = y_err[rnd_int]      p, _ = curve_fit(model, x_samp, y_samp, sigma = y_samp_err)     samples.append(p) samples = np.array(samples) In\u00a0[159]: Copied! <pre>fig, axs = plt.subplots(1,3, figsize = (18,6))\nfor i in range(3):\n    mean = np.mean(samples[:,i])\n    std = np.std(samples[:,i])\n    \n    axs[i].hist(samples[:,i], alpha = 0.5)\n    axs[i].axvline(popt[i], color = \"r\", label = \"From Fit\")\n    axs[i].axvline(popt[i] - parameter_errors[i], ls = \"--\", color = \"r\")\n    axs[i].axvline(popt[i] + parameter_errors[i], ls = \"--\", color = \"r\")\n\n    axs[i].axvline(mean, color = \"C4\", label = \"Bootstrap\")\n    axs[i].axvline(mean - std, ls = \"--\", color = \"C4\")\n    axs[i].axvline(mean + std, ls = \"--\", color = \"C4\")\n    axs[i].axvline(p_true[i], color = \"k\", label = \"True\")\n    axs[i].grid()\n    axs[i].legend()\n</pre> fig, axs = plt.subplots(1,3, figsize = (18,6)) for i in range(3):     mean = np.mean(samples[:,i])     std = np.std(samples[:,i])          axs[i].hist(samples[:,i], alpha = 0.5)     axs[i].axvline(popt[i], color = \"r\", label = \"From Fit\")     axs[i].axvline(popt[i] - parameter_errors[i], ls = \"--\", color = \"r\")     axs[i].axvline(popt[i] + parameter_errors[i], ls = \"--\", color = \"r\")      axs[i].axvline(mean, color = \"C4\", label = \"Bootstrap\")     axs[i].axvline(mean - std, ls = \"--\", color = \"C4\")     axs[i].axvline(mean + std, ls = \"--\", color = \"C4\")     axs[i].axvline(p_true[i], color = \"k\", label = \"True\")     axs[i].grid()     axs[i].legend()      In\u00a0[145]: Copied! <pre>def exp_model(x, n, tau):\n    return n*x**-tau\n</pre> def exp_model(x, n, tau):     return n*x**-tau      In\u00a0[\u00a0]: Copied! <pre>p_true = [10, 0.5]\nx_plot = np.linspace(0,10)\nplt.plot(x_plot, exp_model(x_plot, *p_true))\n</pre> p_true = [10, 0.5] x_plot = np.linspace(0,10) plt.plot(x_plot, exp_model(x_plot, *p_true)) In\u00a0[\u00a0]: Copied! <pre>lam = np.arange(6)\nfig, axs = plt.subplots(2,3, figsize = (11,6))\n\nfor l, ax in zip(lam, axs.ravel()):\n    rnd_x = np.random.poisson(lam = l, size = 1000)\n\n    ax.grid()\nfig.tight_layout()\n</pre> lam = np.arange(6) fig, axs = plt.subplots(2,3, figsize = (11,6))  for l, ax in zip(lam, axs.ravel()):     rnd_x = np.random.poisson(lam = l, size = 1000)      ax.grid() fig.tight_layout()      In\u00a0[\u00a0]: Copied! <pre>x = 10*np.random.random(100)\ny = np.random.poisson(lam = exp_model(x, *p_true))\n</pre> x = 10*np.random.random(100) y = np.random.poisson(lam = exp_model(x, *p_true))       In\u00a0[\u00a0]: Copied! <pre>x_plot = np.linspace(0,10)\ny_err = np.sqrt(y)\npopt, pcov = curve_fit(exp_model, x, y)\n\nplt.plot(x_plot, exp_model(x_plot, *p_true))\nplt.errorbar(x, y, yerr = y_err, fmt = \"C0o\")\nplt.plot(x_plot, exp_model(x_plot, *popt))\nplt.grid()\n\nparameter_errors =\n</pre> x_plot = np.linspace(0,10) y_err = np.sqrt(y) popt, pcov = curve_fit(exp_model, x, y)  plt.plot(x_plot, exp_model(x_plot, *p_true)) plt.errorbar(x, y, yerr = y_err, fmt = \"C0o\") plt.plot(x_plot, exp_model(x_plot, *popt)) plt.grid()  parameter_errors =    In\u00a0[\u00a0]: Copied! <pre># bootstrapping\nsamples = []\nfor i in range(100):\n    rnd_int = np.random.choice(np.arange(len(x)), size=len(x), replace=True)\n    x_samp = x[rnd_int]\n    y_samp = y[rnd_int]\n    y_samp_err = y_err[rnd_int]\n\n    p, _ = curve_fit(exp_model, x_samp, y_samp)\n    samples.append(p)\nsamples = np.array(samples)\n</pre> # bootstrapping samples = [] for i in range(100):     rnd_int = np.random.choice(np.arange(len(x)), size=len(x), replace=True)     x_samp = x[rnd_int]     y_samp = y[rnd_int]     y_samp_err = y_err[rnd_int]      p, _ = curve_fit(exp_model, x_samp, y_samp)     samples.append(p) samples = np.array(samples) In\u00a0[\u00a0]: Copied! <pre>fig, axs = plt.subplots(1,2, figsize = (18,6))\nfor i in range(2):\n    mean = np.mean(samples[:,i])\n    std = np.std(samples[:,i])\n    \n    axs[i].hist(samples[:,i], alpha = 0.5)\n    axs[i].axvline(popt[i], color = \"r\", label = \"From Fit\")\n    axs[i].axvline(popt[i] - parameter_errors[i], ls = \"--\", color = \"r\")\n    axs[i].axvline(popt[i] + parameter_errors[i], ls = \"--\", color = \"r\")\n\n    axs[i].axvline(mean, color = \"C4\", label = \"Bootstrap\")\n    axs[i].axvline(mean - std, ls = \"--\", color = \"C4\")\n    axs[i].axvline(mean + std, ls = \"--\", color = \"C4\")\n    axs[i].axvline(p_true[i], color = \"k\", label = \"True\")\n    axs[i].grid()\n    axs[i].legend()\n</pre> fig, axs = plt.subplots(1,2, figsize = (18,6)) for i in range(2):     mean = np.mean(samples[:,i])     std = np.std(samples[:,i])          axs[i].hist(samples[:,i], alpha = 0.5)     axs[i].axvline(popt[i], color = \"r\", label = \"From Fit\")     axs[i].axvline(popt[i] - parameter_errors[i], ls = \"--\", color = \"r\")     axs[i].axvline(popt[i] + parameter_errors[i], ls = \"--\", color = \"r\")      axs[i].axvline(mean, color = \"C4\", label = \"Bootstrap\")     axs[i].axvline(mean - std, ls = \"--\", color = \"C4\")     axs[i].axvline(mean + std, ls = \"--\", color = \"C4\")     axs[i].axvline(p_true[i], color = \"k\", label = \"True\")     axs[i].grid()     axs[i].legend()      <p>We can see that the bootstrapped distributions are highly non-gaussian. It might not make sense to report the uncertainty as 1 sigma error on the fit parameters. Instead we might report using the bootstrapped quantiles. A common way to represent the uncertinty would be to report the 90% confidience/credibility interval. This says that:</p> <ul> <li>If we were to repeat this experiement 100 times, the measured value would be in this interval 90% of the time</li> </ul> In\u00a0[\u00a0]: Copied! <pre>fig, axs = plt.subplots(1,2, figsize = (18,6))\n\n\nfor i, pt in enumerate(p_true):\n    \n    axs[i].hist(samples[:,i], alpha = 0.5)\n    axs[i].axvline(popt[i], color = \"r\", label = \"From Fit\")\n    axs[i].axvline(popt[i] - parameter_errors[i], ls = \"--\", color = \"r\")\n    axs[i].axvline(popt[i] + parameter_errors[i], ls = \"--\", color = \"r\")\n\n    quan = np.quantile(samples[:,i], [0.05, 0.5, 0.95])\n\n    \n    axs[i].axvline(quan[1], color = \"C4\", label = \"Bootstrap\")\n    axs[i].axvline(quan[0], ls = \"--\", color = \"C4\")\n    axs[i].axvline(quan[2], ls = \"--\", color = \"C4\")\n    axs[i].axvline(p_true[i], color = \"k\", label = \"True\")\n    axs[i].grid()\n    axs[i].legend()\n    \n    print (f\"{pt:0.3f} -&gt; {quan[1]:0.3f} [{quan[0]:0.3f}, {quan[2]:0.3f}]\")\n</pre> fig, axs = plt.subplots(1,2, figsize = (18,6))   for i, pt in enumerate(p_true):          axs[i].hist(samples[:,i], alpha = 0.5)     axs[i].axvline(popt[i], color = \"r\", label = \"From Fit\")     axs[i].axvline(popt[i] - parameter_errors[i], ls = \"--\", color = \"r\")     axs[i].axvline(popt[i] + parameter_errors[i], ls = \"--\", color = \"r\")      quan = np.quantile(samples[:,i], [0.05, 0.5, 0.95])           axs[i].axvline(quan[1], color = \"C4\", label = \"Bootstrap\")     axs[i].axvline(quan[0], ls = \"--\", color = \"C4\")     axs[i].axvline(quan[2], ls = \"--\", color = \"C4\")     axs[i].axvline(p_true[i], color = \"k\", label = \"True\")     axs[i].grid()     axs[i].legend()          print (f\"{pt:0.3f} -&gt; {quan[1]:0.3f} [{quan[0]:0.3f}, {quan[2]:0.3f}]\") In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>class Data():\n    \"\"\"Data Class\n\n    Class for holding x/y data with methods to calculate the properties \n    and some plotting functionalities. \n    \"\"\"\n    # The \"self\" keyword denotes data belonging to the class\n    def __init__(self, x_data : np.ndarray, y_data : np.ndarray) -&gt; None:\n        \"\"\"Initialization function\n\n        Copy x_data and y_data\n\n        Args:\n            x_data : data on the x axis\n            y_data : data on the y axis\n\n        Returns:\n            None\n        \"\"\"\n\n    # Member functions take \"self\" as the first argument\n    def calculate_properties(self) -&gt; None:\n        \"\"\"Calculate properties of X and Y data\n\n        Determine the mean and standard deviation of x_data and y_data.\n        Mean and standard deviation are stored as attributes within Data class\n\n        Args:\n            None\n\n        Returns:\n            None\n        \"\"\"\n\n\n    def plot_data(self, x_label : str = None, y_label : str = None) -&gt; plt.figure:\n        \"\"\"Plot X and Y data\n\n        Plot the X and Y data and return the figure. Add optional x/y labels.\n        Lines are added for the mean x/y and their standard deviations.\n\n        Args:\n            x_label : (optional) string for the label of the x axis. (Default None)\n            y_label : (optional) string for the label of the y axis. (Default None)\n\n        Returns:\n            figure with the plot of y(x)\n        \"\"\"\n\n\n        return fig\n</pre> class Data():     \"\"\"Data Class      Class for holding x/y data with methods to calculate the properties      and some plotting functionalities.      \"\"\"     # The \"self\" keyword denotes data belonging to the class     def __init__(self, x_data : np.ndarray, y_data : np.ndarray) -&gt; None:         \"\"\"Initialization function          Copy x_data and y_data          Args:             x_data : data on the x axis             y_data : data on the y axis          Returns:             None         \"\"\"      # Member functions take \"self\" as the first argument     def calculate_properties(self) -&gt; None:         \"\"\"Calculate properties of X and Y data          Determine the mean and standard deviation of x_data and y_data.         Mean and standard deviation are stored as attributes within Data class          Args:             None          Returns:             None         \"\"\"       def plot_data(self, x_label : str = None, y_label : str = None) -&gt; plt.figure:         \"\"\"Plot X and Y data          Plot the X and Y data and return the figure. Add optional x/y labels.         Lines are added for the mean x/y and their standard deviations.          Args:             x_label : (optional) string for the label of the x axis. (Default None)             y_label : (optional) string for the label of the y axis. (Default None)          Returns:             figure with the plot of y(x)         \"\"\"           return fig                  In\u00a0[\u00a0]: Copied! <pre>x = np.linspace(-3 * np.pi, 3 * np.pi, 100 )\ny = np.sin(x)\n\nmy_data = Data(x, y)\nmy_data.calculate_properties()\nfig = my_data.plot_data(y_label=\"Sin(x)\")\n</pre> x = np.linspace(-3 * np.pi, 3 * np.pi, 100 ) y = np.sin(x)  my_data = Data(x, y) my_data.calculate_properties() fig = my_data.plot_data(y_label=\"Sin(x)\")  In\u00a0[\u00a0]: Copied! <pre>class TimeSeries(Data):\n    \"\"\"Time Series Data Class\n\n    Class for holding x/y data with methods to calculate the properties \n    and some plotting functionalities. \n    The X data is assumed to be time\n    \"\"\"\n    def __init__(self, x_data : np.ndarray, y_data: np.ndarray) -&gt; None:\n        \"\"\"Initialization function\n\n        Copy x_data and y_data\n\n        Args:\n            x_data : data on the x axis\n            y_data : data on the y axis\n\n        Returns:\n            None\n        \"\"\"\n        # We use the \"super\" keyword to call parent class functions\n\n    # We can overwrite functions\n    def plot_data(self) -&gt; plt.figure:\n        \"\"\"Plot X and Y data\n\n        Plot the X and Y data and return the figure. Add optional x/y labels.\n        Lines are added for the mean x/y and their standard deviations.\n\n        Args:\n            None\n\n        Returns:\n            figure with the plot of y(x)\n        \"\"\"\n        return super().plot_data(x_label = \"Time\", y_label = \"AU\")\n    \n    # We can also define new functions\n    def add_to_data(self, y : float) -&gt; None:\n        \"\"\"Add a constant offset to y data.\n\n        A constant offset is added to self.y_data. \n        The properties (mean and std) are calculated for the adjusted dataset\n\n        Args:\n            y : constant offset to be added to self.y_data\n\n        Returns:\n            None\n        \"\"\"\n</pre> class TimeSeries(Data):     \"\"\"Time Series Data Class      Class for holding x/y data with methods to calculate the properties      and some plotting functionalities.      The X data is assumed to be time     \"\"\"     def __init__(self, x_data : np.ndarray, y_data: np.ndarray) -&gt; None:         \"\"\"Initialization function          Copy x_data and y_data          Args:             x_data : data on the x axis             y_data : data on the y axis          Returns:             None         \"\"\"         # We use the \"super\" keyword to call parent class functions      # We can overwrite functions     def plot_data(self) -&gt; plt.figure:         \"\"\"Plot X and Y data          Plot the X and Y data and return the figure. Add optional x/y labels.         Lines are added for the mean x/y and their standard deviations.          Args:             None          Returns:             figure with the plot of y(x)         \"\"\"         return super().plot_data(x_label = \"Time\", y_label = \"AU\")          # We can also define new functions     def add_to_data(self, y : float) -&gt; None:         \"\"\"Add a constant offset to y data.          A constant offset is added to self.y_data.          The properties (mean and std) are calculated for the adjusted dataset          Args:             y : constant offset to be added to self.y_data          Returns:             None         \"\"\"      In\u00a0[\u00a0]: Copied! <pre>my_time_series = TimeSeries(x, y)\n# Calling a function from the parent class\nmy_time_series.calculate_properties()\n# Call a function that only exists in the child class\nmy_time_series.add_to_data(10)\n# Call overridden function\nfig = my_time_series.plot_data()\n</pre> my_time_series = TimeSeries(x, y) # Calling a function from the parent class my_time_series.calculate_properties() # Call a function that only exists in the child class my_time_series.add_to_data(10) # Call overridden function fig = my_time_series.plot_data() <p></p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Python/python-tutorial-empty/#what-is-python","title":"What is Python?\u00b6","text":"<p>Python is one of the most widely used programming languages today. It is a high-level, interpreted programming language that emphasizes readability. Python's readability makes it often the language of choice for both those beginning to learn programming and large collaborative projects.</p> <p>Unlike compiled languages like C++ and Rust, Python is an interpreted language. This means that the Python interpreter will compile the code at runtime, rather than compiling it down to a binary prior to running. This has the benefit of making Python an excellent language for quickly developing and debugging code.</p> <p>Interactive environments like Jupyter Notebooks allow for clear frameworks for developing, testing, sharing, and presenting code.</p>"},{"location":"Python/python-tutorial-empty/#installing-python-and-working-with-environments","title":"Installing Python and working with Environments\u00b6","text":"<p>We will use virtual environments in this tutorial. I recommend that you use an environment manager such as conda or mamba/micromamba.</p> <p>Mamba/Micromamba is a \"fast, robust, and cross-platform package manager\" that offers significant performance advantages over conda when installing and resolving packages.</p> <p>We can create a new environment using the following command:</p> <pre><code>mamba/conda create -n my_environment python=3.9 numpy matplotlib\n</code></pre> <p>Here, we are creating a new environment called <code>my_environment</code>, which installs <code>Python 3.9</code> and the packages <code>numpy</code> and <code>matplotlib</code>. We can activate this environment using:</p> <pre><code>mamba/conda activate my_environment\n</code></pre> <p>Running <code>which python</code> confirms that we are utilizing the Python installed within our environment.</p> <p>Environments enable us to install conflicting versions for various projects. For instance, suppose we need to execute older code dependent on Python 2.7, which is incompatible with modern packages. In that scenario, we can establish an environment with Python 2.7 and install versions of packages compatible with it. This action won't impact any other environment we've established.</p> <p>For this workshop, we will utilize the following environment:</p> <pre><code>mamba/conda create -n workshop -c conda-forge python=3.10 numpy matplotlib scipy pandas jupyter jupyterlab ipykernel\n</code></pre>"},{"location":"Python/python-tutorial-empty/#installing-additional-packages","title":"Installing additional packages\u00b6","text":"<p>Once in the environment, we can install additional packages using the <code>install</code> command:</p> <pre><code>mamba/conda install scipy\n</code></pre> <p>This installs the package <code>scipy</code>, which is a statistics package compatible with <code>numpy</code> data types. We can also remove a package using:</p> <pre><code>mamba/conda remove scipy\n</code></pre> <p>which would remove the package <code>scipy</code>. Some packages will require installation from a specific collection of packages:</p> <pre><code>mamba/conda install -c conda-forge astroquery\n</code></pre> <p>This installs the package <code>astroquery</code>, a package that allows querying astronomical databases like Simbad, which is part of the collection <code>conda-forge</code>.</p> <p>If we ever want to see which packages are currently installed, we can use something like:</p> <pre><code>mamba/conda list\n</code></pre> <p>which gives a list of installed packages and their versions. We can output this to a machine-readable file using:</p> <pre><code>mamba/conda list -e &gt; requirements.txt\n</code></pre> <p>Ensuring that users are using a standard environment can help debug version-specific bugs.</p>"},{"location":"Python/python-tutorial-empty/#hello-world","title":"Hello World\u00b6","text":"<p>Python is a dynamically typed language, which means that the type of a variable does not need to be known until that variable is used. This also means that we can change the type of a variable at any stage of the code.</p> <p>We can define variables like:</p> <pre>my_string = \"Hello\"\n</pre> <p>We can also overwrite variables like:</p>"},{"location":"Python/python-tutorial-empty/#basic-operations","title":"Basic Operations\u00b6","text":"<ul> <li>Addition: a + b</li> <li>Multiplication: a * b</li> <li>Division: a / b</li> <li>Integer division: a // b</li> <li>Modulus: a % b</li> <li>Power: a ** b</li> <li>Equal: a == b</li> <li>Not equal: a != b</li> <li>Less than: a &lt; b</li> <li>Less than or equal to: a &lt;= b</li> <li>Greater than: a &gt; b</li> <li>Greater than or equal to: a &gt;= b</li> </ul>"},{"location":"Python/python-tutorial-empty/#other-logical-statements","title":"Other logical statements:\u00b6","text":"<ul> <li>Or:<ul> <li>a or b</li> <li>a | b</li> </ul> </li> <li>And:<ul> <li>a and b</li> <li>a &amp; b</li> </ul> </li> </ul> <p>For example, if a multiplied by b is less than c divided by d, and e is greater than 10:</p> <pre><code>(a * b &lt; c / d) and (e &gt; 10)\n</code></pre>"},{"location":"Python/python-tutorial-empty/#basic-data-types","title":"Basic Data Types\u00b6","text":"<p>Python has several basic data types:</p> <ul> <li><code>int</code>: integers: -3, -2, -1, 0, 1, 2, 3, etc.</li> <li><code>float</code>: non-integers: 3.14, 42.0, etc.</li> <li><code>bool</code>: boolean. Note in Python, <code>True</code>/<code>False</code> start with a capital letter.<ul> <li><code>x = false</code> will give an error, while <code>x = False</code> will not.</li> </ul> </li> <li><code>str</code>: strings of characters. In Python, strings are wrapped in single (<code>''</code>) or double (<code>\"\"</code>) quotation marks. They can be combined when using strings:<ul> <li><code>my_str = \"hello\"</code>, <code>my_str = 'apple'</code>, <code>answer = 'Computer says \"no\"'</code> - all of these will work just fine.</li> <li><code>my_str = \"Goodbye'</code> will not work since we need to match the quotation marks properly.</li> </ul> </li> </ul> <p>We can cast from one data type to another using the format:</p> <pre>x = 1.3\ny = int(x)\n</pre> <p>Here, <code>x</code> is cast to the <code>int</code> type <code>y</code>. We can also determine the type of a variable using the type function:</p> <pre>type(x)\n</pre>"},{"location":"Python/python-tutorial-empty/#basic-collections-of-data","title":"Basic Collections of Data\u00b6","text":"<p>Python offers several ways to organize and store data efficiently. These data structures play a vital role in managing and manipulating information within a program.</p>"},{"location":"Python/python-tutorial-empty/#lists","title":"Lists\u00b6","text":"<p>A <code>list</code> in Python is a versatile and mutable collection of items, ordered and enclosed within square brackets <code>[]</code>. It allows storing various data types, including integers, strings, or even other lists. Lists are dynamic, meaning elements can be added, removed, or changed after creation using methods like <code>append()</code>, <code>insert()</code>, <code>remove()</code>, or by directly assigning values to specific indices.</p> <pre>my_list = [1, 2, 3, 'apple', 'banana', 'cherry']\n</pre>"},{"location":"Python/python-tutorial-empty/#string-slicing","title":"String slicing\u00b6","text":"<p>In Python we can slice lists (and arrays, more on this later) to access sub sections of the list. We use the syntax:</p> <pre>my_list[start:stop]\n</pre> <p>where <code>start</code> and <code>stop</code> are the range that we want to access, with <code>stop</code> being exclusive. We can also access the last element with:</p> <pre>my_list[-1]\n</pre> <p>with <code>-1</code> being the last element (<code>-2</code> being the second last... etc.). To slice from the 2 element to the second last we would do:</p> <pre>my_list[2:-2]\n</pre>"},{"location":"Python/python-tutorial-empty/#sets","title":"Sets\u00b6","text":"<p><code>Sets</code> are useful collection in Python. They are an unordered and mutable collection of unique elements. Sets are enclosed in curly brakets <code>{}</code> and support set operations like union, intersection, and difference. They are efficient for tasks requiring unique elements and membership testing.</p>"},{"location":"Python/python-tutorial-empty/#tuples","title":"Tuples\u00b6","text":"<p>A <code>tuple</code> is similar to a list but is immutable once created, denoted by parentheses <code>()</code>. Tuples are often used to store related pieces of information together and are faster than lists due to their immutability. They are commonly utilized for items that shouldn't be changed, such as coordinates or configuration settings.</p> <pre>my_tuple = (4, 5, 6, 'dog', 'cat', 'rabbit')\n</pre>"},{"location":"Python/python-tutorial-empty/#dictionaries","title":"Dictionaries\u00b6","text":"<p>A <code>dictionary</code> is an unordered collection of key-value pairs enclosed in curly brackets <code>{}</code>. Each element in a dictionary is accessed by its associated key rather than an index. Dictionaries are suitable for storing data where retrieval by a specific key is a priority. They are flexible and allow storing various data types as values.</p> <pre>my_dict = {'name': 'Alice', 'age': 25, 'country': 'USA'}\n</pre>"},{"location":"Python/python-tutorial-empty/#looping","title":"Looping\u00b6","text":"<p>In Python, there are two primary methods of looping: <code>for</code> loops and <code>while</code> loops.</p>"},{"location":"Python/python-tutorial-empty/#for-loops","title":"For Loops\u00b6","text":"<p><code>for</code> loops use the syntax <code>for variable in iterable</code>, where <code>iterable</code> is some sequence-like object, and <code>variable</code> represents the current instance within the loop. The block of code to be executed within the loop is designated by indentation. Python's standard is to use 4 spaces for indentation, but using tabs (consistently) is also common (avoid mixing spaces and tabs). For example:</p> <pre>my_list = [1, 2, 3, 4, 5]\nfor num in my_list:\n    print(num)\n</pre> <p>This loop iterates through the elements of my_list, assigning each element to the variable num, and then prints each element.</p> <p>The <code>range()</code> function is often used with <code>for</code> loops to generate a sequence of numbers. It allows iterating a specific number of times or generating a sequence within a range.</p>"},{"location":"Python/python-tutorial-empty/#while","title":"While\u00b6","text":"<p><code>while</code> loops execute a block of code as long as a specified condition is <code>True</code>. Care should be taken to avoid infinite loops where the condition always remains <code>True</code>. The syntax for a <code>while</code> loop is <code>while condition:</code> followed by an indented block of code.</p> <p>The <code>break</code> statement can be used to exit a loop prematurely based on a condition, while <code>continue</code> skips the current iteration and proceeds to the next one.</p> <p>When evaluting the <code>condition</code> anything that isn't <code>False</code>, <code>0</code> or <code>None</code> is considered to be <code>True</code>.</p>"},{"location":"Python/python-tutorial-empty/#if-elif-else-statements","title":"if-elif-else Statements\u00b6","text":"<p><code>if</code>-<code>elif</code>-<code>else</code> statements allow us to control the flow of the code based on conditions. They take the syntax:</p> <pre>if condition1:\n    # condition 1 code\nelif condition2:\n    # condition 2 code\nelif condition3:\n    # condition 3 code\nelse:\n    # default code\n</pre> <p>Notice that the <code>if</code> and <code>elif</code> statements take logical expressions, while <code>else</code> does not. You can have any number of <code>elif</code> branches but only one <code>if</code> branch and at most one <code>else</code> branch.</p> <p>This construct allows for branching based on multiple conditions. Python evaluates each condition sequentially. If <code>condition1</code> is true, it executes the code block under <code>condition1</code>. If <code>condition1</code> is false, it checks <code>condition2</code>, and so on. If none of the conditions are true, the code block under <code>else</code> (if provided) is executed as the default action.</p>"},{"location":"Python/python-tutorial-empty/#functions","title":"Functions\u00b6","text":"<p>Creating functions is an effective method to enhance code reusability and streamline debugging. When there's a block of code intended to be executed multiple times, encapsulating it within a function proves beneficial. This practice minimizes human error by necessitating modifications in only one location. Moreover, employing functions to execute smaller code segments can significantly enhance code readability and simplify the debugging process.</p> <p>In Python, we define a function using the <code>def</code> keyword:</p>"},{"location":"Python/python-tutorial-empty/#functions-naming-conventions-and-documentation","title":"Functions, Naming Conventions and Documentation\u00b6","text":"<p>When writing functions and classes (more on this later), we should conform to a consistent convention. This helps both users and developers to better understand the code, improving the ability to use and develop the code.</p> <p>The convention we'll follow in this example is the Google Python Style Guide. Let's look at some examples of why this is useful.</p> <p>Consider the following. We have a function <code>calc</code> which takes three arguments (<code>x</code>, <code>b</code> and <code>i</code>).</p>"},{"location":"Python/python-tutorial-empty/#packages","title":"Packages\u00b6","text":"<p>Python boasts an extensive array of packages developed by the community. In Python, we use the <code>import statement</code> to bring in packages or specific sections of packages into our code.</p> <pre>import package as p\n</pre> <p>In the example above, we import a package named <code>package</code>. The <code>as p</code> statement allows us to assign an alias, <code>p</code>, to the imported package. This aliasing technique proves beneficial when accessing objects from within a package that might share a common name with objects in other packages. For instance:</p>"},{"location":"Python/python-tutorial-empty/#working-with-numpy","title":"Working with Numpy\u00b6","text":"<p>Numpy offers highly optimized functionality for typical matrix and vector operations, with the cornerstone being the numpy array. Arrays resemble lists in their mutability but differ in that they can only contain a single data type.</p>"},{"location":"Python/python-tutorial-empty/#decorators","title":"Decorators\u00b6","text":"<p>Decorators allow us to modify the behavior of a function. They are essentially a function, that take another function as an arguement and modifies the behavior of the function.</p> <p>Let's define a logging decorator:</p>"},{"location":"Python/python-tutorial-empty/#working-with-pandas","title":"Working with Pandas\u00b6","text":"<p>Pandas is an open-source data manipulation and analysis library in Python that's built on top of NumPy. It provides high-level data structures and a variety of tools for working with structured data.</p> <p>The core data structure in Pandas is the DataFrame, which is essentially a two-dimensional array with labeled axes (rows and columns). This DataFrame object is built upon NumPy's ndarray, utilizing its efficient operations and functions.</p>"},{"location":"Python/python-tutorial-empty/#data-analysis-with-python","title":"Data Analysis with Python\u00b6","text":"<p>Python is a great language for high-level data analysis, with jupyter notebooks providing a great \"analysis notebook\" for documenting analysis and displaying results.</p> <p>Let's look at how we might reduce and analyze data using Python and extract some meaningful results.</p>"},{"location":"Python/python-tutorial-empty/#fitting-a-model-to-data","title":"Fitting a model to data\u00b6","text":"<ul> <li>scipy optimize package</li> <li>numpy polyfit</li> <li>Error propagation</li> <li>bootstrapping</li> </ul> <p>Let's start by creating a data set using numpy.</p>"},{"location":"Python/python-tutorial-empty/#are-we-confident-in-our-uncertainty","title":"Are we confident in our uncertainty?\u00b6","text":"<p>It can often be difficult to quantify our uncertainties. Bootstrapping is a useful method to estimate our uncertainties.</p> <p>Assuming we have independent data points, we can randomly sample our data, apply our fit to that data, and then repeat a number of times to estimate the distribution of best fit values.</p> <p></p>"},{"location":"Python/python-tutorial-empty/#how-to-use-bootstrapping-to-handle-no-gaussian-errors","title":"How to use bootstrapping to handle no-gaussian errors\u00b6","text":"<p>If our errors are non-gaussian, it might not make sense to use a $\\chi^2$ fit.</p>"},{"location":"Python/python-tutorial-empty/#poisson-distribution","title":"Poisson Distribution\u00b6","text":"<p>$$p(X = k ; \\lambda) = \\frac{e^{-\\lambda}\\lambda^{k}}{k!}$$</p> <p>Where $k$ is the observed counts, $\\lambda$ is the mean counts. Mean is $\\lambda$, standard deviation is $\\sqrt{\\lambda}$. In counting experiments we typically say if $f= N$, then, $\\Delta f = \\sqrt{N}$.</p> <p>Does this mean that its appropriate to use $\\sqrt{N}$ in a $\\chi^2$ fit?</p>"},{"location":"Python/python-tutorial-empty/#classes-in-python","title":"Classes in Python\u00b6","text":"<p>Classes in Python serve as templates or blueprints defining the attributes (data) and behaviors (methods) of objects. They encapsulate both data and methods that operate on that data within a single structure, promoting code organization and reusability.</p> <p>To create a class in Python, you use the <code>class</code> keyword, allowing you to define properties (attributes) and behaviors (methods) within it.</p>"},{"location":"Python/python-tutorial-empty/#attributes-and-methods","title":"Attributes and Methods\u00b6","text":"<p>Attributes represent the data associated with a class, while methods are functions defined within the class that can access and manipulate this data. These methods can perform various operations on the attributes, thereby altering or providing access to the data encapsulated within the class.</p>"},{"location":"Python/python-tutorial-empty/#example-of-a-simple-class","title":"Example of a Simple Class\u00b6","text":"<p>Consider the following example of a basic class in Python:</p> <pre>class Car:\n    def __init__(self, make, model, year):\n        self.make = make\n        self.model = model\n        self.year = year\n\n    def get_details(self):\n        return f\"{self.year} {self.make} {self.model}\"\n</pre> <p>The <code>__init__</code> function is the \"initialization\" or constructor of the class. This function is called when the object is created.</p>"},{"location":"Python/python-tutorial-empty/#inheritance-in-python","title":"Inheritance in Python\u00b6","text":"<p>Inheritance is a fundamental concept in object-oriented programming that allows a new class to inherit properties and behaviors (attributes and methods) from an existing class. This concept promotes code reuse, enhances readability, and enables the creation of more specialized classes.</p>"},{"location":"Python/python-tutorial-empty/#basics-of-inheritance","title":"Basics of Inheritance\u00b6","text":"<p>In Python, inheritance is achieved by specifying the name of the parent class(es) inside the definition of a new class. The new class, also known as the child class or subclass, inherits all attributes and methods from its parent class or classes, referred to as the base class or superclass.</p>"},{"location":"Python/python-tutorial-empty/#syntax-for-inheriting-classes","title":"Syntax for Inheriting Classes\u00b6","text":"<p>The syntax for creating a subclass that inherits from a superclass involves passing the name of the superclass inside parentheses when defining the subclass:</p> <pre>class ParentClass:\n    # Parent class attributes and methods\n\nclass ChildClass(ParentClass):\n    # Child class attributes and methods\n</pre> <p>Here, <code>ChildClass</code> is inheriting from <code>ParentClass</code>, which means <code>ChildClass</code> will inherit all attributes and methods defined in <code>ParentClass</code>.</p>"},{"location":"Python/python-tutorial/","title":"Classes in Python","text":"In\u00a0[156]: Copied! <pre># Defining a bool\nmy_variable = True\n# Redefining as a float\nmy_variable = 4.23\n# Redefining as a string\nmy_variable = \"goodbye\"\n</pre> # Defining a bool my_variable = True # Redefining as a float my_variable = 4.23 # Redefining as a string my_variable = \"goodbye\"  <p>In python we can print output using the <code>print()</code> function:</p> In\u00a0[157]: Copied! <pre># Defining as a string\nmy_variable = \"Hello, world\"\nprint (my_variable)\n# Redefining a bool\nmy_variable = True\nprint (my_variable)\n# Redefining as a float\nmy_variable = 4.23\nprint (my_variable)\nprint (\"Goodbye, World!\")\n</pre> # Defining as a string my_variable = \"Hello, world\" print (my_variable) # Redefining a bool my_variable = True print (my_variable) # Redefining as a float my_variable = 4.23 print (my_variable) print (\"Goodbye, World!\")  <pre>Hello, world\nTrue\n4.23\nGoodbye, World!\n</pre> <p>when printing we can format strings using <code>fstrings</code>:</p> In\u00a0[158]: Copied! <pre>pi = 3.14159265359\nprint (f\"Pi to 5 digits = {pi:0.5f}\")\nprint (f\"Pi to 3 digits = {pi:0.3f}\")\nprint (f\"Pi to 4 digits in scientific notation = {pi:0.4e}\")\nprint (f\"Pi as an integer = {pi:0.0f}, minus pi to 1 digit {-pi:0.1f}\")\n</pre> pi = 3.14159265359 print (f\"Pi to 5 digits = {pi:0.5f}\") print (f\"Pi to 3 digits = {pi:0.3f}\") print (f\"Pi to 4 digits in scientific notation = {pi:0.4e}\") print (f\"Pi as an integer = {pi:0.0f}, minus pi to 1 digit {-pi:0.1f}\") <pre>Pi to 5 digits = 3.14159\nPi to 3 digits = 3.142\nPi to 4 digits in scientific notation = 3.1416e+00\nPi as an integer = 3, minus pi to 1 digit -3.1\n</pre> <p>We can also define strings to format later:</p> In\u00a0[159]: Copied! <pre>my_string = \"{name}'s favorite number is {number}\"\n\nformatted = my_string.format( name = \"Ste\", number = 42)\nprint(formatted)\n</pre> my_string = \"{name}'s favorite number is {number}\"  formatted = my_string.format( name = \"Ste\", number = 42) print(formatted) <pre>Ste's favorite number is 42\n</pre> <p>And we can add strings together and take slices of string:</p> In\u00a0[160]: Copied! <pre># Adding to a string\nextended = formatted + \" and he likes python\"\nprint (extended)\n# Taking up to the last 6 elements and adding \"pi\"\nprint (extended[:-6] + \"pi\")\n</pre> # Adding to a string extended = formatted + \" and he likes python\" print (extended) # Taking up to the last 6 elements and adding \"pi\" print (extended[:-6] + \"pi\") <pre>Ste's favorite number is 42 and he likes python\nSte's favorite number is 42 and he likes pi\n</pre> In\u00a0[161]: Copied! <pre>a = 7\nb = 2.2\n# Normal division\nc = a/b\n# Integer division\nd = a//b\n# Modulus (remainder)\ne = a % b\n\nprint(f\"{a} / {b} = {c}\")\nprint(f\"{a} // {b} = {d}\")\nprint(f\"{a} % {b} = {e}\")\n\nprint (f\"a &gt; 2: {a &gt; 2}\")\n</pre> a = 7 b = 2.2 # Normal division c = a/b # Integer division d = a//b # Modulus (remainder) e = a % b  print(f\"{a} / {b} = {c}\") print(f\"{a} // {b} = {d}\") print(f\"{a} % {b} = {e}\")  print (f\"a &gt; 2: {a &gt; 2}\") <pre>7 / 2.2 = 3.1818181818181817\n7 // 2.2 = 3.0\n7 % 2.2 = 0.39999999999999947\na &gt; 2: True\n</pre> In\u00a0[7]: Copied! <pre>x = 10\ny = float(x)\nz = bool(x)\nv = str(x)\nprint (x,y,z,v)\nprint (type(x), type(y), type(z), type(v))\n</pre> x = 10 y = float(x) z = bool(x) v = str(x) print (x,y,z,v) print (type(x), type(y), type(z), type(v))  <pre>10 10.0 True 10\n&lt;class 'int'&gt; &lt;class 'float'&gt; &lt;class 'bool'&gt; &lt;class 'str'&gt;\n</pre> In\u00a0[8]: Copied! <pre># Create a list\nmy_list = [4,5.2,-1.3]\nprint (my_list)\n\n# Add a new element to the end\nmy_list.append(21)\nprint (my_list)\n\n# \"Pop\" out the 1st element\nelement = my_list.pop(1)\nprint (element, my_list)\n\n# Reasign a value\nmy_list[0] = -999\nprint (my_list)\n\n\n# Lists can include multiple data types\nmy_list[-2] = \"Hello\"\nprint(my_list)\n</pre> # Create a list my_list = [4,5.2,-1.3] print (my_list)  # Add a new element to the end my_list.append(21) print (my_list)  # \"Pop\" out the 1st element element = my_list.pop(1) print (element, my_list)  # Reasign a value my_list[0] = -999 print (my_list)   # Lists can include multiple data types my_list[-2] = \"Hello\" print(my_list) <pre>[4, 5.2, -1.3]\n[4, 5.2, -1.3, 21]\n5.2 [4, -1.3, 21]\n[-999, -1.3, 21]\n[-999, 'Hello', 21]\n</pre> In\u00a0[9]: Copied! <pre># Define a new list\nmy_list = [1,2,3,4,5,6,7,8,9, 1,2,3,4]\nprint (my_list)\n\n# Create a slice excluding the first and last\nmy_sub_list = my_list[1:-1]\nprint(my_sub_list)\n\n# get the length of the list\nprint (f\"The list is {len(my_list)} elements long\")\n</pre> # Define a new list my_list = [1,2,3,4,5,6,7,8,9, 1,2,3,4] print (my_list)  # Create a slice excluding the first and last my_sub_list = my_list[1:-1] print(my_sub_list)  # get the length of the list print (f\"The list is {len(my_list)} elements long\")     <pre>[1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4]\n[2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3]\nThe list is 13 elements long\n</pre> In\u00a0[11]: Copied! <pre># Create a set using {}\nfirst_set = {1,2,3,4,4,5}\nprint (first_set)\n\n# create a set from the list\nmy_set = set(my_list)\nprint (my_set)\n\n# Add values to the set\nmy_set.add(13)\nmy_set.add(1)\nprint (my_set)\n\n# Sets can have multiple data types\nmy_set.add(\"Hello\")\nprint (my_set)\n</pre> # Create a set using {} first_set = {1,2,3,4,4,5} print (first_set)  # create a set from the list my_set = set(my_list) print (my_set)  # Add values to the set my_set.add(13) my_set.add(1) print (my_set)  # Sets can have multiple data types my_set.add(\"Hello\") print (my_set) <pre>{1, 2, 3, 4, 5}\n{1, 2, 3, 4, 5, 6, 7, 8, 9}\n{1, 2, 3, 4, 5, 6, 7, 8, 9, 13}\n{1, 2, 3, 4, 5, 6, 7, 8, 9, 13, 'Hello'}\n</pre> In\u00a0[12]: Copied! <pre>my_tup = (1,2,3,4, \"Apple\")\nprint (my_tup)\nmy_tup[0] = -2\nprint (my_tup)\n</pre> my_tup = (1,2,3,4, \"Apple\") print (my_tup) my_tup[0] = -2 print (my_tup) <pre>(1, 2, 3, 4, 'Apple')\n</pre> <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[12], line 3\n      1 my_tup = (1,2,3,4, \"Apple\")\n      2 print (my_tup)\n----&gt; 3 my_tup[0] = -2\n      4 print (my_tup)\n\nTypeError: 'tuple' object does not support item assignment</pre> In\u00a0[13]: Copied! <pre>my_dict = {}\nfmt_string = \"entry_{entry}\"\nfor i in range(10):\n    my_dict[fmt_string.format(entry=i)] = -1\n    \nprint (my_dict[\"entry_9\"])\n\nif \"new_key\" in my_dict:\n    print (\"Key exists\")\n\nif \"entry_1\" in my_dict:\n    print (\"Key exists: \", my_dict[\"entry_1\"])\n</pre> my_dict = {} fmt_string = \"entry_{entry}\" for i in range(10):     my_dict[fmt_string.format(entry=i)] = -1      print (my_dict[\"entry_9\"])  if \"new_key\" in my_dict:     print (\"Key exists\")  if \"entry_1\" in my_dict:     print (\"Key exists: \", my_dict[\"entry_1\"])   <pre>-1\nKey exists:  -1\n</pre> In\u00a0[14]: Copied! <pre># Create an empty list\nx = []\n\n# range(n) will return an iteratable type which goes from 0-10 exclusive (0,1,...,9)\nfor i in range(10):\n    # add i to our list\n    x.append(i)\n\nprint (x)\n\n# The list x is also iterable\nfor ix in x:\n    # Print the value squared\n    print (ix**2)\n</pre> # Create an empty list x = []  # range(n) will return an iteratable type which goes from 0-10 exclusive (0,1,...,9) for i in range(10):     # add i to our list     x.append(i)  print (x)  # The list x is also iterable for ix in x:     # Print the value squared     print (ix**2) <pre>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n0\n1\n4\n9\n16\n25\n36\n49\n64\n81\n</pre> <p>We can also use \"list comprehension\" to generate a list from simple loops. This takes the format:</p> <pre>array = [ some_funtion(i) for i in some_loop() ]\n</pre> <p>We can also use list comprehension to do some filtering:</p> <pre>array = [ some_function(i) for i in some_loop() if some_condition(i) ]\n</pre> <p>The if statement can go before or after the <code>for</code> loop:</p> <pre>array = [ some_function(i) if some_condition(i) for i in some_loop()  ]\n</pre> <p>We can also add in an else condition:</p> <pre>array = [ some_function(i) if some_condition(i) else 0 for i in some_loop() ]\n</pre> In\u00a0[15]: Copied! <pre># Create a list with numbers between 0 and 100\nmy_list = [ x for x in range(100)]\n# Create a list with only even numbers between 0 and 100\nmy_even_list = [x for x in range(100) if x % 2 == 0]\n# Create a list with 0 for even indices and 1 for odd indicies\n# Between 0 and 100\nmy_conditional_list = [ 0 if x %2 == 0 else 1 for x in range(100) ]\n\nprint (my_list[:5])\nprint (my_even_list[:5])\nprint (my_conditional_list[:5])\n</pre> # Create a list with numbers between 0 and 100 my_list = [ x for x in range(100)] # Create a list with only even numbers between 0 and 100 my_even_list = [x for x in range(100) if x % 2 == 0] # Create a list with 0 for even indices and 1 for odd indicies # Between 0 and 100 my_conditional_list = [ 0 if x %2 == 0 else 1 for x in range(100) ]  print (my_list[:5]) print (my_even_list[:5]) print (my_conditional_list[:5])  <pre>[0, 1, 2, 3, 4]\n[0, 2, 4, 6, 8]\n[0, 1, 0, 1, 0]\n</pre> In\u00a0[17]: Copied! <pre>i = 0\n# This will not run\nwhile None:\n    i+=1\n    print (i)\n    if i &gt; 5:\n        break\n</pre> i = 0 # This will not run while None:     i+=1     print (i)     if i &gt; 5:         break In\u00a0[18]: Copied! <pre>i = 0\n# Use while loop to print the numbers up to 10\nwhile i &lt; 10:\n    print (i)\n    i+=1\n</pre> i = 0 # Use while loop to print the numbers up to 10 while i &lt; 10:     print (i)     i+=1  <pre>0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n</pre> In\u00a0[21]: Copied! <pre>i = 0\n# Use an if condition to break this infinite loop after the 5th iteration\nwhile \"hello\":\n    print (i)\n    i+=1\n\n    if i &gt;= 5:\n        break\n</pre> i = 0 # Use an if condition to break this infinite loop after the 5th iteration while \"hello\":     print (i)     i+=1      if i &gt;= 5:         break      <pre>0\n1\n2\n3\n4\n</pre> <p>Without the if statement here we would have an infinite loop!  We can exit out of a loop with a <code>break</code> command or we can skip to the end of the current iteration using the <code>continue</code> command.</p> <p>Let's use <code>if</code> statements to see how these work.</p> In\u00a0[16]: Copied! <pre>even_sum = 0\nodd_sum = 0\n\nfor i in range(100):\n    # Exit the loop when i goes above or equal to 10\n    if i &gt;= 10:\n        break\n    # Skip the i = 3 or the i = 0 iteration\n    elif (i == 3) | (i==0):\n        continue\n    elif i % 2 == 0:\n        even_sum += i\n        print (f\"{i} -&gt; Even Sum: {even_sum}\")\n\n    else :\n        odd_sum +=1\n        print (f\"{i} -&gt; Odd Sum: {odd_sum}\")\n</pre> even_sum = 0 odd_sum = 0  for i in range(100):     # Exit the loop when i goes above or equal to 10     if i &gt;= 10:         break     # Skip the i = 3 or the i = 0 iteration     elif (i == 3) | (i==0):         continue     elif i % 2 == 0:         even_sum += i         print (f\"{i} -&gt; Even Sum: {even_sum}\")      else :         odd_sum +=1         print (f\"{i} -&gt; Odd Sum: {odd_sum}\")  <pre>1 -&gt; Odd Sum: 1\n2 -&gt; Even Sum: 2\n4 -&gt; Even Sum: 6\n5 -&gt; Odd Sum: 2\n6 -&gt; Even Sum: 12\n7 -&gt; Odd Sum: 3\n8 -&gt; Even Sum: 20\n9 -&gt; Odd Sum: 4\n</pre> In\u00a0[17]: Copied! <pre># Simple function to print hello\ndef print_hello():\n    print(\"Hello\")\n\n# We can take in arguments\ndef print_message(msg):\n    print(msg)\n\n\n# Simple function to take in two arguments and add them togeter and return the sum\ndef add_numbers(a,b):\n    c = a + b\n    # Use the print message function to print c\n    print_message(c)\n    return a+b\n\n\n# We can also pass a function to a function\ndef repeat(func, n, args):\n    for i in range(n):\n        # Using the *args will unwrap the tuple and pass to the function\n        func(*args)\n\nprint_hello()\nprint_message(42)\n\nc = add_numbers(1.3, 5)\n\n# We can specify which variable is which by specificing the argument name/\nrepeat(func = add_numbers, n = 5, args=(1.3,2.1))\n</pre> # Simple function to print hello def print_hello():     print(\"Hello\")  # We can take in arguments def print_message(msg):     print(msg)   # Simple function to take in two arguments and add them togeter and return the sum def add_numbers(a,b):     c = a + b     # Use the print message function to print c     print_message(c)     return a+b   # We can also pass a function to a function def repeat(func, n, args):     for i in range(n):         # Using the *args will unwrap the tuple and pass to the function         func(*args)  print_hello() print_message(42)  c = add_numbers(1.3, 5)  # We can specify which variable is which by specificing the argument name/ repeat(func = add_numbers, n = 5, args=(1.3,2.1)) <pre>Hello\n42\n6.3\n3.4000000000000004\n3.4000000000000004\n3.4000000000000004\n3.4000000000000004\n3.4000000000000004\n</pre> <p>We can also write <code>lambda</code> functions which are short inline functions:</p> In\u00a0[18]: Copied! <pre># Lambda function to get square root\nmy_function = lambda x : x**0.5\n\n# Lambda function to act as a wrapper\ndef larger_function(x, y):\n    return x**2 / y**3\n\n# Lambda function which calls for y = 1.5\nmy_wrapper = lambda x : larger_function(x, 1.5)\n\n\nprint (my_function(4))\nprint (my_wrapper(3))\nprint (larger_function(3,4))\n</pre> # Lambda function to get square root my_function = lambda x : x**0.5  # Lambda function to act as a wrapper def larger_function(x, y):     return x**2 / y**3  # Lambda function which calls for y = 1.5 my_wrapper = lambda x : larger_function(x, 1.5)   print (my_function(4)) print (my_wrapper(3)) print (larger_function(3,4))  <pre>2.0\n2.6666666666666665\n0.140625\n</pre> In\u00a0[22]: Copied! <pre>def calc(x, b, i):\n    x[i] = x[i] / b\n    return x\n</pre> def calc(x, b, i):     x[i] = x[i] / b     return x <p>The function scales an element in <code>x</code> by <code>1/b</code>. We can choose a name to better describe what the function does.</p> In\u00a0[23]: Copied! <pre>def scale_element(x, b, i):\n    x[i] = x[i] / b\n    return x\n</pre> def scale_element(x, b, i):     x[i] = x[i] / b     return x <p>The use now knows that the function will scale an element of the array, but the user still doesn't know what the arguments are or what is returned. We can add a doc string to help with this.</p> In\u00a0[24]: Copied! <pre>help(scale_element)\n</pre> help(scale_element) <pre>Help on function scale_element in module __main__:\n\nscale_element(x, b, i)\n\n</pre> <p>This help message is automatically generated from the \"docstring\" of the function. The docstring is a small description of a function that we write at the state of the function.</p> <p>Following the Google Python Style Guide, a good template for your docstring is:</p> <pre><code>def function(x,y,z):\n    \"\"\"One line summary of my function\n\n    More detailed description of my function, potentially showing\n    some math relation:\n    $\\frac{dy}{dx} = x^2$\n\n    Args:\n        x: description of x\n        y: description of y\n        z: description of z\n\n    Returns:\n        description of what is returned\n\n    Examples:\n        Some example\n        &gt;&gt;&gt; function (x,y,z)\n        return_value\n\n    Raises:\n        Error: Error raised and description of that error\n    \"\"\"\n</code></pre> <p>This is quite verbose but has huge benifits for the user and allows us to self document our code.</p> In\u00a0[25]: Copied! <pre>def scale_element(x, b, i):\n    \"\"\"Scale an element of the input array\n\n    Scale an element of the array by a constant\n\n    Args:\n        x: list of values\n        b: constant to scale by\n        i: index of element to scale\n\n    Returns:\n        Scaled a list with the element i scaled by 1/b\n\n    Examples:\n        &gt;&gt;&gt; scale_element([1,2,3,4], 2, 1)\n        [1, 1.0, 3, 4]\n\n    \"\"\"\n    x[i] = x[i] / b\n    return x\n</pre> def scale_element(x, b, i):     \"\"\"Scale an element of the input array      Scale an element of the array by a constant      Args:         x: list of values         b: constant to scale by         i: index of element to scale      Returns:         Scaled a list with the element i scaled by 1/b      Examples:         &gt;&gt;&gt; scale_element([1,2,3,4], 2, 1)         [1, 1.0, 3, 4]      \"\"\"     x[i] = x[i] / b     return x <p>Let's break down the sections here:</p> <pre><code>    \"\"\"Scale an element of the input array\n</code></pre> <p>We start off with a short 1 sentence description of the function</p> <pre><code>    Scale an element of the array by a constant\n</code></pre> <p>We then use a more detailed description of the function and how to use it</p> <pre><code>    Args:\n        x: list of values\n        b: constant to scale by\n        i: index of element to scale\n</code></pre> <p>We list the arguments by name and what they are.</p> <pre><code>\n    Returns:\n        Scaled a list with the element i scaled by 1/b\n</code></pre> <p>We list what is returned by the function and what they are.</p> <pre><code>    Examples:\n        &gt;&gt;&gt; scale_element([1,2,3,4], 2, 1)\n        [1,1.0,3,4]\n\n    \"\"\"\n</code></pre> <p>We give an usage example of the functions and the expected output. The user can then access this helpful message anything using:</p> In\u00a0[26]: Copied! <pre>help(scale_element)\n</pre> help(scale_element) <pre>Help on function scale_element in module __main__:\n\nscale_element(x, b, i)\n    Scale an element of the input array\n    \n    Scale an element of the array by a constant\n    \n    Args:\n        x: list of values\n        b: constant to scale by\n        i: index of element to scale\n    \n    Returns:\n        Scaled a list with the element i scaled by 1/b\n    \n    Examples:\n        &gt;&gt;&gt; scale_element([1,2,3,4], 2, 1)\n        [1, 1.0, 3, 4]\n\n</pre> <p>We can do better still. The user doesn't know what type of data to pass. For example if x is just a float rather than an array, this will fail. We can do this with \"type-hinting\". Type hinting is an optional feature in Python where we tell the expect type of the data that a fuction is expecting.</p> <p>For example:</p> In\u00a0[29]: Copied! <pre>def multiply(a, b):\n    return a * b\n\n\n# what happens when we pass two floats to this function?\nmultiply(1.5, 2.3)\n</pre> def multiply(a, b):     return a * b   # what happens when we pass two floats to this function? multiply(1.5, 2.3) Out[29]: <pre>3.4499999999999997</pre> In\u00a0[33]: Copied! <pre># What happens when we pass a string and an int?\nmultiply(\"apple\", 6)\n</pre> # What happens when we pass a string and an int? multiply(\"apple\", 6) Out[33]: <pre>'appleappleappleappleappleapple'</pre> <p>We've originally defined our function to ints or floats but we behaviour we weren't expecting when passing a string. We can use type hinting to be explicit about what can be passed to the function.</p> In\u00a0[34]: Copied! <pre>def multiply(a : float, b : float ) -&gt; float:\n    return a * b\n\n# what happens when we pass two floats to this function?\nmultiply(1.5, 2.3)\n</pre> def multiply(a : float, b : float ) -&gt; float:     return a * b  # what happens when we pass two floats to this function? multiply(1.5, 2.3) Out[34]: <pre>3.4499999999999997</pre> In\u00a0[35]: Copied! <pre># What happens when we pass a string and an int?\nmultiply(\"apple\", 6)\n</pre> # What happens when we pass a string and an int? multiply(\"apple\", 6) Out[35]: <pre>'appleappleappleappleappleapple'</pre> <p>This doesn't stop us from calling the function with a string and int, but it does provide additional information to the <code>help()</code> function.</p> In\u00a0[37]: Copied! <pre>help(multiply)\n</pre> help(multiply) <pre>Help on function multiply in module __main__:\n\nmultiply(a: float, b: float) -&gt; float\n\n</pre> In\u00a0[38]: Copied! <pre>def scale_element(x : list, b : float, i : int) -&gt; list:\n    \"\"\"Scale an element of the input array\n\n    Scale an element of the array by a constant\n\n    Args:\n        x: list of values\n        b: constant to scale by\n        i: index of element to scale\n\n    Returns:\n        Scaled a list with the element i scaled by 1/b\n\n    Examples:\n        &gt;&gt;&gt; scale_element([1,2,3,4], 2, 1)\n        [1, 1.0, 3, 4]\n\n    \"\"\"\n    x[i] = x[i] / b\n    return x\n</pre> def scale_element(x : list, b : float, i : int) -&gt; list:     \"\"\"Scale an element of the input array      Scale an element of the array by a constant      Args:         x: list of values         b: constant to scale by         i: index of element to scale      Returns:         Scaled a list with the element i scaled by 1/b      Examples:         &gt;&gt;&gt; scale_element([1,2,3,4], 2, 1)         [1, 1.0, 3, 4]      \"\"\"     x[i] = x[i] / b     return x <p>From the first line we can see:</p> <pre><code>def scale_element(x : list, b : float, i : int) -&gt; list:\n</code></pre> <p>That <code>x</code> is expected to be a <code>list</code>, <code>b</code> is expected to be a <code>float</code>, <code>i</code> is expected to be a <code>int</code> and that the function will return a <code>list</code>.</p> In\u00a0[39]: Copied! <pre>help(scale_element)\n</pre> help(scale_element) <pre>Help on function scale_element in module __main__:\n\nscale_element(x: list, b: float, i: int) -&gt; list\n    Scale an element of the input array\n    \n    Scale an element of the array by a constant\n    \n    Args:\n        x: list of values\n        b: constant to scale by\n        i: index of element to scale\n    \n    Returns:\n        Scaled a list with the element i scaled by 1/b\n    \n    Examples:\n        &gt;&gt;&gt; scale_element([1,2,3,4], 2, 1)\n        [1, 1.0, 3, 4]\n\n</pre> <p>This is better but let's try and anticpate potential errors</p> In\u00a0[40]: Copied! <pre>scale_element(3, 2, 1)\n</pre> scale_element(3, 2, 1) <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[40], line 1\n----&gt; 1 scale_element(3, 2, 1)\n\nCell In[38], line 19, in scale_element(x, b, i)\n      1 def scale_element(x : list, b : float, i : int) -&gt; list:\n      2     \"\"\"Scale an element of the input array\n      3 \n      4     Scale an element of the array by a constant\n   (...)\n     17 \n     18     \"\"\"\n---&gt; 19     x[i] = x[i] / b\n     20     return x\n\nTypeError: 'int' object is not subscriptable</pre> <p>This error message isn't too helpful... Let's write our own</p> In\u00a0[42]: Copied! <pre>def scale_element(x : list, b : float, i : int) -&gt; list:\n    \"\"\"Scale an element of the input array\n\n    Scale an element of the array by a constant\n\n    Args:\n        x: list of values\n        b: constant to scale by\n        i: index of element to scale\n\n    Returns:\n        Scaled a list with the element i scaled by 1/b\n\n    Examples:\n        &gt;&gt;&gt; scale_element([1,2,3,4], 2, 1)\n        [1, 1.0, 3, 4]\n\n    Raises:\n        ValueError: If x is not a list, i in not an int or b is neither an int or float\n        IndexError: If i is out of bounds in list x\n\n    \"\"\"\n    if not isinstance(x, list):\n        raise ValueError(f'x is expected to be a list, recieved {type(x)}')\n    if not isinstance(i, int):\n        raise ValueError(f'i is expected to be an int, recieved {type(i)}')\n    if not (isinstance(b, int) or isinstance(b, float)):\n        raise ValueError(f'b is expected to be an int or float, recieved {type(b)}')\n\n    if i &gt;= len(x):\n        raise IndexError(f'Index i ({i}) is out of bounds of array x (with len {len(x)})')\n    x[i] = x[i] / b\n    return x\n</pre> def scale_element(x : list, b : float, i : int) -&gt; list:     \"\"\"Scale an element of the input array      Scale an element of the array by a constant      Args:         x: list of values         b: constant to scale by         i: index of element to scale      Returns:         Scaled a list with the element i scaled by 1/b      Examples:         &gt;&gt;&gt; scale_element([1,2,3,4], 2, 1)         [1, 1.0, 3, 4]      Raises:         ValueError: If x is not a list, i in not an int or b is neither an int or float         IndexError: If i is out of bounds in list x      \"\"\"     if not isinstance(x, list):         raise ValueError(f'x is expected to be a list, recieved {type(x)}')     if not isinstance(i, int):         raise ValueError(f'i is expected to be an int, recieved {type(i)}')     if not (isinstance(b, int) or isinstance(b, float)):         raise ValueError(f'b is expected to be an int or float, recieved {type(b)}')      if i &gt;= len(x):         raise IndexError(f'Index i ({i}) is out of bounds of array x (with len {len(x)})')     x[i] = x[i] / b     return x In\u00a0[43]: Copied! <pre>scale_element(3, 2, 1)\n</pre> scale_element(3, 2, 1) <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[43], line 1\n----&gt; 1 scale_element(3, 2, 1)\n\nCell In[42], line 24, in scale_element(x, b, i)\n      2 \"\"\"Scale an element of the input array\n      3 \n      4 Scale an element of the array by a constant\n   (...)\n     21 \n     22 \"\"\"\n     23 if not isinstance(x, list):\n---&gt; 24     raise ValueError(f'x is expected to be a list, recieved {type(x)}')\n     25 if not isinstance(i, int):\n     26     raise ValueError(f'i is expected to be an int, recieved {type(i)}')\n\nValueError: x is expected to be a list, recieved &lt;class 'int'&gt;</pre> In\u00a0[44]: Copied! <pre>scale_element([3,2,2], 2, 5)\n</pre> scale_element([3,2,2], 2, 5) <pre>\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[44], line 1\n----&gt; 1 scale_element([3,2,2], 2, 5)\n\nCell In[42], line 31, in scale_element(x, b, i)\n     28     raise ValueError(f'b is expected to be an int or float, recieved {type(b)}')\n     30 if i &gt;= len(x):\n---&gt; 31     raise IndexError(f'Index i ({i}) is out of bounds of array x (with len {len(x)})')\n     32 x[i] = x[i] / b\n     33 return x\n\nIndexError: Index i (5) is out of bounds of array x (with len 3)</pre> In\u00a0[45]: Copied! <pre>help(scale_element)\n</pre> help(scale_element) <pre>Help on function scale_element in module __main__:\n\nscale_element(x: list, b: float, i: int) -&gt; list\n    Scale an element of the input array\n    \n    Scale an element of the array by a constant\n    \n    Args:\n        x: list of values\n        b: constant to scale by\n        i: index of element to scale\n    \n    Returns:\n        Scaled a list with the element i scaled by 1/b\n    \n    Examples:\n        &gt;&gt;&gt; scale_element([1,2,3,4], 2, 1)\n        [1, 1.0, 3, 4]\n    \n    Raises:\n        ValueError: If x is not a list, i in not an int or b is neither an int or float\n        IndexError: If i is out of bounds in list x\n\n</pre> In\u00a0[47]: Copied! <pre>import doctest\ndoctest.testmod(verbose=True)\n</pre> import doctest doctest.testmod(verbose=True)  <pre>Trying:\n    scale_element([1,2,3,4], 2, 1)\nExpecting:\n    [1, 1.0, 3, 4]\nok\n3 items had no tests:\n    __main__\n    __main__.calc\n    __main__.multiply\n1 items passed all tests:\n   1 tests in __main__.scale_element\n1 tests in 4 items.\n1 passed and 0 failed.\nTest passed.\n</pre> Out[47]: <pre>TestResults(failed=0, attempted=1)</pre> In\u00a0[48]: Copied! <pre>import numpy as np\nimport math as m\n\nprint(np.sin(0))\nprint(m.sin(0))\n</pre> import numpy as np import math as m  print(np.sin(0)) print(m.sin(0)) <pre>0.0\n0.0\n</pre> <p>Here we have imported <code>numpy</code> using the alias <code>np</code> and <code>math</code> using the alias <code>m</code>. Then, we call the <code>sin</code> function from both packages, specifying which version of the <code>sin</code> function we want to invoke.</p> <p>Numpy is a crucial package in scientific programming, and we'll delve deeper into its functionalities shortly.</p> <p>We can also import only a section of a package. For example:</p> In\u00a0[50]: Copied! <pre>import matplotlib.pyplot as plt\nfrom scipy.stats import chi\n</pre> import matplotlib.pyplot as plt from scipy.stats import chi <p>Here, we import the <code>pyplot</code> sub-package from the larger <code>matplotlib</code> package and assign it the alias <code>plt</code>. Additionally, we import <code>chi</code> from the <code>stats</code> sub-package of the <code>scipy</code> package.</p> <p>Both packages hold significant importance:</p> <ul> <li><p>Matplotlib is an extensive library enabling the creation of static, animated, and interactive visualizations in Python. It offers a plethora of tools for various types of plots, charts, and graphical representations.</p> </li> <li><p>SciPy encompasses a wide range of scientific computing tools, providing algorithms for optimization, integration, interpolation, solving eigenvalue problems, handling algebraic and differential equations, statistical computations, and more. It's a fundamental package for scientific and technical computing in Python.</p> </li> </ul> In\u00a0[52]: Copied! <pre># Define an array\nx = np.array([0,1,2,3,4])\ny = x**2\nprint (x)\nprint (y)\n\ndef add_2(arr : np.array ) -&gt; np.array:\n    \"\"\"Add 2 to the value of the array\n\n    Args:\n        arr: array of values\n\n    Returns:\n        arr + 2\n    \"\"\"\n    return arr + 2\n\nz = add_2(x)\nprint(z)\nprint ( (y - x) / z )\n</pre> # Define an array x = np.array([0,1,2,3,4]) y = x**2 print (x) print (y)  def add_2(arr : np.array ) -&gt; np.array:     \"\"\"Add 2 to the value of the array      Args:         arr: array of values      Returns:         arr + 2     \"\"\"     return arr + 2  z = add_2(x) print(z) print ( (y - x) / z ) <pre>[0 1 2 3 4]\n[ 0  1  4  9 16]\n[2 3 4 5 6]\n[0.  0.  0.5 1.2 2. ]\n</pre> In\u00a0[55]: Copied! <pre>def subtract_and_add(x : np.array) -&gt; np.array:\n    \"\"\"Subtract and add to an array\n\n    Subtract 5 from the array and return the array + 10\n\n    Args:\n        x : input numpy array\n\n    Returns:\n        z : x -5 + 10\n    \"\"\"\n    z = x\n    z -= 5\n    return z + 10\n\nx_data = np.arange(0,10,1)\nprint (x_data)\ny_data = subtract_and_add(x_data)\n\n# What happened here!\nprint (x_data)\n\nprint (y_data)\n</pre> def subtract_and_add(x : np.array) -&gt; np.array:     \"\"\"Subtract and add to an array      Subtract 5 from the array and return the array + 10      Args:         x : input numpy array      Returns:         z : x -5 + 10     \"\"\"     z = x     z -= 5     return z + 10  x_data = np.arange(0,10,1) print (x_data) y_data = subtract_and_add(x_data)  # What happened here! print (x_data)  print (y_data) <pre>[0 1 2 3 4 5 6 7 8 9]\n[-5 -4 -3 -2 -1  0  1  2  3  4]\n[ 5  6  7  8  9 10 11 12 13 14]\n</pre> In\u00a0[58]: Copied! <pre>def subtract_and_add_copy(x):\n    \"\"\"Subtract and add to an array\n\n    Subtract 5 from the array and return the array + 10.\n    A copy is used to prevent modifying the input data\n\n    Args:\n        x : input numpy array\n\n    Returns:\n        z : x - 5 + 10\n    \"\"\"\n    z = x.copy()\n    z -= 5\n    return z + 10\n\nx_data = np.arange(0,10,1)\nprint (x_data)\ny_data = subtract_and_add_copy(x_data)\n\n# What happened here!\nprint (x_data)\n\nprint (y_data)\n</pre> def subtract_and_add_copy(x):     \"\"\"Subtract and add to an array      Subtract 5 from the array and return the array + 10.     A copy is used to prevent modifying the input data      Args:         x : input numpy array      Returns:         z : x - 5 + 10     \"\"\"     z = x.copy()     z -= 5     return z + 10  x_data = np.arange(0,10,1) print (x_data) y_data = subtract_and_add_copy(x_data)  # What happened here! print (x_data)  print (y_data) <pre>[0 1 2 3 4 5 6 7 8 9]\n[0 1 2 3 4 5 6 7 8 9]\n[ 5  6  7  8  9 10 11 12 13 14]\n</pre> <p>Numpy arrays allow us to filter them using an array mask. We can pass an array of equal size to the array with a binary mask to select the items we want.</p> In\u00a0[62]: Copied! <pre>x = np.array([1,2,3,4])\nx_mask = np.array([True, False, True, True])\n\n# We can select by indexing by the mask\nprint (x[x_mask])\n\n# We can invert the selection using ~\nprint (x[~x_mask])\n</pre> x = np.array([1,2,3,4]) x_mask = np.array([True, False, True, True])  # We can select by indexing by the mask print (x[x_mask])  # We can invert the selection using ~ print (x[~x_mask]) <pre>[1 3 4]\n[2]\n</pre> In\u00a0[59]: Copied! <pre># We can mask and filter numpy arrays too\nprint (x_data)\n# Get the odd numbers greater than 4\nmask = (x_data &gt; 4) &amp; (x_data %2 == 1)\nprint (mask)\nprint (x_data[mask])\nprint (x_data[mask].sum())\n</pre> # We can mask and filter numpy arrays too print (x_data) # Get the odd numbers greater than 4 mask = (x_data &gt; 4) &amp; (x_data %2 == 1) print (mask) print (x_data[mask]) print (x_data[mask].sum()) <pre>[0 1 2 3 4 5 6 7 8 9]\n[False False False False False  True False  True False  True]\n[5 7 9]\n21\n</pre> In\u00a0[73]: Copied! <pre># use numpy's random number generator to get normal random numbers:\nx_rnd = np.random.normal(loc = 0, scale = 1, size = 1000)\ngreat_that_0 = x_rnd &gt; 0\n\n\n# Use plt.hist to create histograms of the values\nplt.hist(x_rnd)\nplt.hist(x_rnd[great_that_0])\nplt.hist(x_rnd[~great_that_0])\n</pre> # use numpy's random number generator to get normal random numbers: x_rnd = np.random.normal(loc = 0, scale = 1, size = 1000) great_that_0 = x_rnd &gt; 0   # Use plt.hist to create histograms of the values plt.hist(x_rnd) plt.hist(x_rnd[great_that_0]) plt.hist(x_rnd[~great_that_0])   Out[73]: <pre>(array([  2.,   2.,   3.,   4.,  21.,  45.,  62., 113., 100., 132.]),\n array([-3.56610817, -3.20994554, -2.85378291, -2.49762028, -2.14145765,\n        -1.78529501, -1.42913238, -1.07296975, -0.71680712, -0.36064449,\n        -0.00448186]),\n &lt;BarContainer object of 10 artists&gt;)</pre> In\u00a0[86]: Copied! <pre># alpha = transparancy of the histogram\n# color = color of the histogram\n# bins = binning to use\n\n# linspace linearly paced numbers\n# min, max, n\nbinning = np.linspace(-5,5, 20)\nplt.hist(x_rnd, bins= binning, \n         alpha = 0.5, color = \"magenta\", label = \"All\", hatch = \"/\")\nplt.hist(x_rnd[great_that_0], bins= binning, \n         alpha = 0.5, color = \"black\", label = \"X&gt;0\", hatch = \"o\")\nplt.hist(x_rnd[~great_that_0], bins= binning, \n         alpha = 0.5, color = \"darkorange\", label = \"$X \\leq 0$\",hatch = \"*\")\nplt.xlabel(\"X Value\")\nplt.ylabel(\"dN/dX\")\nplt.grid()\nplt.legend()\n</pre> # alpha = transparancy of the histogram # color = color of the histogram # bins = binning to use  # linspace linearly paced numbers # min, max, n binning = np.linspace(-5,5, 20) plt.hist(x_rnd, bins= binning,           alpha = 0.5, color = \"magenta\", label = \"All\", hatch = \"/\") plt.hist(x_rnd[great_that_0], bins= binning,           alpha = 0.5, color = \"black\", label = \"X&gt;0\", hatch = \"o\") plt.hist(x_rnd[~great_that_0], bins= binning,           alpha = 0.5, color = \"darkorange\", label = \"$X \\leq 0$\",hatch = \"*\") plt.xlabel(\"X Value\") plt.ylabel(\"dN/dX\") plt.grid() plt.legend() Out[86]: <pre>&lt;matplotlib.legend.Legend at 0x72ec91609050&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>We can define functions which to operate on numpy arrays\n</pre> We can define functions which to operate on numpy arrays In\u00a0[125]: Copied! <pre>def sqrt(x):\n    return np.sqrt(x)\n</pre> def sqrt(x):     return np.sqrt(x) In\u00a0[126]: Copied! <pre>my_values = np.linspace(0,100)\nmy_sqrts = sqrt(my_values)\nprint (my_sqrts[:5])\n</pre> my_values = np.linspace(0,100) my_sqrts = sqrt(my_values) print (my_sqrts[:5]) <pre>[0.         1.42857143 2.02030509 2.4743583  2.85714286]\n</pre> <p>However we do need to be careful on how we write our functions:</p> In\u00a0[138]: Copied! <pre>def capped_sqrt(x):\n    if x &gt; 0: \n        return np.sqrt(x)\n    else:\n        return 0.\n</pre> def capped_sqrt(x):     if x &gt; 0:          return np.sqrt(x)     else:         return 0. In\u00a0[139]: Copied! <pre>my_values = np.linspace(-10,10)\nmy_sqrts = capped_sqrt(my_values)\nprint (my_sqrts[:5])\n</pre> my_values = np.linspace(-10,10) my_sqrts = capped_sqrt(my_values) print (my_sqrts[:5]) <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[139], line 2\n      1 my_values = np.linspace(-10,10)\n----&gt; 2 my_sqrts = capped_sqrt(my_values)\n      3 print (my_sqrts[:5])\n\nCell In[138], line 2, in capped_sqrt(x)\n      1 def capped_sqrt(x):\n----&gt; 2     if x &gt; 0: \n      3         return np.sqrt(x)\n      4     else:\n\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()</pre> <p>We get around this by vectorizing functions. This allows us to run the function on the entire array without needing to loop over the elements.</p> In\u00a0[140]: Copied! <pre>@np.vectorize\ndef capped_sqrt(x):\n    if x &gt; 0: \n        return np.sqrt(x)\n    else:\n        return 0.\n</pre> @np.vectorize def capped_sqrt(x):     if x &gt; 0:          return np.sqrt(x)     else:         return 0. In\u00a0[141]: Copied! <pre>my_values = np.arange(-1,10)\nmy_sqrts = capped_sqrt(my_values)\nprint (my_sqrts[:5])\n</pre> my_values = np.arange(-1,10) my_sqrts = capped_sqrt(my_values) print (my_sqrts[:5]) <pre>[0.         0.         1.         1.41421356 1.73205081]\n</pre> In\u00a0[153]: Copied! <pre>def my_logger(func):\n    def wrapper(*args, **kwargs):\n        print( f\"Running {func.__name__} with:\\n\\t args = {args}\\n\\t kwargs = {kwargs}\")\n        ret = func(*args, **kwargs)\n        print( f\"Returning {ret}\")\n        return ret\n    return wrapper\n</pre> def my_logger(func):     def wrapper(*args, **kwargs):         print( f\"Running {func.__name__} with:\\n\\t args = {args}\\n\\t kwargs = {kwargs}\")         ret = func(*args, **kwargs)         print( f\"Returning {ret}\")         return ret     return wrapper          In\u00a0[154]: Copied! <pre>@my_logger\ndef sqrt(x):\n    return np.sqrt(x)\n</pre> @my_logger def sqrt(x):     return np.sqrt(x) In\u00a0[155]: Copied! <pre>sqrt(4)\n</pre> sqrt(4) <pre>Running sqrt with:\n\t args = (4,)\n\t kwargs = {}\nReturning 2.0\n</pre> Out[155]: <pre>2.0</pre> In\u00a0[38]: Copied! <pre>import pandas as pd\n\n# Creating a Pandas DataFrame\ndata = {\n    'A': np.random.randn(5),  # Creating a NumPy array for column 'A'\n    'B': np.random.rand(5)    # Creating a NumPy array for column 'B'\n}\n\ndf = pd.DataFrame(data)\nprint(\"Pandas DataFrame:\")\nprint(df)\n</pre> import pandas as pd  # Creating a Pandas DataFrame data = {     'A': np.random.randn(5),  # Creating a NumPy array for column 'A'     'B': np.random.rand(5)    # Creating a NumPy array for column 'B' }  df = pd.DataFrame(data) print(\"Pandas DataFrame:\") print(df) <pre>Pandas DataFrame:\n          A         B\n0 -1.591723  0.883364\n1 -1.550968  0.513569\n2  0.753827  0.700477\n3  0.557346  0.619088\n4  1.900134  0.677126\n</pre> In\u00a0[39]: Copied! <pre>help(np.random.randn)\n</pre> help(np.random.randn) <pre>Help on built-in function randn:\n\nrandn(...) method of numpy.random.mtrand.RandomState instance\n    randn(d0, d1, ..., dn)\n    \n    Return a sample (or samples) from the \"standard normal\" distribution.\n    \n    .. note::\n        This is a convenience function for users porting code from Matlab,\n        and wraps `standard_normal`. That function takes a\n        tuple to specify the size of the output, which is consistent with\n        other NumPy functions like `numpy.zeros` and `numpy.ones`.\n    \n    .. note::\n        New code should use the\n        `~numpy.random.Generator.standard_normal`\n        method of a `~numpy.random.Generator` instance instead;\n        please see the :ref:`random-quick-start`.\n    \n    If positive int_like arguments are provided, `randn` generates an array\n    of shape ``(d0, d1, ..., dn)``, filled\n    with random floats sampled from a univariate \"normal\" (Gaussian)\n    distribution of mean 0 and variance 1. A single float randomly sampled\n    from the distribution is returned if no argument is provided.\n    \n    Parameters\n    ----------\n    d0, d1, ..., dn : int, optional\n        The dimensions of the returned array, must be non-negative.\n        If no argument is given a single Python float is returned.\n    \n    Returns\n    -------\n    Z : ndarray or float\n        A ``(d0, d1, ..., dn)``-shaped array of floating-point samples from\n        the standard normal distribution, or a single such float if\n        no parameters were supplied.\n    \n    See Also\n    --------\n    standard_normal : Similar, but takes a tuple as its argument.\n    normal : Also accepts mu and sigma arguments.\n    random.Generator.standard_normal: which should be used for new code.\n    \n    Notes\n    -----\n    For random samples from the normal distribution with mean ``mu`` and\n    standard deviation ``sigma``, use::\n    \n        sigma * np.random.randn(...) + mu\n    \n    Examples\n    --------\n    &gt;&gt;&gt; np.random.randn()\n    2.1923875335537315  # random\n    \n    Two-by-four array of samples from the normal distribution with\n    mean 3 and standard deviation 2.5:\n    \n    &gt;&gt;&gt; 3 + 2.5 * np.random.randn(2, 4)\n    array([[-4.49401501,  4.00950034, -1.81814867,  7.29718677],   # random\n           [ 0.39924804,  4.68456316,  4.99394529,  4.84057254]])  # random\n\n</pre> In\u00a0[40]: Copied! <pre># Accessing the underlying NumPy array of column 'A'\nnumpy_array = df['A'].values\nprint(\"Numpy array from Pandas DataFrame:\")\nprint(numpy_array)\n</pre> # Accessing the underlying NumPy array of column 'A' numpy_array = df['A'].values print(\"Numpy array from Pandas DataFrame:\") print(numpy_array)  <pre>Numpy array from Pandas DataFrame:\n[-1.59172312 -1.55096775  0.75382695  0.55734567  1.90013409]\n</pre> In\u00a0[41]: Copied! <pre># Loading a csv file using pandas\nurl=\"https://r2.datahub.io/clt98lqg6000el708ja5zbtz0/master/raw/data/monthly.csv\"\ndf=pd.read_csv(url)\ndf.head()\n</pre> # Loading a csv file using pandas url=\"https://r2.datahub.io/clt98lqg6000el708ja5zbtz0/master/raw/data/monthly.csv\" df=pd.read_csv(url) df.head() Out[41]: Source Date Mean 0 GCAG 2016-12 0.7895 1 GISTEMP 2016-12 0.8100 2 GCAG 2016-11 0.7504 3 GISTEMP 2016-11 0.9300 4 GCAG 2016-10 0.7292 In\u00a0[42]: Copied! <pre>df.tail()\n</pre> df.tail() Out[42]: Source Date Mean 3283 GISTEMP 1880-03 -0.1800 3284 GCAG 1880-02 -0.1229 3285 GISTEMP 1880-02 -0.2100 3286 GCAG 1880-01 0.0009 3287 GISTEMP 1880-01 -0.3000 In\u00a0[43]: Copied! <pre>df.plot(x = \"Date\", y = \"Mean\")\n</pre> df.plot(x = \"Date\", y = \"Mean\") Out[43]: <pre>&lt;Axes: xlabel='Date'&gt;</pre> In\u00a0[121]: Copied! <pre># Let define the true model\ndef model(x, p0, p1, p2):\n    return p0 * x**2 + p1 * x + p2\n\n# Set the true parameters\np_true = [0.02, 0.1, -2.5]\n\n# Let the x points be random floats between 0-10\nx = 10*np.random.rand(100)\ny = model(x, p_true[0], p_true[1], p_true[2])\n\n# let's add some gaussian noise\ny_noisey = y + np.random.normal(loc = 0, scale = 0.2, size = x.shape)\n# define our y error as 0.2\ny_err = 0.2 * np.ones(x.shape)\n\n# Plot the data\nplt.errorbar(x, y_noisey, yerr = y_err , fmt = \"C0o\", label = \"Measured\")\nplt.ylabel(\"Y Values\")\nplt.xlabel(\"X Values\")\nplt.grid()\n</pre> # Let define the true model def model(x, p0, p1, p2):     return p0 * x**2 + p1 * x + p2  # Set the true parameters p_true = [0.02, 0.1, -2.5]  # Let the x points be random floats between 0-10 x = 10*np.random.rand(100) y = model(x, p_true[0], p_true[1], p_true[2])  # let's add some gaussian noise y_noisey = y + np.random.normal(loc = 0, scale = 0.2, size = x.shape) # define our y error as 0.2 y_err = 0.2 * np.ones(x.shape)  # Plot the data plt.errorbar(x, y_noisey, yerr = y_err , fmt = \"C0o\", label = \"Measured\") plt.ylabel(\"Y Values\") plt.xlabel(\"X Values\") plt.grid()  <p>Let's use <code>scipy.optimize.curve_fit</code></p> <p><code>curve_fit</code> will perform a lease-squares minimization: $$ (\\vec{y} - y_{model}(\\vec{x}, \\theta))^2 $$</p> <p>If errors are provided than it will perform a $\\chi^2$-minimization $$ \\frac{(\\vec{y} - y_{model}(\\vec{x}, \\theta))^2}{\\vec{\\Delta y}^2} $$</p> <p><code>curve_fit</code> returns the optimal parameters and the correlation matrix for the minimization allowing us to easily extract an uncertainty.</p> In\u00a0[122]: Copied! <pre># Use scipy curve_fit to perform a fit\nfrom scipy.optimize import curve_fit\n\n# Returns optimal (popt) and correation matrix (pcov)\npopt, pcov = curve_fit(\n    model, # Function we want to fit\n    x,     # x data\n    y_noisey,  # y data\n    p0 = [-0.2, 1, 5],  # initial guess\n    sigma=y_err   # error on y\n)\n\nx_plot = np.linspace(0,10)\nplt.errorbar(x, y_noisey, yerr = y_err , fmt = \"C0o\", label = \"Measured\")\nplt.plot(x_plot, model(x_plot, *popt), \"r--\", label = \"Best fit\")\nplt.ylabel(\"Y Values\")\nplt.xlabel(\"X Values\")\nplt.legend()\nplt.grid()\n\nparameter_errors = np.sqrt(np.diag(pcov))\nfor p, perr in zip(popt, parameter_errors):\n    print (f\"{p:0.3f} +/- {perr:0.3f}\")\n</pre> # Use scipy curve_fit to perform a fit from scipy.optimize import curve_fit  # Returns optimal (popt) and correation matrix (pcov) popt, pcov = curve_fit(     model, # Function we want to fit     x,     # x data     y_noisey,  # y data     p0 = [-0.2, 1, 5],  # initial guess     sigma=y_err   # error on y )  x_plot = np.linspace(0,10) plt.errorbar(x, y_noisey, yerr = y_err , fmt = \"C0o\", label = \"Measured\") plt.plot(x_plot, model(x_plot, *popt), \"r--\", label = \"Best fit\") plt.ylabel(\"Y Values\") plt.xlabel(\"X Values\") plt.legend() plt.grid()  parameter_errors = np.sqrt(np.diag(pcov)) for p, perr in zip(popt, parameter_errors):     print (f\"{p:0.3f} +/- {perr:0.3f}\") <pre>0.018 +/- 0.003\n0.117 +/- 0.031\n-2.519 +/- 0.071\n</pre> In\u00a0[123]: Copied! <pre># bootstrapping\nsamples = []\n\nfor i in range(100):\n    # Get random indices\n    # replace = True allows us to reuse indicies\n    # So we could be drawing an estimate from the [0th, 11th, 81st, 0th] elements of our array\n    rnd_int = np.random.choice(np.arange(len(x)), size=len(x), replace=True)\n    # Extract the corresponding values\n    x_samp = x[rnd_int]\n    y_samp = y_noisey[rnd_int]\n    y_samp_err = y_err[rnd_int]\n\n    # Apply fit\n    p, _ = curve_fit(model, x_samp, y_samp, sigma = y_samp_err)\n    # Store Result\n    samples.append(p)\nsamples = np.array(samples)\n</pre>  # bootstrapping samples = []  for i in range(100):     # Get random indices     # replace = True allows us to reuse indicies     # So we could be drawing an estimate from the [0th, 11th, 81st, 0th] elements of our array     rnd_int = np.random.choice(np.arange(len(x)), size=len(x), replace=True)     # Extract the corresponding values     x_samp = x[rnd_int]     y_samp = y_noisey[rnd_int]     y_samp_err = y_err[rnd_int]      # Apply fit     p, _ = curve_fit(model, x_samp, y_samp, sigma = y_samp_err)     # Store Result     samples.append(p) samples = np.array(samples) In\u00a0[96]: Copied! <pre>fig, axs = plt.subplots(1,3, figsize = (18,6))\nfor i in range(3):\n    mean = np.mean(samples[:,i])\n    std = np.std(samples[:,i])\n    \n    axs[i].hist(samples[:,i], alpha = 0.5)\n    axs[i].axvline(popt[i], color = \"r\", label = \"From Fit\")\n    axs[i].axvline(popt[i] - parameter_errors[i], ls = \"--\", color = \"r\")\n    axs[i].axvline(popt[i] + parameter_errors[i], ls = \"--\", color = \"r\")\n\n    axs[i].axvline(mean, color = \"C4\", label = \"Bootstrap\")\n    axs[i].axvline(mean - std, ls = \"--\", color = \"C4\")\n    axs[i].axvline(mean + std, ls = \"--\", color = \"C4\")\n    axs[i].axvline(p_true[i], color = \"k\", label = \"True\")\n    axs[i].grid()\n    axs[i].legend()\n</pre> fig, axs = plt.subplots(1,3, figsize = (18,6)) for i in range(3):     mean = np.mean(samples[:,i])     std = np.std(samples[:,i])          axs[i].hist(samples[:,i], alpha = 0.5)     axs[i].axvline(popt[i], color = \"r\", label = \"From Fit\")     axs[i].axvline(popt[i] - parameter_errors[i], ls = \"--\", color = \"r\")     axs[i].axvline(popt[i] + parameter_errors[i], ls = \"--\", color = \"r\")      axs[i].axvline(mean, color = \"C4\", label = \"Bootstrap\")     axs[i].axvline(mean - std, ls = \"--\", color = \"C4\")     axs[i].axvline(mean + std, ls = \"--\", color = \"C4\")     axs[i].axvline(p_true[i], color = \"k\", label = \"True\")     axs[i].grid()     axs[i].legend()      In\u00a0[97]: Copied! <pre>### What if we under estimate our errors?\n</pre> ### What if we under estimate our errors? In\u00a0[98]: Copied! <pre># let's add some gaussian noise\n# Increase spread to 0.3\ny_noisey = y + np.random.normal(loc = 0, scale = 0.3, size = x.shape)\n# define our y error as 0.1 (decreasing error)\ny_err = 0.1 * np.ones(x.shape)\n</pre> # let's add some gaussian noise # Increase spread to 0.3 y_noisey = y + np.random.normal(loc = 0, scale = 0.3, size = x.shape) # define our y error as 0.1 (decreasing error) y_err = 0.1 * np.ones(x.shape) In\u00a0[99]: Copied! <pre># Returns optimal (popt) and correation matrix (pcov)\npopt, pcov = curve_fit(\n    model, # Function we want to fit\n    x,     # x data\n    y_noisey,  # y data\n    p0 = [-0.2, 1, 5],  # initial guess\n    sigma=y_err   # error on y\n)\n\nx_plot = np.linspace(0,10)\nplt.errorbar(x, y_noisey, yerr = y_err , fmt = \"C0o\", label = \"Measured\")\nplt.plot(x_plot, model(x_plot, *popt), \"r--\", label = \"Best fit\")\nplt.ylabel(\"Y Values\")\nplt.xlabel(\"X Values\")\nplt.legend()\nplt.grid()\n</pre> # Returns optimal (popt) and correation matrix (pcov) popt, pcov = curve_fit(     model, # Function we want to fit     x,     # x data     y_noisey,  # y data     p0 = [-0.2, 1, 5],  # initial guess     sigma=y_err   # error on y )  x_plot = np.linspace(0,10) plt.errorbar(x, y_noisey, yerr = y_err , fmt = \"C0o\", label = \"Measured\") plt.plot(x_plot, model(x_plot, *popt), \"r--\", label = \"Best fit\") plt.ylabel(\"Y Values\") plt.xlabel(\"X Values\") plt.legend() plt.grid()  In\u00a0[100]: Copied! <pre># bootstrapping\nsamples = []\nfor i in range(100):\n    rnd_int = np.random.choice(np.arange(len(x)), size=len(x), replace=True)\n    x_samp = x[rnd_int]\n    y_samp = y_noisey[rnd_int]\n    y_samp_err = y_err[rnd_int]\n\n    p, _ = curve_fit(model, x_samp, y_samp, sigma = y_samp_err)\n    samples.append(p)\nsamples = np.array(samples)\n</pre> # bootstrapping samples = [] for i in range(100):     rnd_int = np.random.choice(np.arange(len(x)), size=len(x), replace=True)     x_samp = x[rnd_int]     y_samp = y_noisey[rnd_int]     y_samp_err = y_err[rnd_int]      p, _ = curve_fit(model, x_samp, y_samp, sigma = y_samp_err)     samples.append(p) samples = np.array(samples) In\u00a0[101]: Copied! <pre>fig, axs = plt.subplots(1,3, figsize = (18,6))\nfor i in range(3):\n    mean = np.mean(samples[:,i])\n    std = np.std(samples[:,i])\n    \n    axs[i].hist(samples[:,i], alpha = 0.5)\n    axs[i].axvline(popt[i], color = \"r\", label = \"From Fit\")\n    axs[i].axvline(popt[i] - parameter_errors[i], ls = \"--\", color = \"r\")\n    axs[i].axvline(popt[i] + parameter_errors[i], ls = \"--\", color = \"r\")\n\n    axs[i].axvline(mean, color = \"C4\", label = \"Bootstrap\")\n    axs[i].axvline(mean - std, ls = \"--\", color = \"C4\")\n    axs[i].axvline(mean + std, ls = \"--\", color = \"C4\")\n    axs[i].axvline(p_true[i], color = \"k\", label = \"True\")\n    axs[i].grid()\n    axs[i].legend()\n</pre> fig, axs = plt.subplots(1,3, figsize = (18,6)) for i in range(3):     mean = np.mean(samples[:,i])     std = np.std(samples[:,i])          axs[i].hist(samples[:,i], alpha = 0.5)     axs[i].axvline(popt[i], color = \"r\", label = \"From Fit\")     axs[i].axvline(popt[i] - parameter_errors[i], ls = \"--\", color = \"r\")     axs[i].axvline(popt[i] + parameter_errors[i], ls = \"--\", color = \"r\")      axs[i].axvline(mean, color = \"C4\", label = \"Bootstrap\")     axs[i].axvline(mean - std, ls = \"--\", color = \"C4\")     axs[i].axvline(mean + std, ls = \"--\", color = \"C4\")     axs[i].axvline(p_true[i], color = \"k\", label = \"True\")     axs[i].grid()     axs[i].legend()      In\u00a0[102]: Copied! <pre>### How to use bootstrapping to handle no-gaussian errors\n</pre> ### How to use bootstrapping to handle no-gaussian errors  In\u00a0[103]: Copied! <pre>def exp_model(x, n, tau):\n    return n*x**-tau\n</pre> def exp_model(x, n, tau):     return n*x**-tau      In\u00a0[104]: Copied! <pre>p_true = [10, 0.5]\nx_plot = np.linspace(0,10)\nplt.plot(x_plot, exp_model(x_plot, *p_true))\n</pre> p_true = [10, 0.5] x_plot = np.linspace(0,10) plt.plot(x_plot, exp_model(x_plot, *p_true)) <pre>/tmp/ipykernel_261108/1764079388.py:2: RuntimeWarning: divide by zero encountered in power\n  return n*x**-tau\n</pre> Out[104]: <pre>[&lt;matplotlib.lines.Line2D at 0x72ec8e584b10&gt;]</pre> In\u00a0[105]: Copied! <pre>lam = np.arange(6)\nfig, axs = plt.subplots(2,3, figsize = (11,6))\n\nfor l, ax in zip(lam, axs.ravel()):\n    rnd_x = np.random.poisson(lam = l, size = 1000)\n    ax.hist(rnd_x, alpha = 0.5, bins = -0.5 + np.arange(0,15))\n    ax.axvline(l)\n    ax.axvline(l - np.sqrt(l))\n    ax.axvline(l + np.sqrt(l))\n    ax.set_title(\"$\\lambda$ = \" + f\"{l}\")\n    ax.grid()\nfig.tight_layout()\n</pre> lam = np.arange(6) fig, axs = plt.subplots(2,3, figsize = (11,6))  for l, ax in zip(lam, axs.ravel()):     rnd_x = np.random.poisson(lam = l, size = 1000)     ax.hist(rnd_x, alpha = 0.5, bins = -0.5 + np.arange(0,15))     ax.axvline(l)     ax.axvline(l - np.sqrt(l))     ax.axvline(l + np.sqrt(l))     ax.set_title(\"$\\lambda$ = \" + f\"{l}\")     ax.grid() fig.tight_layout()      In\u00a0[110]: Copied! <pre>x = 10*np.random.random(100)\n# y = np.array([ \n#     np.random.poisson(lam = exp_model(x_i, *p_true), size = 1) \n#     for x_i in x \n# ])[:,0]\ny = np.random.poisson(lam = exp_model(x, *p_true))\n</pre> x = 10*np.random.random(100) # y = np.array([  #     np.random.poisson(lam = exp_model(x_i, *p_true), size = 1)  #     for x_i in x  # ])[:,0] y = np.random.poisson(lam = exp_model(x, *p_true))       In\u00a0[111]: Copied! <pre>x_plot = np.linspace(0,10)\ny_err = np.sqrt(y)\n# popt, pcov = curve_fit(exp_model, x, y, sigma = y_err)\npopt, pcov = curve_fit(exp_model, x, y)\n\nplt.plot(x_plot, exp_model(x_plot, *p_true))\nplt.errorbar(x, y, yerr = y_err, fmt = \"C0o\")\nplt.plot(x_plot, exp_model(x_plot, *popt))\nplt.grid()\n\nparameter_errors = np.sqrt(np.diag(pcov))\nfor pt, p, perr in zip(p_true, popt, parameter_errors):\n    print (f\"{pt:0.3f} -&gt; {p:0.3f} +/- {perr:0.3f}\")\n</pre> x_plot = np.linspace(0,10) y_err = np.sqrt(y) # popt, pcov = curve_fit(exp_model, x, y, sigma = y_err) popt, pcov = curve_fit(exp_model, x, y)  plt.plot(x_plot, exp_model(x_plot, *p_true)) plt.errorbar(x, y, yerr = y_err, fmt = \"C0o\") plt.plot(x_plot, exp_model(x_plot, *popt)) plt.grid()  parameter_errors = np.sqrt(np.diag(pcov)) for pt, p, perr in zip(p_true, popt, parameter_errors):     print (f\"{pt:0.3f} -&gt; {p:0.3f} +/- {perr:0.3f}\")  <pre>10.000 -&gt; 9.989 +/- 0.369\n0.500 -&gt; 0.485 +/- 0.016\n</pre> <pre>/tmp/ipykernel_261108/1764079388.py:2: RuntimeWarning: divide by zero encountered in power\n  return n*x**-tau\n</pre> In\u00a0[112]: Copied! <pre># bootstrapping\nsamples = []\nfor i in range(100):\n    rnd_int = np.random.choice(np.arange(len(x)), size=len(x), replace=True)\n    x_samp = x[rnd_int]\n    y_samp = y[rnd_int]\n    y_samp_err = y_err[rnd_int]\n\n    p, _ = curve_fit(exp_model, x_samp, y_samp)\n    samples.append(p)\nsamples = np.array(samples)\n</pre> # bootstrapping samples = [] for i in range(100):     rnd_int = np.random.choice(np.arange(len(x)), size=len(x), replace=True)     x_samp = x[rnd_int]     y_samp = y[rnd_int]     y_samp_err = y_err[rnd_int]      p, _ = curve_fit(exp_model, x_samp, y_samp)     samples.append(p) samples = np.array(samples) In\u00a0[113]: Copied! <pre>fig, axs = plt.subplots(1,2, figsize = (18,6))\nfor i in range(2):\n    mean = np.mean(samples[:,i])\n    std = np.std(samples[:,i])\n    \n    axs[i].hist(samples[:,i], alpha = 0.5)\n    axs[i].axvline(popt[i], color = \"r\", label = \"From Fit\")\n    axs[i].axvline(popt[i] - parameter_errors[i], ls = \"--\", color = \"r\")\n    axs[i].axvline(popt[i] + parameter_errors[i], ls = \"--\", color = \"r\")\n\n    axs[i].axvline(mean, color = \"C4\", label = \"Bootstrap\")\n    axs[i].axvline(mean - std, ls = \"--\", color = \"C4\")\n    axs[i].axvline(mean + std, ls = \"--\", color = \"C4\")\n    axs[i].axvline(p_true[i], color = \"k\", label = \"True\")\n    axs[i].grid()\n    axs[i].legend()\n</pre> fig, axs = plt.subplots(1,2, figsize = (18,6)) for i in range(2):     mean = np.mean(samples[:,i])     std = np.std(samples[:,i])          axs[i].hist(samples[:,i], alpha = 0.5)     axs[i].axvline(popt[i], color = \"r\", label = \"From Fit\")     axs[i].axvline(popt[i] - parameter_errors[i], ls = \"--\", color = \"r\")     axs[i].axvline(popt[i] + parameter_errors[i], ls = \"--\", color = \"r\")      axs[i].axvline(mean, color = \"C4\", label = \"Bootstrap\")     axs[i].axvline(mean - std, ls = \"--\", color = \"C4\")     axs[i].axvline(mean + std, ls = \"--\", color = \"C4\")     axs[i].axvline(p_true[i], color = \"k\", label = \"True\")     axs[i].grid()     axs[i].legend()      <p>We can see that the bootstrapped distributions are highly non-gaussian. It might not make sense to report the uncertainty as 1 sigma error on the fit parameters. Instead we might report using the bootstrapped quantiles. A common way to represent the uncertinty would be to report the 90% confidience/credibility interval. This says that:</p> <ul> <li>If we were to repeat this experiement 100 times, the measured value would be in this interval 90% of the time</li> </ul> In\u00a0[120]: Copied! <pre>fig, axs = plt.subplots(1,2, figsize = (18,6))\n\n\nfor i, pt in enumerate(p_true):\n    \n    axs[i].hist(samples[:,i], alpha = 0.5)\n    axs[i].axvline(popt[i], color = \"r\", label = \"From Fit\")\n    axs[i].axvline(popt[i] - parameter_errors[i], ls = \"--\", color = \"r\")\n    axs[i].axvline(popt[i] + parameter_errors[i], ls = \"--\", color = \"r\")\n\n    quan = np.quantile(samples[:,i], [0.05, 0.5, 0.95])\n\n    \n    axs[i].axvline(quan[1], color = \"C4\", label = \"Bootstrap\")\n    axs[i].axvline(quan[0], ls = \"--\", color = \"C4\")\n    axs[i].axvline(quan[2], ls = \"--\", color = \"C4\")\n    axs[i].axvline(p_true[i], color = \"k\", label = \"True\")\n    axs[i].grid()\n    axs[i].legend()\n    \n    print (f\"{pt:0.3f} -&gt; {quan[1]:0.3f} [{quan[0]:0.3f}, {quan[2]:0.3f}]\")\n</pre> fig, axs = plt.subplots(1,2, figsize = (18,6))   for i, pt in enumerate(p_true):          axs[i].hist(samples[:,i], alpha = 0.5)     axs[i].axvline(popt[i], color = \"r\", label = \"From Fit\")     axs[i].axvline(popt[i] - parameter_errors[i], ls = \"--\", color = \"r\")     axs[i].axvline(popt[i] + parameter_errors[i], ls = \"--\", color = \"r\")      quan = np.quantile(samples[:,i], [0.05, 0.5, 0.95])           axs[i].axvline(quan[1], color = \"C4\", label = \"Bootstrap\")     axs[i].axvline(quan[0], ls = \"--\", color = \"C4\")     axs[i].axvline(quan[2], ls = \"--\", color = \"C4\")     axs[i].axvline(p_true[i], color = \"k\", label = \"True\")     axs[i].grid()     axs[i].legend()          print (f\"{pt:0.3f} -&gt; {quan[1]:0.3f} [{quan[0]:0.3f}, {quan[2]:0.3f}]\") <pre>10.000 -&gt; 10.025 [9.340, 10.646]\n0.500 -&gt; 0.491 [0.469, 0.541]\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[129]: Copied! <pre>class Data():\n    \"\"\"Data Class\n\n    Class for holding x/y data with methods to calculate the properties \n    and some plotting functionalities. \n    \"\"\"\n    # The \"self\" keyword denotes data belonging to the class\n    def __init__(self, x_data : np.ndarray, y_data : np.ndarray) -&gt; None:\n        \"\"\"Initialization function\n\n        Copy x_data and y_data\n\n        Args:\n            x_data : data on the x axis\n            y_data : data on the y axis\n\n        Returns:\n            None\n        \"\"\"\n        self.x_data = x_data.copy()\n        self.y_data = y_data.copy()\n\n    # Member functions take \"self\" as the first argument\n    def calculate_properties(self) -&gt; None:\n        \"\"\"Calculate properties of X and Y data\n\n        Determine the mean and standard deviation of x_data and y_data.\n        Mean and standard deviation are stored as attributes within Data class\n\n        Args:\n            None\n\n        Returns:\n            None\n        \"\"\"\n        self.x_mean = np.mean(self.x_data)\n        self.y_mean = np.mean(self.y_data)\n\n        self.x_std = np.std(self.x_data)\n        self.y_std = np.std(self.y_data)\n\n    def plot_data(self, x_label : str = None, y_label : str = None) -&gt; plt.figure:\n        \"\"\"Plot X and Y data\n\n        Plot the X and Y data and return the figure. Add optional x/y labels.\n        Lines are added for the mean x/y and their standard deviations.\n\n        Args:\n            x_label : (optional) string for the label of the x axis. (Default None)\n            y_label : (optional) string for the label of the y axis. (Default None)\n\n        Returns:\n            figure with the plot of y(x)\n        \"\"\"\n\n        fig = plt.figure(figsize = (11,6))\n        plt.plot(self.x_data, self.y_data)\n\n        # Plotting the means as solid lines and the +/- 1 sigma as dashed lines\n        plt.axhline(self.y_mean, color = \"C1\", ls = \"-\", label = r\"$\\mu_{y}$\")\n        plt.axhline(self.y_mean + self.y_std, color = \"C1\", ls = \"--\")\n        plt.axhline(self.y_mean - self.y_std, color = \"C1\", ls = \"--\", label = r\"$\\mu_{y} \\pm  \\sigma_{y}$\")\n        \n        plt.axvline(self.x_mean, color = \"C2\", ls = \"-\", label = r\"$\\mu_{x}$\")\n        plt.axvline(self.x_mean + self.x_std, color = \"C2\", ls = \"--\", )\n        plt.axvline(self.x_mean - self.x_std, color = \"C2\", ls = \"--\", label = r\"$\\mu_{x} \\pm  \\sigma_{x}$\")\n        \n        if x_label is not None:\n            plt.xlabel(x_label)\n        if y_label is not None:\n            plt.ylabel(y_label)\n        plt.legend()\n        plt.grid()\n\n        return fig\n</pre> class Data():     \"\"\"Data Class      Class for holding x/y data with methods to calculate the properties      and some plotting functionalities.      \"\"\"     # The \"self\" keyword denotes data belonging to the class     def __init__(self, x_data : np.ndarray, y_data : np.ndarray) -&gt; None:         \"\"\"Initialization function          Copy x_data and y_data          Args:             x_data : data on the x axis             y_data : data on the y axis          Returns:             None         \"\"\"         self.x_data = x_data.copy()         self.y_data = y_data.copy()      # Member functions take \"self\" as the first argument     def calculate_properties(self) -&gt; None:         \"\"\"Calculate properties of X and Y data          Determine the mean and standard deviation of x_data and y_data.         Mean and standard deviation are stored as attributes within Data class          Args:             None          Returns:             None         \"\"\"         self.x_mean = np.mean(self.x_data)         self.y_mean = np.mean(self.y_data)          self.x_std = np.std(self.x_data)         self.y_std = np.std(self.y_data)      def plot_data(self, x_label : str = None, y_label : str = None) -&gt; plt.figure:         \"\"\"Plot X and Y data          Plot the X and Y data and return the figure. Add optional x/y labels.         Lines are added for the mean x/y and their standard deviations.          Args:             x_label : (optional) string for the label of the x axis. (Default None)             y_label : (optional) string for the label of the y axis. (Default None)          Returns:             figure with the plot of y(x)         \"\"\"          fig = plt.figure(figsize = (11,6))         plt.plot(self.x_data, self.y_data)          # Plotting the means as solid lines and the +/- 1 sigma as dashed lines         plt.axhline(self.y_mean, color = \"C1\", ls = \"-\", label = r\"$\\mu_{y}$\")         plt.axhline(self.y_mean + self.y_std, color = \"C1\", ls = \"--\")         plt.axhline(self.y_mean - self.y_std, color = \"C1\", ls = \"--\", label = r\"$\\mu_{y} \\pm  \\sigma_{y}$\")                  plt.axvline(self.x_mean, color = \"C2\", ls = \"-\", label = r\"$\\mu_{x}$\")         plt.axvline(self.x_mean + self.x_std, color = \"C2\", ls = \"--\", )         plt.axvline(self.x_mean - self.x_std, color = \"C2\", ls = \"--\", label = r\"$\\mu_{x} \\pm  \\sigma_{x}$\")                  if x_label is not None:             plt.xlabel(x_label)         if y_label is not None:             plt.ylabel(y_label)         plt.legend()         plt.grid()          return fig                  In\u00a0[130]: Copied! <pre>x = np.linspace(-3 * np.pi, 3 * np.pi, 100 )\ny = np.sin(x)\n\nmy_data = Data(x, y)\nmy_data.calculate_properties()\nfig = my_data.plot_data(y_label=\"Sin(x)\")\n</pre> x = np.linspace(-3 * np.pi, 3 * np.pi, 100 ) y = np.sin(x)  my_data = Data(x, y) my_data.calculate_properties() fig = my_data.plot_data(y_label=\"Sin(x)\")  In\u00a0[131]: Copied! <pre>class TimeSeries(Data):\n    \"\"\"Time Series Data Class\n\n    Class for holding x/y data with methods to calculate the properties \n    and some plotting functionalities. \n    The X data is assumed to be time\n    \"\"\"\n    def __init__(self, x_data : np.ndarray, y_data: np.ndarray) -&gt; None:\n        \"\"\"Initialization function\n\n        Copy x_data and y_data\n\n        Args:\n            x_data : data on the x axis\n            y_data : data on the y axis\n\n        Returns:\n            None\n        \"\"\"\n        # We use the \"super\" keyword to call parent class functions\n        super().__init__(x_data, y_data)\n\n    # We can overwrite functions\n    def plot_data(self) -&gt; plt.figure:\n        \"\"\"Plot X and Y data\n\n        Plot the X and Y data and return the figure. Add optional x/y labels.\n        Lines are added for the mean x/y and their standard deviations.\n\n        Args:\n            None\n\n        Returns:\n            figure with the plot of y(x)\n        \"\"\"\n        return super().plot_data(x_label = \"Time\", y_label = \"AU\")\n    \n    # We can also define new functions\n    def add_to_data(self, y : float) -&gt; None:\n        \"\"\"Add a constant offset to y data.\n\n        A constant offset is added to self.y_data. \n        The properties (mean and std) are calculated for the adjusted dataset\n\n        Args:\n            y : constant offset to be added to self.y_data\n\n        Returns:\n            None\n        \"\"\"\n        self.y_data += y\n        # Recalculate the properties\n        super().calculate_properties()\n</pre> class TimeSeries(Data):     \"\"\"Time Series Data Class      Class for holding x/y data with methods to calculate the properties      and some plotting functionalities.      The X data is assumed to be time     \"\"\"     def __init__(self, x_data : np.ndarray, y_data: np.ndarray) -&gt; None:         \"\"\"Initialization function          Copy x_data and y_data          Args:             x_data : data on the x axis             y_data : data on the y axis          Returns:             None         \"\"\"         # We use the \"super\" keyword to call parent class functions         super().__init__(x_data, y_data)      # We can overwrite functions     def plot_data(self) -&gt; plt.figure:         \"\"\"Plot X and Y data          Plot the X and Y data and return the figure. Add optional x/y labels.         Lines are added for the mean x/y and their standard deviations.          Args:             None          Returns:             figure with the plot of y(x)         \"\"\"         return super().plot_data(x_label = \"Time\", y_label = \"AU\")          # We can also define new functions     def add_to_data(self, y : float) -&gt; None:         \"\"\"Add a constant offset to y data.          A constant offset is added to self.y_data.          The properties (mean and std) are calculated for the adjusted dataset          Args:             y : constant offset to be added to self.y_data          Returns:             None         \"\"\"         self.y_data += y         # Recalculate the properties         super().calculate_properties()     In\u00a0[132]: Copied! <pre>my_time_series = TimeSeries(x, y)\n# Calling a function from the parent class\nmy_time_series.calculate_properties()\n# Call a function that only exists in the child class\nmy_time_series.add_to_data(10)\n# Call overridden function\nfig = my_time_series.plot_data()\n</pre> my_time_series = TimeSeries(x, y) # Calling a function from the parent class my_time_series.calculate_properties() # Call a function that only exists in the child class my_time_series.add_to_data(10) # Call overridden function fig = my_time_series.plot_data() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Python/python-tutorial/#what-is-python","title":"What is Python?\u00b6","text":"<p>Python is one of the most widely used programming languages today. It is a high-level, interpreted programming language that emphasizes readability. Python's readability makes it often the language of choice for both those beginning to learn programming and large collaborative projects.</p> <p>Unlike compiled languages like C++ and Rust, Python is an interpreted language. This means that the Python interpreter will compile the code at runtime, rather than compiling it down to a binary prior to running. This has the benefit of making Python an excellent language for quickly developing and debugging code.</p> <p>Interactive environments like Jupyter Notebooks allow for clear frameworks for developing, testing, sharing, and presenting code.</p>"},{"location":"Python/python-tutorial/#installing-python-and-working-with-environments","title":"Installing Python and working with Environments\u00b6","text":"<p>We will use virtual environments in this tutorial. I recommend that you use an environment manager such as conda or mamba/micromamba.</p> <p>Mamba/Micromamba is a \"fast, robust, and cross-platform package manager\" that offers significant performance advantages over conda when installing and resolving packages.</p> <p>We can create a new environment using the following command:</p> <pre><code>mamba/conda create -n my_environment python=3.9 numpy matplotlib\n</code></pre> <p>Here, we are creating a new environment called <code>my_environment</code>, which installs <code>Python 3.9</code> and the packages <code>numpy</code> and <code>matplotlib</code>. We can activate this environment using:</p> <pre><code>mamba/conda activate my_environment\n</code></pre> <p>Running <code>which python</code> confirms that we are utilizing the Python installed within our environment.</p> <p>Environments enable us to install conflicting versions for various projects. For instance, suppose we need to execute older code dependent on Python 2.7, which is incompatible with modern packages. In that scenario, we can establish an environment with Python 2.7 and install versions of packages compatible with it. This action won't impact any other environment we've established.</p> <p>For this workshop, we will utilize the following environment:</p> <pre><code>mamba/conda create -n workshop -c conda-forge python=3.10 numpy matplotlib scipy pandas jupyter jupyterlab ipykernel\n</code></pre>"},{"location":"Python/python-tutorial/#installing-additional-packages","title":"Installing additional packages\u00b6","text":"<p>Once in the environment, we can install additional packages using the <code>install</code> command:</p> <pre><code>mamba/conda install scipy\n</code></pre> <p>This installs the package <code>scipy</code>, which is a statistics package compatible with <code>numpy</code> data types. We can also remove a package using:</p> <pre><code>mamba/conda remove scipy\n</code></pre> <p>which would remove the package <code>scipy</code>. Some packages will require installation from a specific collection of packages:</p> <pre><code>mamba/conda install -c conda-forge astroquery\n</code></pre> <p>This installs the package <code>astroquery</code>, a package that allows querying astronomical databases like Simbad, which is part of the collection <code>conda-forge</code>.</p> <p>If we ever want to see which packages are currently installed, we can use something like:</p> <pre><code>mamba/conda list\n</code></pre> <p>which gives a list of installed packages and their versions. We can output this to a machine-readable file using:</p> <pre><code>mamba/conda list -e &gt; requirements.txt\n</code></pre> <p>Ensuring that users are using a standard environment can help debug version-specific bugs.</p>"},{"location":"Python/python-tutorial/#hello-world","title":"Hello World\u00b6","text":"<p>Python is a dynamically typed language, which means that the type of a variable does not need to be known until that variable is used. This also means that we can change the type of a variable at any stage of the code.</p> <p>We can define variables like:</p> <pre>my_string = \"Hello\"\n</pre> <p>We can also overwrite variables like:</p>"},{"location":"Python/python-tutorial/#basic-operations","title":"Basic Operations\u00b6","text":"<ul> <li>Addition: a + b</li> <li>Multiplication: a * b</li> <li>Division: a / b</li> <li>Integer division: a // b</li> <li>Modulus: a % b</li> <li>Power: a ** b</li> <li>Equal: a == b</li> <li>Not equal: a != b</li> <li>Less than: a &lt; b</li> <li>Less than or equal to: a &lt;= b</li> <li>Greater than: a &gt; b</li> <li>Greater than or equal to: a &gt;= b</li> </ul>"},{"location":"Python/python-tutorial/#other-logical-statements","title":"Other logical statements:\u00b6","text":"<ul> <li>Or:<ul> <li>a or b</li> <li>a | b</li> </ul> </li> <li>And:<ul> <li>a and b</li> <li>a &amp; b</li> </ul> </li> </ul> <p>For example, if a multiplied by b is less than c divided by d, and e is greater than 10:</p> <pre><code>(a * b &lt; c / d) and (e &gt; 10)\n</code></pre>"},{"location":"Python/python-tutorial/#basic-data-types","title":"Basic Data Types\u00b6","text":"<p>Python has several basic data types:</p> <ul> <li><code>int</code>: integers: -3, -2, -1, 0, 1, 2, 3, etc.</li> <li><code>float</code>: non-integers: 3.14, 42.0, etc.</li> <li><code>bool</code>: boolean. Note in Python, <code>True</code>/<code>False</code> start with a capital letter.<ul> <li><code>x = false</code> will give an error, while <code>x = False</code> will not.</li> </ul> </li> <li><code>str</code>: strings of characters. In Python, strings are wrapped in single (<code>''</code>) or double (<code>\"\"</code>) quotation marks. They can be combined when using strings:<ul> <li><code>my_str = \"hello\"</code>, <code>my_str = 'apple'</code>, <code>answer = 'Computer says \"no\"'</code> - all of these will work just fine.</li> <li><code>my_str = \"Goodbye'</code> will not work since we need to match the quotation marks properly.</li> </ul> </li> </ul> <p>We can cast from one data type to another using the format:</p> <pre>x = 1.3\ny = int(x)\n</pre> <p>Here, <code>x</code> is cast to the <code>int</code> type <code>y</code>. We can also determine the type of a variable using the type function:</p> <pre>type(x)\n</pre>"},{"location":"Python/python-tutorial/#basic-collections-of-data","title":"Basic Collections of Data\u00b6","text":"<p>Python offers several ways to organize and store data efficiently. These data structures play a vital role in managing and manipulating information within a program.</p>"},{"location":"Python/python-tutorial/#lists","title":"Lists\u00b6","text":"<p>A <code>list</code> in Python is a versatile and mutable collection of items, ordered and enclosed within square brackets <code>[]</code>. It allows storing various data types, including integers, strings, or even other lists. Lists are dynamic, meaning elements can be added, removed, or changed after creation using methods like <code>append()</code>, <code>insert()</code>, <code>remove()</code>, or by directly assigning values to specific indices.</p> <pre>my_list = [1, 2, 3, 'apple', 'banana', 'cherry']\n</pre>"},{"location":"Python/python-tutorial/#string-slicing","title":"String slicing\u00b6","text":"<p>In Python we can slice lists (and arrays, more on this later) to access sub sections of the list. We use the syntax:</p> <pre>my_list[start:stop]\n</pre> <p>where <code>start</code> and <code>stop</code> are the range that we want to access, with <code>stop</code> being exclusive. We can also access the last element with:</p> <pre>my_list[-1]\n</pre> <p>with <code>-1</code> being the last element (<code>-2</code> being the second last... etc.). To slice from the 2 element to the second last we would do:</p> <pre>my_list[2:-2]\n</pre>"},{"location":"Python/python-tutorial/#sets","title":"Sets\u00b6","text":"<p><code>Sets</code> are useful collection in Python. They are an unordered and mutable collection of unique elements. Sets are enclosed in curly braces <code>{}</code> and support set operations like union, intersection, and difference. They are efficient for tasks requiring unique elements and membership testing.</p>"},{"location":"Python/python-tutorial/#tuples","title":"Tuples\u00b6","text":"<p>A <code>tuple</code> is similar to a list but is immutable once created, denoted by parentheses <code>()</code>. Tuples are often used to store related pieces of information together and are faster than lists due to their immutability. They are commonly utilized for items that shouldn't be changed, such as coordinates or configuration settings.</p> <pre>my_tuple = (4, 5, 6, 'dog', 'cat', 'rabbit')\n</pre>"},{"location":"Python/python-tutorial/#dictionaries","title":"Dictionaries\u00b6","text":"<p>A <code>dictionary</code> is an unordered collection of key-value pairs enclosed in curly braces <code>{}</code>. Each element in a dictionary is accessed by its associated key rather than an index. Dictionaries are suitable for storing data where retrieval by a specific key is a priority. They are flexible and allow storing various data types as values.</p> <pre>my_dict = {'name': 'Alice', 'age': 25, 'country': 'USA'}\n</pre>"},{"location":"Python/python-tutorial/#looping","title":"Looping\u00b6","text":"<p>In Python, there are two primary methods of looping: <code>for</code> loops and <code>while</code> loops.</p>"},{"location":"Python/python-tutorial/#for-loops","title":"For Loops\u00b6","text":"<p><code>for</code> loops use the syntax <code>for variable in iterable</code>, where <code>iterable</code> is some sequence-like object, and <code>variable</code> represents the current instance within the loop. The block of code to be executed within the loop is designated by indentation. Python's standard is to use 4 spaces for indentation, but using tabs (consistently) is also common (avoid mixing spaces and tabs). For example:</p> <pre>my_list = [1, 2, 3, 4, 5]\nfor num in my_list:\n    print(num)\n</pre> <p>This loop iterates through the elements of my_list, assigning each element to the variable num, and then prints each element.</p> <p>The <code>range()</code> function is often used with <code>for</code> loops to generate a sequence of numbers. It allows iterating a specific number of times or generating a sequence within a range.</p>"},{"location":"Python/python-tutorial/#while","title":"While\u00b6","text":"<p><code>while</code> loops execute a block of code as long as a specified condition is <code>True</code>. Care should be taken to avoid infinite loops where the condition always remains <code>True</code>. The syntax for a <code>while</code> loop is <code>while condition:</code> followed by an indented block of code.</p> <p>The <code>break</code> statement can be used to exit a loop prematurely based on a condition, while <code>continue</code> skips the current iteration and proceeds to the next one.</p> <p>When evaluting the <code>condition</code> anything that isn't <code>False</code>, <code>0</code> or <code>None</code> is considered to be <code>True</code>.</p>"},{"location":"Python/python-tutorial/#if-elif-else-statements","title":"if-elif-else Statements\u00b6","text":"<p><code>if</code>-<code>elif</code>-<code>else</code> statements allow us to control the flow of the code based on conditions. They take the syntax:</p> <pre>if condition1:\n    # condition 1 code\nelif condition2:\n    # condition 2 code\nelif condition3:\n    # condition 3 code\nelse:\n    # default code\n</pre> <p>Notice that the <code>if</code> and <code>elif</code> statements take logical expressions, while <code>else</code> does not. You can have any number of <code>elif</code> branches but only one <code>if</code> branch and at most one <code>else</code> branch.</p> <p>This construct allows for branching based on multiple conditions. Python evaluates each condition sequentially. If <code>condition1</code> is true, it executes the code block under <code>condition1</code>. If <code>condition1</code> is false, it checks <code>condition2</code>, and so on. If none of the conditions are true, the code block under <code>else</code> (if provided) is executed as the default action.</p>"},{"location":"Python/python-tutorial/#functions","title":"Functions\u00b6","text":"<p>Creating functions is an effective method to enhance code reusability and streamline debugging. When there's a block of code intended to be executed multiple times, encapsulating it within a function proves beneficial. This practice minimizes human error by necessitating modifications in only one location. Moreover, employing functions to execute smaller code segments can significantly enhance code readability and simplify the debugging process.</p> <p>In Python, we define a function using the <code>def</code> keyword:</p>"},{"location":"Python/python-tutorial/#functions-naming-conventions-and-documentation","title":"Functions, Naming Conventions and Documentation\u00b6","text":"<p>When writing functions and classes (more on this later), we should conform to a consistent convention. This helps both users and developers to better understand the code, improving the ability to use and develop the code.</p> <p>The convention we'll follow in this example is the Google Python Style Guide. Let's look at some examples of why this is useful.</p> <p>Consider the following. We have a function <code>calc</code> which takes three arguments (<code>x</code>, <code>b</code> and <code>i</code>).</p>"},{"location":"Python/python-tutorial/#packages","title":"Packages\u00b6","text":"<p>Python boasts an extensive array of packages developed by the community. In Python, we use the <code>import statement</code> to bring in packages or specific sections of packages into our code.</p> <pre>import package as p\n</pre> <p>In the example above, we import a package named <code>package</code>. The <code>as p</code> statement allows us to assign an alias, <code>p</code>, to the imported package. This aliasing technique proves beneficial when accessing objects from within a package that might share a common name with objects in other packages. For instance:</p>"},{"location":"Python/python-tutorial/#working-with-numpy","title":"Working with Numpy\u00b6","text":"<p>Numpy offers highly optimized functionality for typical matrix and vector operations, with the cornerstone being the numpy array. Arrays resemble lists in their mutability but differ in that they can only contain a single data type.</p>"},{"location":"Python/python-tutorial/#decorators","title":"Decorators\u00b6","text":"<p>Decorators allow us to modify the behavior of a function. They are essentially a function, that take another function as an arguement and modifies the behavior of the function.</p> <p>Let's define a logging decorator:</p>"},{"location":"Python/python-tutorial/#working-with-pandas","title":"Working with Pandas\u00b6","text":"<p>Pandas is an open-source data manipulation and analysis library in Python that's built on top of NumPy. It provides high-level data structures and a variety of tools for working with structured data.</p> <p>The core data structure in Pandas is the DataFrame, which is essentially a two-dimensional array with labeled axes (rows and columns). This DataFrame object is built upon NumPy's ndarray, utilizing its efficient operations and functions.</p>"},{"location":"Python/python-tutorial/#data-analysis-with-python","title":"Data Analysis with Python\u00b6","text":"<p>Python is a great language for high-level data analysis, with jupyter notebooks providing a great \"analysis notebook\" for documenting analysis and displaying results.</p> <p>Let's look at how we might reduce and analyze data using Python and extract some meaningful results.</p>"},{"location":"Python/python-tutorial/#fitting-a-model-to-data","title":"Fitting a model to data\u00b6","text":"<ul> <li>scipy optimize package</li> <li>numpy polyfit</li> <li>Error propagation</li> <li>bootstrapping</li> </ul> <p>Let's start by creating a data set using numpy.</p>"},{"location":"Python/python-tutorial/#are-we-confident-in-our-uncertainty","title":"Are we confident in our uncertainty?\u00b6","text":"<p>It can often be difficult to quantify our uncertainties. Bootstrapping is a useful method to estimate our uncertaities.</p> <p>Assuming we have independent data points, we can randomly sample our data, apply out fit to that data and then repeat a number of times, to estimate the distribution of best fit values.</p> <p></p>"},{"location":"Python/python-tutorial/#poisson-distribution","title":"Poisson Distribution\u00b6","text":"<p>$$p(X = k ; \\lambda) = \\frac{e^{-\\lambda}\\lambda^{k}}{k!}$$</p> <p>Where $k$ is the observed counts, $\\lambda$ is the mean counts. Mean is $\\lambda$, standard deviation is $\\sqrt{\\lambda}$. In counting experiments we typically say if $f= N$, then, $\\Delta f = \\sqrt{N}$.</p> <p>Does this mean that its appropriate to use $\\sqrt{N}$ in a $\\chi^2$ fit?</p>"},{"location":"Python/python-tutorial/#classes-in-python","title":"Classes in Python\u00b6","text":"<p>Classes in Python serve as templates or blueprints defining the attributes (data) and behaviors (methods) of objects. They encapsulate both data and methods that operate on that data within a single structure, promoting code organization and reusability.</p> <p>To create a class in Python, you use the <code>class</code> keyword, allowing you to define properties (attributes) and behaviors (methods) within it.</p>"},{"location":"Python/python-tutorial/#attributes-and-methods","title":"Attributes and Methods\u00b6","text":"<p>Attributes represent the data associated with a class, while methods are functions defined within the class that can access and manipulate this data. These methods can perform various operations on the attributes, thereby altering or providing access to the data encapsulated within the class.</p>"},{"location":"Python/python-tutorial/#example-of-a-simple-class","title":"Example of a Simple Class\u00b6","text":"<p>Consider the following example of a basic class in Python:</p> <pre>class Car:\n    def __init__(self, make, model, year):\n        self.make = make\n        self.model = model\n        self.year = year\n\n    def get_details(self):\n        return f\"{self.year} {self.make} {self.model}\"\n</pre> <p>The <code>__init__</code> function is the \"initialization\" or constructor of the class. This function is called when the object is created.</p>"},{"location":"Python/python-tutorial/#inheritance-in-python","title":"Inheritance in Python\u00b6","text":"<p>Inheritance is a fundamental concept in object-oriented programming that allows a new class to inherit properties and behaviors (attributes and methods) from an existing class. This concept promotes code reuse, enhances readability, and enables the creation of more specialized classes.</p>"},{"location":"Python/python-tutorial/#basics-of-inheritance","title":"Basics of Inheritance\u00b6","text":"<p>In Python, inheritance is achieved by specifying the name of the parent class(es) inside the definition of a new class. The new class, also known as the child class or subclass, inherits all attributes and methods from its parent class or classes, referred to as the base class or superclass.</p>"},{"location":"Python/python-tutorial/#syntax-for-inheriting-classes","title":"Syntax for Inheriting Classes\u00b6","text":"<p>The syntax for creating a subclass that inherits from a superclass involves passing the name of the superclass inside parentheses when defining the subclass:</p> <pre>class ParentClass:\n    # Parent class attributes and methods\n\nclass ChildClass(ParentClass):\n    # Child class attributes and methods\n</pre> <p>Here, <code>ChildClass</code> is inheriting from <code>ParentClass</code>, which means <code>ChildClass</code> will inherit all attributes and methods defined in <code>ParentClass</code>.</p>"},{"location":"Python/OOP/","title":"Object Orientated Programming and Packaging","text":"<p>So you've gotten your code to the stage where you'd like to provide a stable, usable, code for other to use and to build upon. This is a fantastic stage to reach! Whether your code is going to be completely open source (visible to everyone in the world) or only usable to a number of collaborators, there are some key things we should keep in mind when making our code available for others to use.</p> <p>In this workshop we'll walk taking out code from a simple Python script, to a versitile, well-documented, Python package. We'll be focusing on the following topics:</p> <ul> <li>Classes and Inheritance</li> <li>Documentation</li> <li>Packaging</li> <li>Testing</li> </ul>"},{"location":"Python/OOP/#starting-point","title":"Starting Point","text":"<p>Let's start with some pre-existing code that we've written and would like to package up.</p> analaysis_script.py<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n\n# load in the data from a csv file\ndata = np.loadtxt(\"data.csv\", delimiter=\",\", comments=\"#\")\n\n# Get the properties of the data\ndata_mean = np.zeros(data.shape[1])\ndata_std = np.zeros(data.shape[1])\ndata_min = np.zeros(data.shape[1])\ndata_max = np.zeros(data.shape[1])\n\n# Set the properties of the data in a loop\nfor i in range(data.shape[1]):\n    data_mean[i] = data[:,i].mean()\n    data_std[i] = data[:,i].std()\n    data_min[i] = data[:,i].min()\n    data_max[i] = data[:,i].max()\n\n\n\n# Perform a min/max normalization of some columns\ncolumns = [0,1,2]\nfor col in columns:\n    data[:,col] = (data[:,col] - data_min[col]) / (data_max[col] - data_min[col])\n\n# Perform a standard normalization of some other columns\ncolumns = [3,4,5]\nfor col in columns:\n    data[:,col] = (data[:,col] - data_mean[col]) / data_std[col]\n\n# Recalculate the properties of the data\nfor i in range(data.shape[1]):\n    data_mean[i] = data[:,i].mean()\n    data_std[i] = data[:,i].std()\n    data_min[i] = data[:,i].min()\n    data_max[i] = data[:,i].max()\n\n\n# Print out a summary of the datasets\nfor i in range(data.shape[1]):\n    print(f\"Column {i} has a mean value of {data_mean[i]:0.2f} and a standard deviation of {data_std[i]:0.2f}\")\n    print(f\"The minimum value is {data_min[i]:0.2f} and the maximum value is {data_max[i]:0.2f}\")\n    print(\"\\n\")\n\n\n# Plot the data in a histogram\nfor i in range(data.shape[1]):\n    fig = plt.figure()\n    plt.hist(data[:,i], bins = 20, alpha = 0.5, label = f\"Column {i}\")\n    fig.show()\n\n\n# Dump updated data to a file\nnp.savetxt(\"normalized_data.csv\", data, delimiter=\",\")\n\ninput(\"Press Enter to continue...\")\n</code></pre> <p>Let's break this down a little:</p> <ul> <li>On lines 1-2 we're importing the required libraries, namely <code>numpy</code> and <code>matplotlib</code>. </li> <li>On lines 6-12 we're loading in data and assigning empty arrays for properties of our data.</li> <li>On lines 15-19 we're assigning to the property arrays.</li> <li>On lines 23-26 and 28-31 we're normalizing with either a min/max normalization or a standard normalization.</li> <li>On lines 34-38 we're resetting the property arrays.</li> <li>On lines 42-45 we're printing out a summary of the data column.</li> <li>On lines 49-52 we're plotting the data to a histogram.</li> <li>On line 56 we're dumping the modified data to a new CSV file.</li> </ul> <p>Keeping in mind that we want our code to be versatile, easy to use and well documented, let's consider the following takeaways:</p> <ul> <li>The user needs to have <code>numpy</code> and <code>matplotlib</code> to run this code. These should be considered requirements or dependencies.</li> <li>When we load in data, we perform a number of operations and save some derived data. The user shouldn't need to worry about whether these arrays are getting properly set.</li> <li>We want to normalize the data using two similar ideas but different implementations. The user should expect similar interface when using either method to normalize the data.</li> <li>When the data is modified, we need to update the derived data. The user again shouldn't need to worry about correctly setting them. This would introduce a potential source of error.</li> <li>The derived data is used when printing summaries. We need to make sure these are up-to-date.</li> <li>When dumping to a CSV file, there is no check to see if we're overwriting data. This could lead to an unintentional loss of data.</li> <li>There is a lot of repeated code. Repeated code is highly susceptible to errors as one needs to make sure the same change is made in multiple places.</li> </ul> <p>With these points in mind, let's start writing a Python package to make this workflow more user friendly.</p>"},{"location":"Python/OOP/#python-package-layout","title":"Python Package Layout","text":"<p>Let's start off by created a new directory called <code>my_package</code>.  Within <code>my_package</code> we'll create an empty file called <code>__init__.py</code> (here <code>__file__.py</code> is pronounced \"dunder file\", hence <code>__init__.py</code> is \"dunder init\", calling it \"init\" is also understandable).  This <code>__init__.py</code> is a special file in Python with a few functions. Firstly <code>__init__.py</code> tells Python that files in this directory can be <code>import</code>-ed into python.  Once Python sees <code>__init__.py</code> it knows that any <code>.py</code> file within this directory can be <code>import</code>-ed.  Secondly, <code>__init__.py</code> will act as an entry point to the package (or sub-package, more on this later).  Within <code>__init__.py</code> we can specify what is <code>import</code>-ed by default using <code>from my_package import *</code>.  We can do things like specify alias for functions within <code>__init__.py</code>. Let's say we have the following: __init__.py<pre><code>...\ndef my_super_complicated_long_function_name():\n    ...\n\n\nshort_name = my_super_complicated_long_function_name\n</code></pre> On line 2 we define a cumbersome function name that will be awkward to keep typing out. However, on line 6 we assign <code>short_name</code> to be <code>my_super_complicated_long_function_name</code>.  We can then import this function as <code>from my_package import short_name</code>. We'll come back to this in more detail later, for now let's just stick with an empty <code>my_package/__init__.py</code></p> <p>Now let's just copy the <code>analysis_script.py</code> into <code>my_package</code> and wrap the main body of the script into a simple function, which takes the input filename as an argument: my_package/analaysis_script.py<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\ndef run_analysis(input_data):\n    # load in the data from a csv file\n    data = np.loadtxt(input_data, delimiter=\",\", comments=\"#\")\n\n    # Get the properties of the data\n    data_mean = np.zeros(data.shape[1])\n    data_std = np.zeros(data.shape[1])\n    data_min = np.zeros(data.shape[1])\n    data_max = np.zeros(data.shape[1])\n\n    # Set the properties of the data in a loop\n    for i in range(data.shape[1]):\n        data_mean[i] = data[:,i].mean()\n        data_std[i] = data[:,i].std()\n        data_min[i] = data[:,i].min()\n        data_max[i] = data[:,i].max()\n\n\n\n    # Perform a min/max normalization of some columns\n    columns = [0,1,2]\n    for col in columns:\n        data[:,col] = (data[:,col] - data_min[col]) / (data_max[col] - data_min[col])\n\n    # Perform a standard normalization of some other columns\n    columns = [3,4,5]\n    for col in columns:\n        data[:,col] = (data[:,col] - data_mean[col]) / data_std[col]\n\n    # Recalculate the properties of the data\n    for i in range(data.shape[1]):\n        data_mean[i] = data[:,i].mean()\n        data_std[i] = data[:,i].std()\n        data_min[i] = data[:,i].min()\n        data_max[i] = data[:,i].max()\n\n\n    # Print out a summary of the datasets\n    for i in range(data.shape[1]):\n        print(f\"Column {i} has a mean value of {data_mean[i]:0.2f} and a standard deviation of {data_std[i]:0.2f}\")\n        print(f\"The minimum value is {data_min[i]:0.2f} and the maximum value is {data_max[i]:0.2f}\")\n        print(\"\\n\")\n\n\n    # Plot the data in a histogram\n    for i in range(data.shape[1]):\n        fig = plt.figure()\n        plt.hist(data[:,i], bins = 20, alpha = 0.5, label = f\"Column {i}\")\n        fig.show()\n\n\n    # Dump updated data to a file\n    np.savetxt(\"normalized_data.csv\", data, delimiter=\",\")\n</code></pre></p> <p>Let's also add a test data file to the base directory, this can be downloaded from here  TO DO.</p> <p>Our folder layout should look like this: <pre><code>.\n\u251c\u2500\u2500 data.csv\n\u2514\u2500\u2500 my_package\n    \u251c\u2500\u2500 analysis_script.py\n    \u2514\u2500\u2500 __init__.py\n</code></pre></p> <p>From here we can try and run the code (I'm using iPython): <pre><code>In [1]: from my_package.analysis_script import run_analysis\nIn [2]: run_analysis(\"./data.csv\")\n</code></pre></p> <p>Which worked! However, this is only local to this directory. If we change to a different location and try to rerun the above: <pre><code>In [1]: from my_package.analysis_script import run_analysis\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[1], line 1\n----&gt; 1 from my_package.analysis_script import run_analysis\n\nModuleNotFoundError: No module named 'my_package'\n</code></pre></p> <p>We want to package to be installed so that we can run the code from anywhere on our system.  Let's look at how to do that next.</p>"},{"location":"Python/OOP/#environment-wide-install-using-pip-and-pyprojecttoml","title":"Environment-wide install using pip and pyproject.toml","text":"<p>There are a few different methods we can choose when creating a package and installing it:</p> <ul> <li>Append the PYTHONPATH: Using this method we simply modify the location where Python searches when running <code>import</code>. This isn't a good choice, it requires user messing around with environmental variables. Not all users will be comfterble doing this. </li> <li>Install using <code>setup.py</code>. This method will compile the Python code to byte code and place it into the correct location for Python to find when running <code>import</code>. Using this method, any user who has <code>pip</code> installed can install our package using <code>pip install .</code>. This isn't a bad option, it is also fairly widely used, however Python standards and package maintainers tend to move away from this method.</li> <li>Install using a <code>pyproject.toml</code> file. This is very similar to the <code>setup.py</code> method, except we'll be defining things like package name, file locations, additional data file, project metadata, all within the <code>pyproject.toml</code> file. </li> </ul> <p><code>pyproject.toml</code> is the preferred method for a number of reasons.  Firstly, it is tool agnostic, allowing us to use the installation tools that we are most fimilre with. We can also include the requirements/dependencies within the <code>pyproject.toml</code> file. A <code>toml</code> file is human-readable file format which uses modern syntax. <code>pyproject.toml</code> integrates with other tools such as <code>commitizen</code>. <code>pyproject.toml</code> conforms to python standards (PEP 517/518)!</p> <p>Let's start with a simple <code>pyproject.toml</code> file: pyproject.toml<pre><code>[build-system]\nrequires = [\"setuptools &gt;= 61.0\", \"setuptools-scm&gt;=8.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\ndynamic = [\"version\"]\nname = \"my_package\"\nrequires-python = \"&gt;= 3.8\"\ndependencies = [\n    \"numpy\",\n    \"matplotlib\",\n]\nauthors = [\n  {name = \"Stephan O'Brien\", email = \"stephan.obrien@mcgill.ca\"},\n]\nmaintainers = [\n  {name = \"Stephan O'Brien\", email = \"stephan.obrien@mcgill.ca\"}\n]\ndescription = \"A python package for doing cool stuff\"\nreadme = \"README.md\"\nlicense = {file = \"LICENSE\"}\n\n[tool.setuptools]\npackages=[\n  \"my_package\", \n]\n\n\n[project.urls]\nHomepage = \"https://github.com/steob92/my_package/\"\nDocumentation = \"https://github.com/steob92/my_package/\"\nRepository = \"https://github.com/steob92/my_package.git\"\nIssues = \"https://github.com/steob92/my_package/issues\"\nChangelog = \"https://github.com/steob92/my_package/blob/main/CHANGELOG.md\"\n</code></pre></p> <p>There's a lot going on in this file. Let's take it apart: <pre><code>[build-system]\nrequires = [\"setuptools &gt;= 61.0\", \"setuptools-scm&gt;=8.0\"]\nbuild-backend = \"setuptools.build_meta\"\n</code></pre></p> <p>Here we are specifying the <code>build-system</code>, essentially what we're using to build the package.  In this case we're using <code>setuptools</code>, hence adding it as a requirement with the <code>requires</code> variable.</p> <pre><code>[project]\ndynamic = [\"version\"]\nname = \"my_package\"\nrequires-python = \"&gt;= 3.8\"\ndependencies = [\n    \"numpy\",\n    \"matplotlib\",\n]\nauthors = [\n  {name = \"Stephan O'Brien\", email = \"stephan.obrien@mcgill.ca\"},\n]\nmaintainers = [\n  {name = \"Stephan O'Brien\", email = \"stephan.obrien@mcgill.ca\"}\n]\ndescription = \"A python package for doing cool stuff\"\nreadme = \"README.md\"\nlicense = {file = \"LICENSE\"}\n</code></pre> <p>Here we're specifying some metadata about our package, including the name, version, who the authors/maintainers are and a short description.  Here we can also specify a README and license file.  The README acts as an introduction page about our package.  Note here we're also specifying the required python version <code>requires-python = \"&gt;= 3.8\"</code>.  This syntax means that we need a python version which is greater than or equal to 3.8 to run the code. We can also specify dependencies using the <code>dependencies</code> keyword.  When we start building the package, pip will search for these dependencies within our environment and install them if they don't exist.</p> <p><pre><code>[tool.setuptools]\npackages=[\n  \"my_package\", \n]\n</code></pre> Here we're specifying the packages (and sub-packages) that are to be installed as part of our package. We'll look at this is some more details later. <pre><code>[project.urls]\nHomepage = \"https://github.com/steob92/my_package/\"\nDocumentation = \"https://github.com/steob92/my_package/\"\nRepository = \"https://github.com/steob92/my_package.git\"\nIssues = \"https://github.com/steob92/my_package/issues\"\nChangelog = \"https://github.com/steob92/my_package/blob/main/CHANGELOG.md\"\n</code></pre></p> <p>Here we're specifying some URLs that users can consult. For example the \"Issues\" URL will link to the github issues page of this repository.</p> <p>With this package in place, we can simply install this package into our environment using <code>pip install .</code>: <pre><code>Processing /raid/RAID1/Tutorials/python_packaging_working\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\nRequirement already satisfied: numpy in /home/obriens/mambaforge/envs/dev/lib/python3.11/site-packages (from my_package==0.0.0) (1.26.4)\nRequirement already satisfied: matplotlib in /home/obriens/mambaforge/envs/dev/lib/python3.11/site-packages (from my_package==0.0.0) (3.8.0)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /home/obriens/mambaforge/envs/dev/lib/python3.11/site-packages (from matplotlib-&gt;my_package==0.0.0) (1.2.0)\nRequirement already satisfied: cycler&gt;=0.10 in /home/obriens/mambaforge/envs/dev/lib/python3.11/site-packages (from matplotlib-&gt;my_package==0.0.0) (0.11.0)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /home/obriens/mambaforge/envs/dev/lib/python3.11/site-packages (from matplotlib-&gt;my_package==0.0.0) (4.25.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /home/obriens/mambaforge/envs/dev/lib/python3.11/site-packages (from matplotlib-&gt;my_package==0.0.0) (1.4.4)\nRequirement already satisfied: packaging&gt;=20.0 in /home/obriens/mambaforge/envs/dev/lib/python3.11/site-packages (from matplotlib-&gt;my_package==0.0.0) (23.1)\nRequirement already satisfied: pillow&gt;=6.2.0 in /home/obriens/mambaforge/envs/dev/lib/python3.11/site-packages (from matplotlib-&gt;my_package==0.0.0) (10.2.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /home/obriens/mambaforge/envs/dev/lib/python3.11/site-packages (from matplotlib-&gt;my_package==0.0.0) (3.0.9)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /home/obriens/mambaforge/envs/dev/lib/python3.11/site-packages (from matplotlib-&gt;my_package==0.0.0) (2.8.2)\nRequirement already satisfied: six&gt;=1.5 in /home/obriens/mambaforge/envs/dev/lib/python3.11/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib-&gt;my_package==0.0.0) (1.16.0)\nBuilding wheels for collected packages: my_package\n  Building wheel for my_package (pyproject.toml) ... done\n  Created wheel for my_package: filename=my_package-0.0.0-py3-none-any.whl size=2183 sha256=dd2ca561171519e9b5755e595d8ae619d67e3ec7593c00e8e29096d29e6b943d\n  Stored in directory: /tmp/pip-ephem-wheel-cache-om8ak4ov/wheels/11/62/fc/593354b5442752b3bf9fa9c9fc8fe5e054389e3b47a6fb69bf\nSuccessfully built my_package\nInstalling collected packages: my_package\nSuccessfully installed my_package-0.0.0\n</code></pre></p> <p>This will now work from any directory as long as we're using the same Python environment!</p>"},{"location":"Python/OOP/#converting-our-script-into-a-versatile-package-using-object-orientated-programming","title":"Converting our script into a versatile package using Object Orientated Programming","text":"<p>As was previously mentioned, <code>my_package/analysis_script.py</code> isn't very versatile.  We ideally want something that other users can build upon.  To do this we'll convert some data types into classes that we can then build upon.  Let's start by making a base data class that we'll build upon.  Create a subdirectory called <code>my_package/data</code>, within this directory let's create a <code>base.py</code> file: my_package/data/base.py<pre><code>from numpy import ndarray, savetxt\n\nclass Base:\n\n    _data = None\n    data_mean = 0\n    data_std = 0\n    data_min = 0\n    data_max = 0\n    normalize_methods = ['minmax', 'standard']\n\n\n    def __init__(self):\n        pass\n\n    def __str__(self):\n        desc_string = self.describe()\n        print (desc_string)\n        return self.__class__.__name__    \n\n\n    @property\n    def data(self):\n        return self._data\n\n    @data.setter\n    def data(self, value):\n        self._data = value\n        self.data_mean = self.data.mean()\n        self.data_std = self.data.std()\n        self.data_min = self.data.min()\n        self.data_max = self.data.max()\n        self.data_shape = self.data.shape\n\n\n\n    def describe(self):\n        data_string = f'Data has a mean value of {self.data_mean:0.2f}.\\n'\n        data_string += f'Data has a minimum value of {self.data_min:0.2f} and a maximum value of {self.data_max:0.2f}'\n        return data_string\n\n\n    def get_properties(self):\n        return self.data_mean, self.data_std, self.data_min, self.data_max\n\n\n    def normalize(self, method = 'minmax'):\n\n        if method.lower() == 'minmax':\n            self.data = (self.data  - self.data_min) / (self.data_max - self.data_min)\n\n        elif method.lower() == 'standard':\n            self.data = (self.data - self.data_mean) / self.data_std \n\n        else:\n            raise RuntimeError (f'Method {method} not implemented. Please use one of {self.normalize_methods}')\n\n\n    def to_file(self, filename):\n        savetxt(filename, self.data, delimiter=\",\")\n</code></pre></p> <p>Some comments:</p> <ul> <li>On line 1 we're only <code>import</code>-ing what we need to use from <code>numpy</code></li> <li>On line 3 we define the name of the class as <code>Base</code></li> <li>On lines 5-10 we define some class member and data that we'll be using</li> <li>On lines 16-19 we're defining the <code>__str__</code> method which we define the behaviour when we run something like \"print (my_base_class)\".</li> <li>On lines 22-24 we're defining a property called data using the <code>@property</code> decorator. This acts as a \"getter\", instead of accessing the real data <code>_data</code> the user accesses a copy of the data called <code>data</code>.</li> <li>On lines 26-33 we're defining the \"setter\" of <code>data</code> using the <code>@data.setter</code> decorator. This defines the behaviour when we're assigning values to <code>data</code>. When the user calls <code>my_base_class.data = some_data</code>, they're actually calling this function. This allows for <code>_data</code> to be updated and the data properties to be automatically determined whenever <code>data</code> is modified.</li> <li>On lines 47-56 we've defined a <code>normalize</code> function which allows the user to normalize the data using either a min/max or standard normalization based on a keyword. This defaults to a min/max normalization. Since we've set the property <code>data</code>, when we assign with <code>self.data = ...</code> the setter function is called, <code>self._data</code> is updated the data properties (min, max, mean, std) are also recalculated.</li> <li>On lines 59-60 we're created a member function to dump the data to a CSV file.</li> </ul> <p>Aside on data classes</p> <p>Python now has a special class type called \"Data Classes\".  Using data classes allows one to create data-based classes like the one above. For the sake of illustration, we won't be using data classes in this tutorial. See here for more details.</p> <p>We now want to make this package visible anyone using this package. Firstly we need to add a <code>__init__.py</code> file to this folder: <pre><code>\u251c\u2500\u2500 data.csv\n\u251c\u2500\u2500 my_package\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 analysis_script.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 base.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 __init__.py\n\u251c\u2500\u2500 normalized_data.csv\n\u2514\u2500\u2500 pyproject.toml\n</code></pre></p> <p>Secondly, we need to add this location to the <code>pyproject.toml</code> file: <pre><code>[build-system]\nrequires = [\"setuptools &gt;= 61.0\", \"setuptools-scm&gt;=8.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\ndynamic = [\"version\"]\nname = \"my_package\"\nrequires-python = \"&gt;= 3.8\"\ndependencies = [\n    \"numpy\",\n    \"matplotlib\",\n]\nauthors = [\n  {name = \"Stephan O'Brien\", email = \"stephan.obrien@mcgill.ca\"},\n]\nmaintainers = [\n  {name = \"Stephan O'Brien\", email = \"stephan.obrien@mcgill.ca\"}\n]\ndescription = \"A python package for doing cool stuff\"\nreadme = \"README.md\"\nlicense = {file = \"LICENSE\"}\n\n[tool.setuptools]\npackages=[\n  \"my_package\", \n  \"my_package.data\", \n]\n\n\n[project.urls]\nHomepage = \"https://github.com/steob92/my_package/\"\nDocumentation = \"https://github.com/steob92/my_package/\"\nRepository = \"https://github.com/steob92/my_package.git\"\nIssues = \"https://github.com/steob92/my_package/issues\"\nChangelog = \"https://github.com/steob92/my_package/blob/main/CHANGELOG.md\"\n</code></pre></p> <p>If we now rerun <code>pip install .</code>, we can now import our base class: <pre><code>In [1]: import numpy as np\nIn [2]: from my_package.data.base import Base\nIn [3]: my_data = Base()\nIn [4]: my_data.data = np.random.random((10,10))\nIn [5]: my_data.data_min\nOut[5]: 0.010934507239608426\nIn [6]: my_data.data_max\nOut[6]: 0.9917396588604535\n</code></pre></p> <p>It is often very useful to have a method or class of randomly generated data for testing pipelines. Let's build on top of the <code>Base</code> class by creating a class that will generate random data. my_package/data/random.py<pre><code>from .base import Base\nfrom numpy.random import random, normal\n\nclass RandomData(Base):\n\n    # Define the implemented methods\n    implemented_methods = ['uniform', 'normal']\n\n    def __init__(\n            self, \n            shape, \n            rnd = 'uniform',\n            mean = 0,\n            width = 1 ):\n\n        # Call the parent's constructor\n        super().__init__()  \n\n        # Uniform Random Numbers\n        if rnd.lower() == 'uniform':\n            self.data = random(shape)\n        # Normal Random Numbers\n        elif rnd.lower() == 'normal':\n            self.data = normal(loc = mean, scale = width, size = shape)\n        else:\n            raise RuntimeError(f'Random method \"{rnd}\" is not implemented. Please use one of the implemented methods {self.implemented_methods} ')\n</code></pre></p> <ul> <li>Notice on line 1 we're importing using <code>from .base import Base</code>. Here the <code>.base</code> is specifying that we're looking for a file called <code>base.py</code> within the same directory (<code>.</code>). If we wanted to search in the parent directory (one directory above) we'd use <code>from ..base import Base</code>. This is similar to the <code>.</code> and <code>..</code> path when working with <code>bash</code>.</li> <li>On line 4 we define the <code>RandomData</code> class, specifying that it inherits from the <code>Base</code> class. This means that <code>RandomData</code> will have all the same functionality as <code>Base</code>, meaning the <code>self.data</code> setter will also work for <code>RandomData</code>.</li> <li>On line 17 we're calling the parent class's constructor using <code>super().__init__()</code>. When ever we want to explicitly call the parent class's version of a function, we can call <code>super().function()</code>.</li> </ul> <p>Since <code>\"my_package.data\"</code> has already been added to the <code>pyproject.toml</code> file <code>my_package/data/random_data.py</code> will be installed with <code>pip install .</code>. We can try run this: <pre><code>In [1]: from my_package.data.random_data import RandomData\nIn [2]: my_data = RandomData((5,100))\nIn [3]: my_data.data_max\nOut[3]: 0.9959056068848283\nIn [4]: my_data.data_shape\nOut[4]: (5, 100)\n</code></pre></p> <p>You'll notice that the import is pretty long and cumbersome to type.  Let's make this a little easier for the user by playing around with the <code>__init__.py</code> files.  Firstly let's modify <code>my_package/data/__init__.py</code>: my_package/data/__init__.py<pre><code>from .random_data import RandomData\n\n# Define what is imported with 'from here import *'\n__all__ = [\"RandomData\"]\n</code></pre> Here we're importing <code>RandomData</code> making it easily available from <code>my_package.data</code> rather than <code>my_package.data.random_data</code>.  We then create a variable called <code>__all__</code> and assign it to a list (<code>[\"RandomData\"]</code>). This specifies what is imported when running <code>from my_package.data import *</code>.</p> <p>Next let's modify <code>my_package/__init__.py</code>: my_package/__init__.py<pre><code>from .data import RandomData\n\n# Define what is imported with 'from here import *'\n__all__ = [\"RandomData\"]\n</code></pre> Notice that we no longer need to import from <code>.data.random_data</code>, we can just import from <code>.data</code>.  We can do this because <code>RandomData</code> is already visible within <code>my_package.data</code>.  We now have a few ways to import <code>RandomData</code>:</p> <ul> <li><code>from my_package import *</code></li> <li><code>from my_package import RandomData</code></li> <li><code>from my_package.data import *</code></li> <li><code>from my_package.data import RandomData</code></li> <li><code>from my_package.data.random_data import RandomData</code></li> </ul> <p>A user no longer needs to know that <code>RandomData</code> is nested within <code>data</code>. Reinstalling with <code>pip install .</code>, we can run the previous example with: <pre><code>In [1]: from my_package import RandomData\nIn [2]: my_data = RandomData((5,100))\nIn [3]: my_data.data_max\nOut[3]: 0.9959056068848283\nIn [4]: my_data.data_shape\nOut[4]: (5, 100)\n</code></pre></p>"},{"location":"Python/OOP/#making-a-csv-data-loader","title":"Making a CSV Data loader","text":"<p>Now that we have a base class <code>Base</code> and a tester class <code>RandomData</code>, let's add a CSV reader class within the <code>my_package/data</code> directory: my_package/data/csv_data.py<pre><code>from .base import Base\nimport numpy as np\n\n\nclass CSVData:\n\n    def __init__(self, filename):\n\n        self.data = []\n        self.read_file(filename)\n\n    def read_file(self, filename):\n        data = np.loadtxt(filename, delimiter=',', comments = '#')\n        n_data = data.shape[0]\n        n_cols = data.shape[1]\n\n        for i in range(n_cols):\n            datum = Base()\n            datum.data = data[:,i]\n            self.data.append(datum)\n        self.len = len(self.data)\n\n    @classmethod\n    def from_file(cls, filename):\n        return cls(filename)\n</code></pre></p> <p>Here <code>CSVData</code> doesn't explicitly inherent from <code>Base</code>, but there is an internal data type <code>self.data</code> which is a list of <code>Base</code>. </p> <p>Lines 12-21 defines the <code>read_file</code> function which will extract the data from the CSV file and stores each column into a <code>Base</code> data type.</p> <p>On lines 23-25 we're defining a class method using the <code>@classmethod</code> decorator.  A class method allows us to call functions associated to the class which aren't associated with a specific instance.  In this case the <code>from_file</code> method will call <code>__init__</code>, passing the <code>filename</code>. This will allow us to generate a new object using <code>CSVData.from_file(filename)</code>.</p> <p>Let's update the <code>__init__.py</code> files with the new class:</p> my_package/data/__init__.py<pre><code>from .random_data import RandomData\nfrom .csv_data import CSVData\n\n# Define what is imported with 'from here import *'\n__all__ = [\"RandomData\", \"CSVData\"]\n</code></pre> <p>Next let's modify <code>my_package/__init__.py</code>: my_package/__init__.py<pre><code>from .data import RandomData, CSVData\n\n# Define what is imported with 'from here import *'\n__all__ = [\"RandomData\", \"CSVData\"]\n</code></pre></p>"},{"location":"Python/OOP/#adding-some-package-data","title":"Adding some package data","text":"<p>It might be useful to have some data included within the package.  This could be some reference data or some test data. Let's start by adding the CSV file into our package.  We'll add it to <code>my_package/package_data/data.csv</code>.  For the package data to be visible, we'll need to add a <code>MANIFEST.in</code> file MANIFEST.in<pre><code>include my_package/package_data/*.csv\n</code></pre></p> <p>Let's then update the <code>CSVData</code> class to have a method to import this data: my_package/data/csv_data.py<pre><code>from .base import Base\nimport numpy as np\nimport pkg_resources\n\nclass CSVData:\n\n    def __init__(self, filename):\n\n        self.data = []\n        self.read_file(filename)\n\n    def read_file(self, filename):\n        data = np.loadtxt(filename, delimiter=',', comments = '#')\n        n_data = data.shape[0]\n        n_cols = data.shape[1]\n\n        for i in range(n_cols):\n            datum = Base()\n            datum.data = data[:,i]\n            self.data.append(datum)\n        self.len = len(self.data)\n\n    @classmethod\n    def from_file(cls, filename):\n        return cls(filename)\n\n\n    @classmethod\n    def get_titanic(cls) -&gt; 'CSVData':\n        return cls(pkg_resources.resource_filename('my_package','package_data/Titanic.csv'))\n</code></pre></p> <p>Here we're using <code>pkg_resources</code> to access the location of the package data from the installation location. Now calling <code>CSVData.get_titanic_data()</code> will return a <code>CSVData</code> object with the dataset loaded.</p>"},{"location":"Python/OOP/#documenting-our-code","title":"Documenting Our Code","text":"<p>So far we've written code with little-to-no documentation.  For a small single-use analysis script, this might be ok. However, as our code increases in complexity and we want to start sharing it with others, we need to better document our code. Furthermore, we need to remember that undocumented code my become completely unreadable to anyone not in your current mindstate. This include ourselves.  A very common problem people have is returning to old code and failing to understand the code without hours of reading and debugging.</p> <p>In this section we'll look at the following topics:</p> <ul> <li>Documenting our code with docstrings, providing useful help messages for users.</li> <li>Using \"type-hinting\" to show the data types being used and returned by functions</li> <li>Combining the above to easily make documentation pages like numba.readthedocs.io</li> </ul>"},{"location":"Python/OOP/#docstrings","title":"Docstrings","text":"<p>Docstrings are a standard method used to describe one's code.  Docstrings use a format that is well defined and utilised by Python and documenation software.</p> <p>Here are some links discussing documenting one's code using docstrings:</p> <ul> <li>Real Python: Documenting Python Code</li> <li>Example Google Style Python Docstrings</li> <li>THe Hitchhiker's Guide to Python: Documentation</li> </ul> <p>There are many different standards, but they all use a common syntax, that is a long string defined just after a function/class declaration: docstring_example.py<pre><code>def my_function(a,b):\n    \"\"\"A simple function to add two numbers together\n\n    This function will take in two numbers (a and b) and return their sum (a + b)\n\n    Parameters\n    ----------\n    a : float\n        The first number \n\n    b : float\n        The second number\n\n    Returns\n    -------\n    float\n        The sum of a and b\n    \"\"\"\n\n    return a + b\n</code></pre></p> <p>Notice that the docstring starts and end with 3 quotation marks <code>\"\"\"</code>.  It is place directly after the functions/class declaration. The first line is a high-level description of the function/class followed by an optional more detailed description. We then define <code>Parameters</code> and <code>Returns</code> and list what the parameters/returns are, their type and how they are used. We can also add things like examples, tests (which can be evaluated with a module like doctest) and any error handling does using <code>raise exception</code> (see here for some examples). Using a docstring like this, a new developer has a good idea of what the function does and the expected behaviour. Futhermore the user can also call the <code>help</code> function to return the docstring: <pre><code>help(my_function)\n</code></pre> <pre><code>Help on function my_function in module __main__:\n\nmy_function(a, b)\n    A simple function to add two numbers together\n\n    This function will take in two numbers (a and b) and return their sum (a + b)\n\n    Parameters\n    ----------\n    a : float\n        The first number \n\n    b : float\n        The second number\n\n    Returns\n    -------\n    float\n        The sum of a and b\n</code></pre> This provides a useful resource for the user without the need to dig through the code to determine the expected behaviour of the function.</p> <p>Let's start looking at <code>my_package/data/base.py</code> and add some docstrings: my_package/data/base.py<pre><code>from numpy import ndarray\n\nclass Base:\n    \"\"\"This is the base class for all the classes in this package.\n\n    \"\"\"\n\n    _data = 0\n    data_mean = 0\n    data_std = 0\n    data_min = 0\n    data_max = 0\n\n    normalize_methods = ['minmax', 'standard']\n\n\n    def __init__(self):\n        \"\"\"This is the constructor of the class.\"\"\"\n\n        pass\n\n    def __str__(self):\n        \"\"\"This is the string representation of the class.\n\n        Returns\n        -------\n        str\n            The name of the class\n        \"\"\"\n\n        desc_string = self.describe()\n        print (desc_string)\n        return self.__class__.__name__    \n\n\n    @property\n    def data(self):\n        \"\"\"This is the data property.\n\n        Returns\n        -------\n        ndarray\n            A numpy array of the data\n        \"\"\"\n\n        return self._data\n\n    @data.setter\n    def data(self, value):\n        \"\"\"This is the setter for the data property.\n\n        Parameters\n        ----------\n        value : ndarray\n            A numpy array that will be stored as the data\n        \"\"\"\n\n        self._data = value\n        self.data_mean = self.data.mean()\n        self.data_std = self.data.std()\n        self.data_min = self.data.min()\n        self.data_max = self.data.max()\n        self.data_shape = self.data.shape\n\n\n\n    def describe(self):\n        \"\"\"This method prints a summary of the data.\n\n        Returns\n        -------\n        str\n            A description of the data\n        \"\"\"\n\n        data_string = f'Data has shape {self.data_shape}, with a mean value of {self.data_mean:0.2f}.\\n'\n        data_string += f'Data has a minimum value of {self.data_min:0.2f} and a maximum value of {self.data_max:0.2f}'\n        return data_string\n\n\n    def get_properties(self):\n        \"\"\"This method returns the mean, standard deviation, minimum and maximum of the data.\n\n        Returns\n        -------\n        (float, float, float)\n            A tuple with the mean, standard deviation, minimum and maximum of the data.\n\n        \"\"\"\n\n        return self.data_mean, self.data_std, self.data_min, self.data_max\n\n\n    def normalize(self, method = 'minmax'):\n        \"\"\"This method normalizes the data.\n\n        Parameters\n        ----------\n        method : str\n            The method to use for normalization. Can be 'minmax' or 'standard'.\n\n        Raises\n        ------\n        RuntimeError\n            If `method` has not yet been implemented\n        \"\"\"\n\n        if method.lower() == 'minmax':\n            self.data = (self.data  - self.data_min) / (self.data_max - self.data_min)\n\n        elif method.lower() == 'standard':\n            self.data = (self.data - self.data_mean) / self.data_std \n\n        else:\n            raise RuntimeError (f'Method {method} not implemented. Please use one of {self.normalize_methods}')\n</code></pre></p> <p>In the above example we've added docstrings to <code>my_package/data/base.py</code>. Notice that the <code>normalize</code> function has the potential to raise a <code>RuntimeError</code> (line 115). This potential error steam is documented in the docstring on lines 102-105.</p>"},{"location":"Python/OOP/#type-hinting","title":"Type Hinting","text":"<p>Type hinting is an excellent way to inform developers and users of what type the arguments and returns are.  This is done by labeling arguments and returns with the expected type. Let's look again at simple function we used to get the sum of two numbers:</p> type_hinting_example.py<pre><code>def my_function(a : float ,b : float) -&gt; float:\n    \"\"\"A simple function to add two numbers together\n\n    This function will take in two numbers (a and b) and return their sum (a + b)\n\n    Parameters\n    ----------\n    a : float\n        The first number \n\n    b : float\n        The second number\n\n    Returns\n    -------\n    float\n        The sum of a and b\n    \"\"\"\n\n    return a + b\n</code></pre> <p>Here on line 1 we modify the function definition to include the data types.  We specify the type of <code>a</code> and <code>b</code> using the syntax <code>var : type</code>. In this case we use <code>a : float, b : float</code> to indicate that <code>a</code> and <code>b</code> are expected to be of type <code>float</code>. Additionally we indicate what the return value is using <code>-&gt; type</code> after the function definition. In this case we're returning a float, so <code>def my_function(a : float ,b : float) -&gt; float:</code>.  We can also specify default parameters using the syntax <code>var : type = default</code>. For example: type_hinting_example.py<pre><code>def my_function(a : float ,b : float = 1.) -&gt; float:\n    \"\"\"A simple function to add two numbers together\n\n    This function will take in two numbers (a and b) and return their sum (a + b)\n\n    Parameters\n    ----------\n    a : float\n        The first number \n\n    b : float\n        The second number, defaults to 1.\n\n    Returns\n    -------\n    float\n        The sum of a and b\n    \"\"\"\n\n    return a + b\n</code></pre></p> <p>Here <code>b</code> is of type <code>float</code> and has a default value of <code>1.0</code>.  This means that if the user calls <code>my_function(3)</code>, they'll get a return of <code>4</code>, whereas if the user calls <code>my_function(3, 2)</code> they'll get a return of <code>5</code>.</p> <p>A very useful package to use when implementing type hinting is the <code>typing</code> library. <code>typing</code> has a lot of useful imports, for example:</p> <ul> <li><code>List[type1, type2, ..., typen]</code>, this can be used when we expect a <code>list</code> of types <code>type1,...,typen</code>.</li> <li><code>Tuple[type1, type2, ..., typen]</code>, this can be used when we expect a <code>tuple</code> of types <code>type1,...,typen</code>.</li> <li><code>Option[type]</code>, this can be used for optional values</li> <li><code>Union[type1, type2]</code>, this can be used when a varibable can have either <code>type1</code> or <code>type2</code>.</li> </ul> <p>See typing cheat sheet for more examples.</p> <p>Let's use <code>typing</code> in the <code>my_function</code> example:</p> type_hinting_example.py<pre><code>from typing import Optional\ndef my_function(a : float ,b : Optional[float] = 1.) -&gt; float:\n    \"\"\"A simple function to add two numbers together\n\n    This function will take in two numbers (a and b) and return their sum (a + b)\n\n    Parameters\n    ----------\n    a : float\n        The first number \n\n    b : Optional[float]\n        The second number, defaults to 1.\n\n    Returns\n    -------\n    float\n        The sum of a and b\n    \"\"\"\n\n    return a + b\n</code></pre> <p>On line 1 we import <code>Optional</code> from <code>typing</code>. On line 2 the <code>my_function</code> definition is updated to reflect that <code>b</code> is an <code>Optional</code> parameter of type <code>float</code>. We also update the docstring on line 12 to show that this is an <code>Optional</code> parameter.</p> <p>Now that we have an idea of how type hinting works, let's update the <code>my_package/data/base.py</code> file: my_package/data/base.py<pre><code>from numpy import ndarray\nfrom typing import List, Tuple, Optional\n\nclass Base:\n    \"\"\"This is the base class for all the classes in this package.\n\n    \"\"\"\n    _data : ndarray\n    data_mean : float\n    data_std : float\n    data_min : float\n    data_max : float\n\n    normalize_methods : List[str] = ['minmax', 'standard']\n\n\n    def __init__(self):\n        \"\"\"This is the constructor of the class.\"\"\"\n        pass\n\n    def __str__(self) -&gt; str:\n        \"\"\"This is the string representation of the class.\n\n        Returns\n        -------\n        str\n            The name of the class\n        \"\"\"\n\n        desc_string = self.describe()\n        print (desc_string)\n        return self.__class__.__name__    \n\n\n    @property\n    def data(self) -&gt; ndarray:\n        \"\"\"This is the data property.\n\n        Returns\n        -------\n        ndarray\n            A numpy array of the data\n        \"\"\"\n\n        return self._data\n\n    @data.setter\n    def data(self, value) -&gt; None:\n        \"\"\"This is the setter for the data property.\n\n        Parameters\n        ----------\n        value : ndarray\n            A numpy array that will be stored as the data\n        \"\"\"\n\n        self._data = value\n        self.data_mean = self.data.mean()\n        self.data_std = self.data.std()\n        self.data_min = self.data.min()\n        self.data_max = self.data.max()\n        self.data_shape = self.data.shape\n\n\n\n    def describe(self) -&gt; str:\n        \"\"\"This method prints a summary of the data.\n\n        Returns\n        -------\n        str\n            A description of the data\n        \"\"\"\n\n        data_string = f'Data has shape {self.data_shape}, with a mean value of {self.data_mean:0.2f}.\\n'\n        data_string += f'Data has a minimum value of {self.data_min:0.2f} and a maximum value of {self.data_max:0.2f}'\n        return data_string\n\n\n    def get_properties(self) -&gt; Tuple[float, float, float]:\n        \"\"\"This method returns the mean, standard deviation, minimum and maximum of the data.\n\n        Returns\n        -------\n        Tuple[float, float, float]\n            A tuple with the mean, standard deviation, minimum and maximum of the data.\n\n        \"\"\"\n        return self.data_mean, self.data_std, self.data_min, self.data_max\n\n\n    def normalize(self, method : Optional[str] = 'minmax') -&gt; None:\n        \"\"\"This method normalizes the data.\n\n        Parameters\n        ----------\n        method : Optional[str]\n            The method to use for normalization. Can be 'minmax' or 'standard'.\n\n        Raises\n        ------\n        RuntimeError\n            If `method` has not yet been implemented\n        \"\"\"\n\n        if method.lower() == 'minmax':\n            self.data = (self.data  - self.data_min) / (self.data_max - self.data_min)\n\n        elif method.lower() == 'standard':\n            self.data = (self.data - self.data_mean) / self.data_std \n\n        else:\n            raise RuntimeError (f'Method {method} not implemented. Please use one of {self.normalize_methods}')\n</code></pre></p> <p>On line 2 we import <code>List</code>, <code>Tuple</code> and <code>Optional</code> from <code>typing</code>.  On lines 8-12 the types of the class data are added, noting that <code>ndarray</code> was imported on line 1. On lines 90, 92 we're utilizing the <code>typing</code> imports in the function definition. Some functions don't have a return (e.g. line 48), in these cases we can use the type <code>None</code> to specify that nothing is returned.</p>"},{"location":"Python/OOP/inital_script/","title":"Inital script","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n</pre> import numpy as np import matplotlib.pyplot as plt In\u00a0[\u00a0]: Copied! <pre># Create an array of 100x100 random numbers drawn from a gaussian of mean 10 and width 1\ndata = np.random.normal(loc = 10, scale = 1, size = (1000,10))\n</pre> # Create an array of 100x100 random numbers drawn from a gaussian of mean 10 and width 1 data = np.random.normal(loc = 10, scale = 1, size = (1000,10)) In\u00a0[\u00a0]: Copied! <pre>data_mean = data.mean()\ndata_std = data.std()\ndata_min = data.min()\ndata_max = data.max()\n</pre> data_mean = data.mean() data_std = data.std() data_min = data.min() data_max = data.max() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Rust/","title":"Rust","text":"<ul> <li>Programming in Rust</li> </ul>"},{"location":"Rust/rust_intro/","title":"Rust intro","text":"<p>In this tutorial, I'll assume that you have some programming experience. I'll also assume that you use programming as a tool and that you don't necessarily have a background in computer science. Our focus will be more on the \"hows\" rather than the \"whys,\" whenever possible.</p> <p>By the end of this tutorial, you will have gained enough knowledge of Rust to begin developing high-performing code. I encourage you to work through the examples at your own pace, attempting to solve issues before reviewing the solutions. For your convenience, unanswered problems are available on GitHub, with the solutions provided in the \"solutions\" branch.</p>"},{"location":"Rust/rust_intro/#what-is-rust","title":"What is Rust?","text":"<p>Rust stands out as a high-performance and memory-efficient programming language that emerged from the endeavors of Mozilla's research employees (Rust, not Firefox, is Mozilla\u2019s greatest industry contribution).  Prioritizing performance and memory safety, Rust utilizes a robust type system and an innovative \"ownership\" model to guarantee memory safety and thread safety at compile time.  This approach aims to address vulnerabilities arising from memory errors, with estimates from Microsoft suggesting that approximately 70% of code vulnerabilities stem from memory-related issues (source).</p> <p>Rust's ability to produce fast, efficient, and resilient code has catapulted it to the top of the list as the most admired language amongst developers. The community of Rust programmers affectionately refers to themselves as \"Rustaceans,\" and the language's unofficial mascot, Ferris the crab:</p> <p> </p>  Ferris the Crab (https://Rustacean.net). Ferris being a reference to ferrous, a compound that contains iron."},{"location":"Rust/rust_intro/#rust-compared-to-python","title":"Rust Compared to Python","text":"<p>When comparing Rust to a language like Python, several key differences become apparent:</p> <ul> <li> <p>Performance: Rust is renowned for its high performance, often being comparable to languages like C or C++. Python, on the other hand, tends to be significantly slower than Rust, emphasizing ease of development over raw performance.</p> </li> <li> <p>Type System: Python is a dynamically typed language, meaning the interpreter infers variable types at runtime, allowing flexibility but increasing the potential for type-related errors. In contrast, Rust requires variables to have known types at compile time, enhancing safety and allowing for optimizations to be made by the compiler.</p> </li> <li> <p>Compilation: Rust is a compiled language, while Python is interpreted. Python code is executed by an interpreter, converting code to bytecode at runtime, resulting in slower performance. Rust, as a compiled language, produces machine code binaries before runtime, reducing overhead and enabling compiler optimizations for faster, more memory-efficient execution.</p> </li> <li> <p>Memory Management: Python employs a \"garbage collector\" to manage memory by periodically checking and freeing memory occupied by variables that go out of scope, impacting speed and memory efficiency. Rust utilizes an \"ownership\" memory model enforced by the \"borrow checker\" at compile time. Each variable in Rust has a single owner, and memory is automatically freed when the owner goes out of scope. This approach, without a costly garbage collector, contributes to Rust's fast runtime.</p> </li> <li> <p>Thread Safety: Python's Global Interpreter Lock (GIL) allows only one CPU-bound thread to execute at a time, ensuring safety across threads but causing a bottleneck for parallel code execution. In Rust, the ownership model, combined with allowing either numerous immutable references or a single mutable reference at a time, guarantees thread safety at compile time without the restrictions posed by a GIL.</p> </li> <li> <p>Package Management: Both Python and Rust use package management systems to handle external libraries or crates (in Rust). Rust utilizes the <code>cargo</code> package manager and <code>toml</code> files to manage project dependencies, while Python uses tools like <code>pip</code> and <code>requirements.txt</code> to manage packages.</p> </li> </ul> <p>Rust and Python employ different approaches to achieve their goals, with Rust focusing on performance, memory safety, and concurrency, whereas Python emphasizes ease of use and flexibility.</p>"},{"location":"Rust/rust_intro/#installing-rust","title":"Installing Rust","text":"<p>Comprehensive installation instructions for Rust can be accessed here. The installation process involves utilizing <code>rustup</code>, a tool used for installing both the Rust compiler (<code>rustc</code>) and the package manager (<code>cargo</code>). These tools are compatible with Linux, macOS, and Windows (WSL). <pre><code>curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n</code></pre></p> <p>For alternative methods of installing Rust, refer to this page.</p> <p>For this tutorial, we'll utilize the official Rust Docker image to compile and run code within a container. To pull the image, execute:</p> <p><pre><code>docker pull Rust\n</code></pre> Create an interactive Docker container using the following command:</p> <p><pre><code>docker run -it --rm -v $(pwd):/local_data -w /local_data Rust bash\n</code></pre> Explanation:</p> <ul> <li><code>run</code> executes the bash command in an interactive mode (<code>-it</code>) to provide an interactive Bash shell for work.</li> <li><code>--rm</code> ensures the container is deleted after use.</li> <li><code>-v $(pwd):/local_data</code> mounts the current directory on the local machine to <code>/local_data</code> in the container.</li> <li><code>-w /local_data</code> sets the working directory to <code>/local_data</code> within the container.</li> </ul> <p>(Free) Learning resources:</p> <ul> <li>\"The Book\", the offical </li> <li>\"Rustlings Course on GitHub\"</li> <li>\"Offical Website\"</li> <li>\"Rust Playground\" a web coding enviroment for Rust</li> <li>\"Rust Standard Library Crate\"</li> <li>\"Command line apps in Rust\"</li> <li>\"The Embedded Rust Book\"</li> </ul>"},{"location":"Rust/rust_intro/#basics-of-rust","title":"Basics of Rust","text":"<p>In this section we will cover the basics of Rust.</p>"},{"location":"Rust/rust_intro/#hello-world","title":"Hello World","text":"<p>To create a new project in Rust, utilize the <code>cargo</code> command: <pre><code>cargo new hello\n</code></pre> This will create a new directory called <code>hello</code>.  <pre><code>-&gt; ls -ah hello\n.  ..  .git  .gitignore  Cargo.toml  src\n</code></pre></p> <p>When using <code>cargo new</code>, a new Rust project is initialized. Alongside creating the project structure, <code>cargo</code> automatically sets up a new Git repository for the package and adds a Rust-specific <code>.gitignore</code> file.  The newly created project includes a <code>Cargo.toml</code> file, which serves as the manifest file for the project.  This file contains details about the project, including external dependencies, package name, and versions used. <pre><code>-&gt; cat hello/Cargo.toml \n[package]\nname = \"hello\"\nversion = \"0.1.0\"\nedition = \"2021\"\n\n# See more keys and their definitions at https://doc.Rust-lang.org/cargo/reference/manifest.html\n\n[dependencies]\n</code></pre> Cargo also sets up a <code>src</code> directory with a <code>main.rs</code> file containing an example program that will print \"Hello, world\":  hello.rs<pre><code>fn main() {\n    println!(\"Hello, world!\");\n}\n</code></pre></p> <p>In this example, the following points are illustrated:</p> <ul> <li>Functions in Rust are defined using the <code>fn</code> keyword.</li> <li>The <code>main</code> function designates the entry point of the code to the compiler.</li> <li>Code blocks are enclosed within <code>{}</code> to denote scopes.</li> <li>Rust includes macros (indicated by <code>!</code>, which will be covered later) like <code>println!</code> used to print the string <code>\"Hello, world!\"</code> to the screen.</li> <li>Statements in Rust are terminated with a <code>;</code> (exceptions for when to omit the <code>;</code> will be explained later).</li> </ul> <p>This example can be compiled using <code>rustc</code>: <pre><code>rustc src/main.rs -o main\n</code></pre></p> <p>to create the executable <code>main</code>.</p> <p>Alternatively we can use <code>cargo build</code> to compile: <pre><code>-&gt; cargo build\n   Compiling hello v0.1.0 (/local_data/hello)\n    Finished dev [unoptimized + debuginfo] target(s) in 0.28s\n</code></pre> Executing this command will compile an executable located at <code>target/debug/hello</code>. To run the executable, you can either call the executable directly or use the <code>cargo run</code> command. When using <code>cargo run</code>, if there are changes in the code or if the code hasn't been compiled previously, it automatically triggers the <code>cargo build</code> command before executing the program.</p> <pre><code>-&gt; cargo run\n    Finished dev [unoptimized + debuginfo] target(s) in 0.01s\n     Running `target/debug/hello`\nHello, world!\n</code></pre> <p>The executable is typically found within a <code>debug</code> folder. By default, Rust generates debug information useful for code analysis and debugging. To create an optimized version for end-users, the <code>--release</code> flag can be utilized: <pre><code>-&gt; cargo build --release\n   Compiling hello v0.1.0 (/local_data/hello)\n    Finished release [optimized] target(s) in 0.25s\n</code></pre></p> <p>This will take longer to compile as <code>rustc</code> is optimizing the code.</p>"},{"location":"Rust/rust_intro/#types-in-rust","title":"Types in Rust","text":"<p>In Rust, types must be known at compile time. You can explicitly specify the type of a variable using the syntax <code>let my_variable: type = value</code>, where the <code>type</code> is specified after the variable name using a <code>:</code>. The following example demonstrates explicit declaration of variable types on lines 3-6:</p> types.rs<pre><code>fn main() {\n    // Explicitly declaring the type of the variable\n    let my_integer: i32 = 42;\n    let my_float: f64 = 3.14;\n    let my_character: char = 'A';\n    let my_boolean: bool = true;\n\n    // Rust can infer types in many cases, so explicit annotation is not always necessary\n    let inferred_integer = 10;\n    let inferred_float = 2.5;\n    let inferred_character = 'B';\n    let inferred_boolean = false;\n\n\n    // Explicitly declaring the type of the variable within the passed value\n    let my_integer_in_value = 17_i8;\n    let my_float_in_value = 6.28_f32;\n    let my_large_unsigned_32 = 1_000_000_u32;\n\n\n    // Printing the values along with their types\n    println!(\"Integer: {} (Type: i32)\", my_integer);\n    println!(\"Float: {} (Type: f64)\", my_float);\n    println!(\"Character: {} (Type: char)\", my_character);\n    println!(\"Boolean: {} (Type: bool)\", my_boolean);\n\n    println!(\"Inferred Integer: {} (Type: inferred)\", inferred_integer);\n    println!(\"Inferred Float: {} (Type: inferred)\", inferred_float);\n    println!(\"Inferred Character: {} (Type: inferred)\", inferred_character);\n    println!(\"Inferred Boolean: {} (Type: inferred)\", inferred_boolean);\n\n\n    println!(\"Integer: {} (Type: inferred from value)\", my_integer_in_value);\n    println!(\"Float: {} (Type: inferred from value)\", my_float_in_value);\n    println!(\"Unsigned: {} (Type: inferred from value)\", my_large_unsigned_32);\n}\n</code></pre> <p>The Rust compiler features type inference, enabling omission of the variable type, as it can deduce the type based on the assigned value. Internally, the compiler determines the variable's type during compilation based on the provided value. An example illustrating this behavior is demonstrated in lines 9-12 of <code>types.rs</code>.</p> <p>Additionally, we can explicitly specify the variable type by adding <code>::&lt;type&gt;</code> after the assigned value. This method is showcased in lines 16-18 of <code>types.rs</code>.</p> <p>In Rust, type conversion between different types can be achieved using keywords such as <code>into</code>, <code>try_into</code>, <code>from</code>, <code>try_from</code>, or <code>as</code>. Below are some examples: <pre><code>fn main() {\n    let integer_a: i32 = 40;\n    let float_b:f32 = integer_a as f32;\n\n\n    let integer_c: i32 = 3;\n    // We're using \"try into\" here because we could have a negative integer\n    let unsigned_d: u32 = integer_c.try_into().unwrap();\n\n    let float_64_e: f64 = 6.5;\n    // This wont work because going from f64-&gt;f32 loses percision and range\n    // There are also some funky behaviour around inf\n    // let float_32_f: f32 = f32::try_from(float_64_e).unwrap();\n    let float_32_f: f32 = float_64_e as f32;\n\n    let float_32_g:f32 = f32::from(3.13);\n    let i8_h:i8 = i8::from(-3);\n    let u32_i :u32 =  u32::try_from(8).unwrap();\n}\n</code></pre></p>"},{"location":"Rust/rust_intro/#ownership-in-rust","title":"Ownership in Rust","text":"<p>Ownership and the borrow checker constitute the foundation of Rust's memory management. When dealing with ownership in Rust, it's essential to remember three fundamental rules:</p> <ul> <li>Every value in Rust has a designated owner.</li> <li>At any given time, there can only be a single owner for a value.</li> <li>When the owner goes out of scope, the associated value is automatically dropped.</li> </ul> <p>Let's delve into an example to illustrate this concept: Ownership Example<pre><code>fn main() {\n\n    let mut x = String::from(\"Hello\");\n\n    let y = x;\n    println!(\"{}\", y);\n\n    println!(\"{}\", x);\n\n}\n</code></pre></p> <p>In the provided code, a new variable <code>x</code> of type <code>String</code> is created. At line 5, a new variable <code>y</code> is assigned the value of <code>x</code>. Subsequently, attempts to print <code>x</code> and <code>y</code> on lines 6 and 8, respectively, would result in a compilation error: <pre><code>error[E0382]: borrow of moved value: `x`\n --&gt; src/main.rs:8:20\n  |\n3 |     let mut x = String::from(\"Hello\");\n  |         ----- move occurs because `x` has type `String`, which does not implement the `Copy` trait\n4 |  \n5 |     let y = x;\n  |             - value moved here\n...\n8 |     println!(\"{}\", x);\n  |                    ^ value borrowed here after move\n</code></pre></p> <p>So what's happening? Well on line 5 we changed the ownership of the part of the memory that holds \"Hello\". The ownership of this has changed from <code>x</code> to <code>y</code>. Since we can only ever have one owner at a time, <code>x</code> cannot be printed. We could however run this example: Ownership Example Corrected<pre><code>fn main() {\n\n    let mut x = String::from(\"Hello\");\n\n    let y = x;\n    println!(\"{}\", y);\n    x = y;\n    println!(\"{}\", x);\n\n}\n</code></pre> In the above example once we have finished using <code>y</code> we have passed ownership back to <code>x</code>. </p> <p>The fact that all value in Rust only ever has one owner guarentees that we can never acidently drop or delete a value that is still in use. This might seem very limiting and a heavy cost to pay for safety, but we can use \"borrowing\" to circumvent this issue.  Ownership Example Corrected<pre><code>fn main() {\n\n    let x = String::from(\"Hello\");\n\n    let y = &amp;x;\n    println!(\"{}\", y);\n\n    println!(\"{}\", x);\n\n}\n</code></pre></p> <p>In the above value we have \"borrowed\" the value of <code>x</code>. By borrowing the values instead of taking ownership, <code>x</code> maintains ownership over the value, allowing different parts of the code to access the value of the value.</p> <p>When borrowing values Rust's \"borrow checker\" will keep track of all refeneces and make sure that we don't have dangling references or data races. Consider the following: Mutable and immutable borrows<pre><code>fn main() {\n\n    let mut x = String::from(\"Hello\");\n\n    let y = &amp;x;\n    println!(\"{}\", y);\n\n    x += \", world\";\n    println!(\"{}\", x);\n    println!(\"{}\", y);\n}\n</code></pre> In the preceding example, an immutable reference to <code>x</code> is established in line 5. However, an attempt to modify <code>x</code> occurs in line 8, resulting in a compilation error: <pre><code>   Compiling tutorial v0.1.0 (/local_data)\nerror[E0502]: cannot borrow `x` as mutable because it is also borrowed as immutable\n  --&gt; src/main.rs:8:5\n   |\n5  |     let y = &amp;x;\n   |             -- immutable borrow occurs here\n...\n8  |     x += \", world\";\n   |     ^^^^^^^^^^^^^^ mutable borrow occurs here\n9  |     println!(\"{}\", x);\n10 |     println!(\"{}\", y);\n   |                    - immutable borrow later used here\n\nFor more information about this error, try `rustc --explain E0502`.\n</code></pre> What's happening in this code? In Rust, strings occupy a fixed memory space.  When modifying a String, a new memory allocation is required since the memory size needed to store the string has changed.  The <code>+=</code> operator, used to alter the value of <code>x</code>, takes a \"mutable\" reference to <code>x</code> and then assigns the modified value back to <code>x</code>.  Essentially, the <code>+=</code> operator takes ownership of <code>x</code>'s value momentarily and then returns it to the variable <code>x</code>.</p> <p>Rust enforces a rule allowing only one mutable reference or any number of immutable references at any given time.  This constraint aligns with memory safety principles: preventing a scenario where one part of the code attempts to modify a value while another part tries to read it.  Such a situation could lead to a race condition, causing the code's behavior to be undefined and reliant on the order of execution.  While it might not appear problematic for sequential code like this, attempting read and write actions across different threads could result in significant issues.</p> <p>So how can we work with mutable and immutable references? Consider the following example: Mutable and immutable references<pre><code>fn main() {\n\n    let mut x:i32 = 42;\n\n    let y: &amp;i32 = &amp;x;\n\n    println!(\"y = {}\", y);\n\n    let mut z: i32 = x;\n    z += 1;\n\n    println!(\"x = {}\", x);\n    println!(\"y = {}\", y);\n    println!(\"z = {}\", z);\n\n\n    {\n        let a = y;\n        let another = &amp;x;\n        println!(\"a = {}\", a);\n        println!(\"another = {}\", another);\n    }\n\n    x += 1;\n    println!(\"x = {}\", x);\n\n    {\n        let b  = &amp;mut x;\n        *b +=  10;\n        println!(\"b = {}\", b);\n\n    }\n\n    println!(\"x = {}\", x);\n    let last = &amp;mut x;\n    *last -= 100;\n    println!(\"last = {}\", last);\n}\n</code></pre></p> <p>In this code snippet, we perform several operations with mutable and immutable references to showcase Rust's ownership and borrowing principles.</p> <ul> <li>Line 3 initializes a mutable <code>i32</code> assigned to variable <code>x</code>.</li> <li>Line 5 creates an immutable reference <code>y</code> to the value of <code>x</code>.</li> <li>Line 9 assigns a new value to <code>x</code>. This operation works because we only have a single immutable reference, <code>y</code>. As <code>i32</code> can be copied, <code>z</code> receives a copy of the value of <code>x</code>, not the actual value.</li> </ul> <p>In lines 17-22, a new scope is created. Here, we transfer ownership of reference <code>y</code> to <code>a</code> and establish a second immutable reference, <code>another</code>, to <code>x</code>. Remembering the three ownership rules (\"When the owner goes out of scope, the value will be dropped\"), when the scope ends at line 22, the values of <code>a</code> and <code>another</code> are dropped. Since <code>a</code> took ownership of <code>y</code>, there are now 0 immutable references. Any attempt to access <code>y</code> would result in an error.</p> <p>In lines 27-32, a new scope introduces a mutable reference <code>b</code> to <code>x</code>. At this point, there are 0 immutable references and 1 mutable reference. Modification of the value behind <code>x</code> is possible by \"dereferencing\" <code>b</code>, illustrated in line 29 (<code>*b +=  10;</code>), which adds 10 to the actual value <code>b</code> is referencing. When this scope ends at line 32, <code>b</code> is dropped, leaving 0 immutable references and 0 mutable references.</p> <p>Finally, lines 35 and 36 create a mutable reference to <code>x</code> and modify its value.</p> <p>Throughout this example, <code>x</code> remains the sole owner of the value, never relinquishing ownership. Borrowing the value (<code>y</code>, <code>a</code>, <code>another</code>, <code>b</code>, <code>last</code>) occurs at multiple stages, but <code>x</code> retains ownership. Although <code>y</code> initially held an immutable reference to <code>x</code>, preventing <code>last</code> from taking a mutable reference, ownership of the reference shifted from <code>y</code> to <code>a</code>. Upon <code>a</code>'s scope exit, the immutable reference was dropped. Throughout this code, multiple immutable references or a single mutable reference were consistently present.</p> <p>Understanding ownership and borrowing is the most challenging concept in Rust. Proficiency in these concepts is crucial for mastering Rust.</p>"},{"location":"Rust/rust_intro/#functional-programming","title":"Functional Programming","text":"<p>In Rust, functional programming can be achieved through two primary methods: using functions defined with the <code>fn</code> keyword or leveraging closures.</p> <p>Functions, declared using the <code>fn</code> keyword, represent a fundamental approach to functional programming in Rust. They encapsulate blocks of code that can be called multiple times with different arguments.</p> <p>Closures, on the other hand, are more powerful and flexible. They are similar to functions but can capture variables from their surrounding environment. Closures allow for defining anonymous functions on the fly, making them highly adaptable for tasks requiring flexibility in behavior and data encapsulation.</p> <p>Both functions and closures play integral roles in enabling functional programming paradigms within Rust, offering different levels of flexibility and usability in various scenarios.</p>"},{"location":"Rust/rust_intro/#functions","title":"Functions","text":"<p>Functions in Rust are defined using the following syntax: Examples of functions<pre><code>fn add_numbers(a: i32, b: i32) -&gt; i32{\n    return a + b;\n}\n\nfn multiply_numbers(a :i32, b :i32) -&gt; i32{\n    a * b\n}\n\nfn print_numbers(a :i32, b :i32) {\n    println!(\"{} + {} = {}\", a,b, a+b);\n}\n\nfn print_numbers_multiply(a :i32, b :i32) -&gt; () {\n    println!(\"{} * {} = {}\", a,b, a*b);\n}\n\nfn main(){\n    let x = 3;\n    let y = 4;\n\n    let sum = add_numbers(x,y);\n    print_numbers(x,y);\n    let product = multiply_numbers(x,y);\n    print_numbers_multiply(x,y);\n\n    println!(\"sum = {}, product = {}\", sum, product);\n\n}\n</code></pre></p> <p>In the example above, three functions are defined using the <code>fn</code> keyword to indicate their creation. When defining functions, specifying the data types of passed arguments is necessary, as demonstrated here by using <code>i32</code> types in all cases. Additionally, if a function returns a value, explicit declaration of the return type is required. Lines 1 and 5 explicitly define the return type as <code>i32</code>, denoted by <code>-&gt; T</code>, where <code>T</code> represents the data type.</p> <p>Lines 9 and 13 introduce functions that do not return any value. When a function doesn't return anything, the <code>-&gt;</code> can be omitted. Alternatively, it's possible to explicitly state the absence of a return value using <code>-&gt; ()</code>.</p> <p>The functions <code>add_numbers</code> and <code>multiply_numbers</code> both return an <code>i32</code>. However, only <code>add_numbers</code> uses a <code>return</code> keyword. In Rust, if a statement isn't followed by a <code>;</code>, it's assumed to be the return value. In the case of <code>multiply_numbers</code>, the absence of <code>;</code> specifies that the function should return <code>a * b</code>.</p> <p>It's important to note that in all these functions, ownership of <code>a</code> and <code>b</code> is taken within the functions. Consequently, when the function's scope ends, both <code>a</code> and <code>b</code> are dropped. While this behavior might not be problematic for <code>i32</code> due to its copy trait, allowing passing a copy of the value rather than the value itself, it's a crucial consideration for other types where ownership might cause different behavior. Consider the following example:</p> Problems with borrowing<pre><code>fn print_string( msg : String) -&gt; (){\n    println!(\"{}\", msg);\n}\n\n\nfn main(){\n    let my_string = String::from(\"Save Ferris!\");\n    print_string(my_string);\n}\n</code></pre> <p>This will give the following error: <pre><code>error[E0382]: borrow of moved value: `my_string`\n --&gt; src/main.rs:9:20\n  |\n7 |     let my_string = String::from(\"Save Ferris!\");\n  |         --------- move occurs because `my_string` has type `String`, which does not implement the `Copy` trait\n8 |     print_string(my_string);\n  |                  --------- value moved here\n9 |     println!(\"{}\", my_string);\n  |                    ^^^^^^^^^ value borrowed here after move\n  |\n</code></pre> Remember that strings have variable lengths, making direct copying non-trivial. Therefore, when <code>print_string</code> receives <code>my_string</code>, it assumes ownership. To address this, we have two solutions: either use the <code>clone</code> method when passing <code>my_string</code> to <code>print_string</code>, or modify <code>print_string</code> to borrow the string by taking a reference instead. The corrected code would appear as follows: Examples of functions with borrowing<pre><code>fn print_string( msg : String) -&gt; (){\n    println!(\"{}\", msg);\n}\n\n\nfn print_string_borrow( msg : &amp;String) -&gt; (){\n    println!(\"{}\", msg);\n}\n\n\nfn main(){\n    let my_string = String::from(\"Save Ferris!\");\n    print_string(my_string.clone());\n    print_string_borrow(&amp;my_string);\n    println!(\"{}\", my_string);\n}\n</code></pre></p> <p>Functions can also return tuples. Consider the following: Example of function returning a tuple<pre><code>fn get_powers( a: i32 ) -&gt; (i32, f32){\n    (a.pow(2), (a as f32).powf(0.33))\n}\n\nfn main(){\n    let x :i32 = 8;\n    let tup = get_powers(x);\n    // Deconstruct tuple\n    let (y, z) : (i32, f32) = get_powers(x);\n\n    println!(\"{}, {}\", tup.0, tup.1 );\n    println!(\"{}, {}\", y, z );\n}\n</code></pre></p> <p>In the example above, a tuple of type <code>(i32, f32)</code> is returned. Line 7 stores the tuple as a variable, while on line 9, explicit deconstruction of the tuple occurs, assigning its elements to variables <code>y</code> and <code>z</code>. Accessing elements of the tuple can be achieved using <code>tup.n</code> to retrieve the nth element.</p>"},{"location":"Rust/rust_intro/#closures","title":"Closures","text":"<p>Closures in Rust bear similarities to lambda functions found in other programming languages. They offer a concise means to create short blocks of functionality within code. Closures, like functions, can capture and manipulate variables from their enclosing scope. They are defined using the <code>|argument| { body }</code> syntax, where <code>argument</code> represents parameters and <code>body</code> signifies the functionality of the closure.</p> <p>An example of a closure definition:</p> Examples of Closures<pre><code>fn main(){\n    let pi  = 3.14_f32;\n\n    let area = |x| pi * x*x;\n\n    let print_area = |x| {\n        println!(\"Area of circle with radius {} is {}\", {x}, area(x));\n    };\n\n    println!(\"The area is: {}\", area(2.));\n    print_area(1.5);\n}\n</code></pre> <p>In lines 4 and 6, two closures are defined. The <code>area</code> closure accepts a variable <code>x</code> and computes the area of a circle with radius <code>x</code>. This closure borrows the value of <code>pi</code> for the duration of its scope. On the other hand, the <code>print_area</code> closure accepts a variable <code>x</code>, prints a statement, and then passes a copy of <code>x</code> to the <code>area</code> closure.</p>"},{"location":"Rust/rust_intro/#flow-control","title":"Flow Control","text":""},{"location":"Rust/rust_intro/#if-statements","title":"If statements","text":"<p>Rust's <code>if</code> statements follow the subsequent syntax:</p> if statements<pre><code>let a :i32 = 4;\n\nif a &gt; 3{\n    println!(\"a is greater than 3\");\n} else if a &lt; 3{\n    println!(\"a is less than 3\");\n} else{\n    println!(\"a is equal to 3\");\n}\n</code></pre> <p>Note that an <code>if</code> block must start with an <code>if</code> statement and may have only one <code>if</code> branch and at most one <code>else</code> branch. However, multiple <code>else if</code> branches can be included as needed.</p> <p><code>if</code> statements are also capable of assigning variables or returning values. Let's consider the following example: returning if statements<pre><code>let a :i32 = 4;\n\nlet my_string :String = if a &gt; 3{\n    \"a is greater than 3\".to_string()\n} else if a &lt; 3{\n    \"a is less than 3\".to_string()\n} else{\n    \"a is equal to 3\".to_string()\n};\n</code></pre> Line 2 defines an immutable string <code>my_string</code>, assigned the value from this <code>if</code> block. In lines 4, 6, and 8, the absence of <code>;</code> at the end of these lines allows them to return the <code>String</code> type. Finally, line 9 concludes the assignment by adding a <code>;</code> at the end of the final block.</p>"},{"location":"Rust/rust_intro/#match","title":"Match","text":"<p><code>match</code> statements in Rust are akin to <code>switch</code> statements found in other programming languages. They enable pattern matching on variables, allowing for concise and comprehensive conditional branching.</p> <p>Matching involves specifying the pattern to match against, which can either be a variable or a condition evaluation (e.g., <code>x &gt; 10</code>). It commences with the keyword <code>match</code> and encloses different options within a set of <code>{}</code>. For each pattern, code branches to run are assigned using the <code>=&gt;</code> syntax. </p> match statements<pre><code>fn main(){\n\n    let a :i32 = 4;\n\n    match a {\n        0..=3 =&gt; {\n            println!(\"a is less than 3\");\n        },\n        4..=10 =&gt; {\n            println!(\"a is greater than 3\");\n        },\n        3 =&gt; {\n            println!(\"a is 3\");\n        },\n        _ =&gt; {\n            println!(\"a is &gt; 10\");\n        },\n    }\n\n    let b = match a {\n        0 =&gt; \"0\",\n        1 =&gt; \"alpha\",\n        2 =&gt; \"2\",\n        3 =&gt; \"delta\",\n        4 =&gt; \"for\",\n        _ =&gt; \"Something else\",\n    };\n\n    println!(\"b is {}\", b);\n}\n</code></pre> <p>In lines 6 and 9, the code searches for values of <code>x</code> within the ranges 0-3 and 4-10, respectively. On line 12, it checks if <code>a</code> equals 3. Finally, on line 15, the default case is defined using <code>_</code>. Each branch in this <code>match</code> statement executes a block of code enclosed within its scope.</p> <p>In the example from line 20-27 we are returning a <code>str</code> based on the pattern found. </p>"},{"location":"Rust/rust_intro/#loops","title":"Loops","text":"<p>Loops in Rust are straightforward and flexible. The <code>loop</code> keyword initiates an infinite loop, allowing code to execute repeatedly within a defined scope until explicitly interrupted by a <code>break</code> statement.</p> <p>For instance: loop example<pre><code>fn main(){\n    let mut i = 0;\n\n    loop {\n        i+=1;\n        if i == 3{\n            continue;\n        } else if i &gt; 10{\n            break;\n        } else{\n            println!(\"i = {}\", i);\n        }\n    }\n}\n</code></pre></p> <p>Lines 4-12 constitute the content wrapped within the <code>loop</code> block, as indicated on line 4. At line 7, a <code>continue</code> statement is employed to skip the iteration where <code>i</code> equals 3. Moreover, line 9 utilizes a <code>break</code> statement to exit the loop when the condition <code>i &gt; 10</code> is met.</p> <p>In Rust, it is possible to assign labels to loops to facilitate <code>continue</code> or <code>break</code> operations targeting a specific loop. This is achieved using the <code>'name: loop {}</code> syntax: <pre><code>fn main(){\n    let mut i = 0;\n\n    'astra : loop {\n        let mut j = 0;\n\n        'kafka : loop{\n            if i &gt; 10{\n                break 'astra;\n            } else if j &gt; 3{\n                break 'kafka;\n            } else{\n                println!(\"i,j = {},{}\", i,j);\n            }\n            j+=1;\n        }\n        i+=1;\n    }\n}\n</code></pre> In the provided example, we establish a parent loop named <code>'astra</code>, encompassing the scope from line 4 to line 18. Within <code>'astra</code>, we define a nested loop named <code>'kafka</code>, spanning lines 7 to 16. </p> <p>At line 8, a <code>break</code> statement exits the <code>'astra</code> loop if <code>i &gt; 10</code>. Furthermore, line 11 employs a <code>break</code> statement to exit the <code>'kafka</code> loop if <code>j &gt; 3</code>. </p> <p>The output of this code will be: <pre><code>i,j = 0,0\ni,j = 0,1\ni,j = 0,2\ni,j = 0,3\ni,j = 1,0\n...\ni,j = 10,2\ni,j = 10,3\n</code></pre></p>"},{"location":"Rust/rust_intro/#for-loops","title":"For Loops","text":"<p>For loops in Rust operate on any data that conforms to an iterator. This includes constructs such as <code>for element in list</code> or <code>for i in a range</code>. The syntax used for these loops is as follows: <pre><code>fn main(){\n    let n:i32 = 10;\n\n    for i in 0..n{\n        println!(\"i = {}\", i);\n    }\n}\n</code></pre> In this context, we define a range <code>0..n</code>, representing the inclusive range from 0 to 9 (Alternatively, we could use <code>0..=9</code>).</p> <p>When dealing with an array or vector of items, we can iterate over them as follows: <pre><code>fn main(){\n    let my_arr: [f32;5] = [1.,2.,3.,43., 3.14];\n    for a in my_arr{\n        println!(\"{}\",a);\n    }\n    println!(\"{:?}\", my_arr);\n}\n</code></pre></p> <p>In the above example, <code>a</code> stores a copy of the values from <code>my_arr</code> rather than a reference to those values. Modifying a will not alter <code>my_arr</code>. However, the behavior slightly differs when working with vectors.</p> <pre><code>fn main(){\n    let my_arr: Vec&lt;f32&gt; = vec![1.,2.,3.,43., 3.14];\n    for a in my_arr{\n        println!(\"{}\",a);\n    }\n\n    println!(\"{:?}\", my_arr);\n}\n</code></pre> <p>The above example will return an error on line 7. <pre><code>   --&gt; src/main.rs:7:22\n    |\n2   |     let my_arr: Vec&lt;f32&gt; = vec![1.,2.,3.,43., 3.14];\n    |         ------ move occurs because `my_arr` has type `Vec&lt;f32&gt;`, which does not implement the `Copy` trait\n3   |     for a in my_arr{\n    |              ------ `my_arr` moved due to this implicit call to `.into_iter()`\n...\n7   |     println!(\"{:?}\", my_arr);\n    |                      ^^^^^^ value borrowed here after move\n    |\n</code></pre></p> <p>The error indicates that <code>Vec&lt;f32&gt;</code> doesn't implement the <code>Copy</code> trait. Consequently, when attempting to iterate over its values, Rust borrows the values rather than making copies. As a result, the ownership of these values is temporarily transferred into the for loop's scope at line 5. However, as the loop ends, these borrowed values are automatically dropped, as their ownership wasn't transferred back outside the loop.</p> <p>Looking further at the compile output we see: <pre><code>help: consider iterating over a slice of the `Vec&lt;f32&gt;`'s content to avoid moving into the `for` loop\n    |\n3   |     for a in &amp;my_arr{\n    |              +\n\nFor more information about this error, try `rustc --explain E0382`.\n</code></pre> Here we see some of the awesome features of the Rust compile. It is smart enough to understand what we are trying to do and suggest a fix to the code. The fixed code would look like:</p> <p>vector for loop<pre><code>fn main(){\n    let my_arr: Vec&lt;f32&gt; = vec![1.,2.,3.,43., 3.14];\n    for a in &amp;my_arr{\n        println!(\"{}\",a);\n    }\n\n    println!(\"{:?}\", my_arr);\n}\n</code></pre> At line 3, we're iterating over a reference to a slice of the vector. In this instance, the vector slice represents the entire range of the vector.</p> <p>We can iterate over tuples to access and combine their values: Asigning Values in a Loop<pre><code>fn main(){\n    let x: Vec&lt;f32&gt; = vec![1.,2.,3.,43., 3.14];\n    let y: Vec&lt;f32&gt; = vec![2.,0.1,5.3,0.001, 3.14];\n    let mut z: Vec&lt;f32&gt; = vec![0.0_f32; 5];\n\n    for ((a, b), i) in x.iter().zip(&amp;y).zip(0..x.len()){\n        println!(\"{},{}\",a, b);\n        z[i] = a + b;\n    }\n\n    println!(\"{:?}\", z);\n}\n</code></pre></p> <p>In the provided example, there are three vectors: <code>x</code>, <code>y</code>, and <code>z</code>, where <code>z</code> is a mutable vector.</p> <p>At line 6, <code>iter</code> is utilized to obtain an iterable reference to <code>x</code>. Subsequently, it is <code>zip</code>ped with a reference to <code>y</code>, invoking the <code>into_iter</code> method for <code>y</code> (similar to the vector for loop example). This action results in a <code>tuple</code> of type <code>(&amp;f32, &amp;f32)</code>. Additionally, another <code>zip</code> operation is performed with the range <code>0..x.len()</code>, effectively creating a loop over a <code>tuple</code> of <code>((&amp;f32, &amp;f32), usize)</code>.</p> <p>Within this loop, values are assigned to <code>z</code>.</p> <p>Aside on <code>iter</code> vs <code>into_iter</code></p> <p>In the \"vector for loop\" example, we employed the <code>for a in my_arr</code> syntax, which implicitly calls the <code>into_iter</code> method. The <code>into_iter</code> method, being a generic method, returns either a copy, a reference, or the value itself. On the other hand, the <code>iter</code> method explicitly returns a reference.</p> <p>If distinguishing between the two seems perplexing, consider <code>into_iter</code> as moving the value \"into\" the scope. If ownership needs to be maintained, it's advisable to use <code>iter</code>. Conversely, if the value can be consumed by the scope, <code>into_iter</code> is preferable.</p> <p>For a more detailed explanation, refer to this Stack Overflow question.</p>"},{"location":"Rust/rust_intro/#looping-the-rust-way","title":"Looping the Rust way","text":"<p>In the \"Assigning Values in a Loop\" section, we explored how to derive values from two vectors to assign to a third vector. However, this approach isn't considered very idiomatic in Rust. A more idiomatic way to achieve this would be:</p> Idomatic Rust For Loop<pre><code>fn main(){\n    let x: Vec&lt;f32&gt; = vec![1.,2.,3.,43., 3.14];\n    let y: Vec&lt;f32&gt; = vec![2.,0.1,5.3,0.001, 3.14];\n\n    let z  = x.iter().zip(&amp;y).map(|(a,b)| a + b).collect::&lt;Vec&lt;f32&gt;&gt;();\n    println!(\"{:?}\", z);\n}\n</code></pre> <p>In this example, we condense the entire loop into a single line of code. Starting with <code>x.iter()</code>, we iterate over references to the values within <code>x</code>. Using the <code>zip</code> function with a reference to <code>y</code> facilitates the iteration over a tuple of type <code>(&amp;f32, &amp;f32)</code>.</p> <p>Each tuple undergoes processing within a closure passed to the <code>map</code> method. This closure deconstructs the tuple into two values and adds them together. The <code>collect()</code> method accumulates the values returned by the closure used in the <code>map</code> method.</p> <p>The \"Turbofish\" syntax, <code>collect::&lt;type&gt;()</code>, informs <code>collect</code> about the desired return type. In this instance, using <code>collect::&lt;Vec&lt;f32&gt;&gt;()</code>, we obtain a <code>Vec&lt;f32&gt;</code>.</p> <p>Using a reduction, as shown in the 'Idiomatic Rust For Loop' example, is a powerful tool. For instance, suppose we aim to extract all even values from a vector, we could employ: <pre><code>fn main() {\n    let values = 0..100;\n\n    let even_squared = values.clone()\n        .filter(|x| x % 2 == 0)\n        .map(|x| x * x)\n        .collect::&lt;Vec&lt;i32&gt;&gt;();\n\n\n    let odd_sum = values.clone()\n        .filter(|x| x % 2 == 1)\n        .sum::&lt;i32&gt;();\n\n        println!(\"{:?}\", even_squared);\n        println!(\"Sum of the odd values = {}\", odd_sum);\n}\n</code></pre></p> <p>In the given code, <code>values</code> is a range from 0 to 99 inclusive, represented as a collection of integers (<code>i32</code>). The operations on this range illustrate various methods provided by Rust's Iterator trait.</p> <p>Starting from line 4, <code>even_squared</code> is created by cloning the <code>values</code> range. The <code>clone</code> method is used here to avoid consuming the original range, enabling separate iteration over the original range (<code>values</code>) and the cloned range. The <code>filter</code> method is then applied to this cloned range, utilizing a closure (<code>|x| x % 2 == 0</code>) to test each element for evenness by performing a modulo operation and checking if the remainder is zero. Elements satisfying this condition are retained, while those failing the test are discarded. The subsequent <code>map</code> method takes the retained even numbers, squares each value by multiplying it by itself (<code>x * x</code>), and produces a transformed iterator. Finally, the <code>collect</code> method is used to gather the squared even numbers into a <code>Vec&lt;i32&gt;</code>.</p> <p>The <code>filter</code> method implicitly calls <code>into_iter</code> on the cloned <code>values</code> range, which temporarily takes ownership of the elements within the scope of the filter operation. After the <code>collect</code> method consumes the iterator, the clone of the <code>values</code> range is no longer needed and gets dropped, releasing its resources.</p> <p>Next, between lines 10 and 12, <code>odd_sum</code> is calculated using a similar approach. Here, the <code>filter</code> method is again used on a cloned range of <code>values</code>, but this time with a closure (<code>|x| x % 2 == 1</code>) that filters for odd numbers. The <code>sum</code> method is applied to this filtered iterator to compute the sum of the odd numbers present in the range.</p> <p>The code concludes by displaying the vector containing squared even numbers (<code>even_squared</code>) and printing the sum of the odd numbers (<code>odd_sum</code>).</p>"},{"location":"Rust/rust_intro/#object-orientated-programming","title":"Object Orientated Programming","text":"<p>In contrast to languages like Python and C++, Rust diverges from class-based inheritance.  It emphasizes struct composition and trait-based polymorphism.  Rather than relying on class inheritance, Rust promotes struct composition, allowing structs to contain instances of other structs or types.  Traits, serving as a form of polymorphism, define sets of methods that types can implement, offering shared behaviors across different types without a single inheritance hierarchy.  This trait-based approach fosters modularity and flexibility while ensuring safety and performance.</p>"},{"location":"Rust/rust_intro/#structs-in-rust","title":"Structs in Rust","text":"<p>Structs in Rust form the foundation of object-oriented programming (OOP). They can be seen as collections of variables that serve a related purpose or represent a specific context. </p> <p>Consider the example below:</p> Example of a Struct<pre><code>struct Point3D {\n    x: f32,\n    y: f32,\n    z: f32,\n    coord_system: String,\n}\n</code></pre> <p>We've created a <code>struct</code> named <code>Point3D</code>, representing a point in 3D space.  The <code>struct</code> is defined by encapsulating member data within curly brackets <code>{}</code> after naming it.  Within this <code>struct</code>, we've defined fields such as <code>x</code>, <code>y</code>, and <code>z</code>, each having the data type <code>f32</code>, representing the coordinates in the x, y, and z axes, respectively.  Additionally, there's a field named <code>coord_system</code> of type <code>String</code>, serving to describe the coordinate system.  In Rust, it's common practice to separate each field with a <code>,</code> and a new line for readability.  The presence of a trailing <code>,</code> after the last field doesn't cause a compile-time error and is often used to facilitate future struct modifications.</p> <p>Methods can be implemented for a <code>struct</code> in Rust, functioning as functions that the <code>struct</code> itself can utilize. These methods can modify the <code>struct</code>, perform actions based on the field data, and more. The <code>impl</code> keyword is used to define these methods:</p> Example of implementing structs<pre><code>impl Point3D {\n\n    // Return a new Point3D\n    fn new() -&gt; Point3D{\n        Point3D{\n            x: 0.0_f32,\n            y: 0.0_f32,\n            z: 0.0_f32,\n            coord_system: \"cartesian\",\n        }\n    }\n\n    // Get the magnitude of the Point3D\n    fn get_magnitude(self :&amp;Self) -&gt; f32{\n        (self.x.powi(2) + self.y.powi(2) + self.z.powi(2))\n        .sqrt()  \n    }\n\n    // Add a constant value to the Point3D\n    fn add_constant(self: &amp;mut Self, c : &amp;f32) -&gt; (){\n        self.x += c / 3.0_f32.sqrt();\n        self.y += c / 3.0_f32.sqrt();\n        self.z += c / 3.0_f32.sqrt();\n    }\n}\n</code></pre> <p>In the code example above, we've implemented three methods for the <code>Point3D</code> struct.</p> <ul> <li> <p>The <code>new</code> function on line 3 creates and returns a new <code>Point3D</code> with default values at the origin (0, 0, 0). To use it: <code>let mut my_point = Point3D::new();</code></p> </li> <li> <p>Line 14 contains the <code>get_magnitude</code> method, which accesses the struct's data without modifying it. It takes a non-mutable reference to itself (<code>&amp;Self</code>) and returns a <code>f32</code>. To call it: <code>my_point.get_magnitude()</code>.</p> </li> <li> <p>The <code>add_constant</code> method, defined on line 20, modifies the <code>Point3D</code>'s data using a given value. It requires a mutable reference to itself (<code>&amp;mut Self</code>) and takes a non-mutable reference to the constant (<code>&amp;f32</code>). Usage example: <code>my_point.add_constant(&amp;3.14);</code>. By taking a reference to the constant, it prevents ownership issues and avoids unintentional dropping of <code>c</code> after line 24.</p> </li> </ul>"},{"location":"Rust/rust_intro/#traits","title":"Traits","text":"<p>Traits in Rust provide a means to define common interfaces that can be implemented by different structs. They enable struct types to share behavior or functionality through shared methods.</p> <p>For instance, let's consider the following struct:</p> Point2D<pre><code>struct Point2D{\n    x: f32,\n    y: f32,\n    coord_system: String,\n}\n</code></pre> <p>The <code>Point2D</code> struct shares similarities with <code>Point3D</code>. It would be beneficial if these structs had some common methods. To achieve this, we can define a <code>trait</code> that provides a shared interface for both structs. This approach allows for greater code flexibility and consistency. Let's explore this concept:</p> Trait Example<pre><code>trait PointLike{\n    fn get_magnitude(self: &amp;Self) -&gt; f32;\n    fn add(self: &amp;mut Self, c : &amp;f32) -&gt; ();    \n}\n</code></pre> <p>The <code>PointLike</code> trait defines a common interface for types that exhibit point-like behavior. For a type to be considered <code>PointLike</code>, it must implement two functions:</p> <ul> <li><code>get_magnitude</code>: This method calculates the magnitude of the point and returns a <code>f32</code>.</li> <li><code>add</code>: Accepts a mutable reference to itself, along with a reference to a <code>f32</code>, and does not return any value.</li> </ul> <p>Notably, the <code>Point3D</code> struct possesses a method named <code>get_magnitude</code>, aligning with the trait's requirements. However, it lacks a function named <code>add</code>, although it has a similar method called <code>add_constant</code>. To conform <code>Point3D</code> to the <code>PointLike</code> trait, we can provide an implementation that satisfies the trait's functions:</p> <p>Implementing Traits<pre><code>struct Point3D {\n    x: f32,\n    y: f32,\n    z: f32,\n    coord_system: String,\n}\n\n\ntrait PointLike{\n    fn get_magnitude(self: &amp;Self) -&gt; f32;\n    fn add(self: &amp;mut Self, c : &amp;f32) -&gt; ();    \n}\n\nimpl Point3D {\n\n    // Return a new Point3D\n    fn new() -&gt; Point3D{\n        Point3D{\n            x: 0.0_f32,\n            y: 0.0_f32,\n            z: 0.0_f32,\n            coord_system: \"cartesian\",\n        }\n    }\n\n    // Get the magnitude of the Point3D\n    fn get_magnitude(self :&amp;Self) -&gt; f32{\n        (self.x.powi(2) + self.y.powi(2) + self.z.powi(2))\n        .sqrt()  \n    }\n\n    // Add a constant value to the Point3D\n    fn add_constant(self: &amp;mut Self, c : &amp;f32) -&gt; (){\n        self.x += c / 3.0_f32.sqrt();\n        self.y += c / 3.0_f32.sqrt();\n        self.z += c / 3.0_f32.sqrt();\n    }\n}\n\nimpl PointLike for Point3D{\n    fn get_magnitude(self: &amp;Self) -&gt; f32{\n        self.get_magnitude()\n    }\n\n    fn add(self: &amp;mut Self, c : &amp;f32) -&gt; (){\n        self.add_constant(c)\n    }    \n}\n</code></pre> Above, we've implemented the <code>PointLike</code> trait for the <code>Point3D</code> struct, utilizing the methods we had previously defined. Extending this trait implementation to <code>Point2D</code> would involve providing similar implementations for the required trait methods.</p> <pre><code>struct Point2D{\n    x: f32,\n    y: f32,\n    coord_system: String,\n}\n\nimpl Point2D{\n    fn new()-&gt;Point2D{\n        Point2D{\n            x: 0.0_f32,\n            y: 0.0_f32,\n            coord_system: \"cartesian\",\n        }\n    }\n}\n\nimpl PointLike for Point2D{\n    fn get_magnitude(self: &amp;Self) -&gt; f32{\n        (self.x.powi(2) + self.y.powi(2)).sqrt()\n    }\n\n    fn add(self: &amp;mut Self, c : &amp;f32) -&gt; (){\n        self.x += c /2.0_f32.sqrt();\n        self.y += c /2.0_f32.sqrt();\n    }    \n}\n</code></pre> <p>In the given example, the trait implementation for <code>Point2D</code> directly utilizes existing methods. When implementing traits, we have the flexibility to use pre-existing methods or define new ones. This versatility allows us to employ these implementations within our code as demonstrated. Using traits<pre><code>fn main(){\n\n    let mut my_3d = Point3D::new();\n    let mut my_2d = Point2D::new();\n\n    my_3d.add(&amp;4.5);\n    my_2d.add(&amp;5.0);\n\n\n    println!(\"Magnitude of 3D {}\", my_3d.get_magnitude());\n    println!(\"Magnitude of 2D {}\", my_2d.get_magnitude());\n}\n</code></pre></p>"},{"location":"Rust/rust_intro/#generics","title":"Generics","text":"<p>Generic types in Rust bear resemblance to C++ templates. They enable us to write versatile code that isn't tied to specific data types. This allows for more reusable and adaptable code. Consider the following illustration:</p> Generic Types<pre><code>Point2D&lt;T&gt;{\n    x:T,\n    y:T,\n}\n</code></pre> <p>Here we have defined a generic <code>Point2D</code> struct that represents a 2D point in space. This struct is designed to work with any data type, as it utilizes the placeholder type <code>T</code> for both the <code>x</code> and <code>y</code> coordinates. Using this placeholder type allows the struct to remain agnostic to the specific data type used for its coordinates.</p> <p>The versatility of this generic struct becomes apparent when implementing methods or functionalities that can work universally across various data types.</p> <pre><code>struct Point2D&lt;T&gt;{\n    x:T,\n    y:T,\n}\n\nimpl &lt;T&gt; Point2D&lt;T&gt;{\n    fn get_x(self : &amp;Self) -&gt; &amp;T{\n        &amp;self.x\n    }\n\n    fn get_y(self : &amp;Self) -&gt; &amp;T{\n        &amp;self.y\n    }\n}\n</code></pre> <p>In the above we have implemented two functions assuming a type <code>T</code>. Each return references of type <code>T</code>. If we wanted to use these we could run: <pre><code>fn main(){\n    let my_point :Point2D&lt;f32&gt; = Point2D{x:0.1, y:4.3};\n    println!(\"x = : {} \", my_point.get_x());\n    println!(\"y = : {} \", my_point.get_y());\n}\n</code></pre> On line two, we are explicitly specifying the data type as <code>f32</code>.</p> <p>We can combine traits and generics to enable custom data types that possess specific traits. Consider the example below: <pre><code>struct PlanetarySystem &lt;T&gt;{\n    planet_location: Vec&lt;T&gt;,\n    planet_name: Vec&lt;String&gt;,\n}\n\n\nimpl &lt;T: PointLike&gt; PlanetarySystem&lt;T&gt; {\n    fn  print_distance_from_star(self : &amp;Self) -&gt; (){\n        for i in 0..self.planet_location.len(){\n            println!(\n                    \"Distance from {} to it's star: {} AU\",\n                    self.planet_name[i], \n                    self.planet_location[i].get_magnitude()\n                )\n        }\n    }\n}\n</code></pre></p> <p>Here, we've defined a struct called <code>PlanetarySystem</code> that takes a generic type implementing the <code>PointLike</code> trait. As the <code>PointLike</code> trait includes the <code>get_magnitude</code> method, this code is applicable to any generic type adhering to the <code>PointLike</code> trait. This flexibility allows us to use the code as demonstrated below:</p> <pre><code>fn main(){\n\n    let mut solar_system: PlanetarySystem&lt;Point3D&gt; = PlanetarySystem{\n        planet_location: vec![\n            Point3D::new(), \n            Point3D::new(), \n            Point3D::new()\n        ],\n        planet_name: vec![\n            \"Mercury\".to_string(), \n            \"Venus\".to_string(), \n            \"Earth\".to_string()\n        ],\n    };\n\n    solar_system.planet_location[0].add(&amp;0.39);\n    solar_system.planet_location[1].add(&amp;0.72);\n    solar_system.planet_location[2].add(&amp;1.);\n    solar_system.print_distance_from_star();\n\n\n    let mut trappst_system: PlanetarySystem&lt;Point2D&gt; = PlanetarySystem{\n        planet_location: vec![Point2D::new(), Point2D::new(), Point2D::new()],\n        planet_name: vec![ \"TRAPPIST-1b\".to_string(), \"TRAPPIST-1c\".to_string(), \"TRAPPIST-1e\".to_string()],\n\n    };\n\n    trappst_system.planet_location[0].add(&amp;0.01154);\n    trappst_system.planet_location[1].add(&amp;0.01580);\n    trappst_system.planet_location[2].add(&amp;0.029);\n    trappst_system.print_distance_from_star();\n}\n</code></pre> <p>Here, we are defining <code>solar_system</code> as a <code>PlanetarySystem</code> that utilizes a <code>Point3D</code> data type. Subsequently, we invoke the <code>print_distance_from_star</code> method, leveraging the fact that <code>Point3D</code> conforms to <code>PointLike</code> and therefore possesses the <code>get_magnitude</code> method.</p> <p>In the context of our <code>PlanetarySystem</code>, the magnitude represents the distance between the planet and the origin, which we designate as the star's location. Running this code yields the following output:</p> <pre><code>Distance from Mercury to it's star: 0.39 AU\nDistance from Venus to it's star: 0.72 AU\nDistance from Earth to it's star: 0.99999994 AU\nDistance from TRAPPIST-1b to it's star: 0.011540001 AU\nDistance from TRAPPIST-1c to it's star: 0.0158 AU\nDistance from TRAPPIST-1e to it's star: 0.029 AU\n</code></pre> <p>In this example, the <code>PlanetarySystem</code> struct doesn't differentiate between using <code>Point3D</code> or <code>Point2D</code>.</p>"},{"location":"VersionControl/","title":"Version Control","text":"<ul> <li>Git (I), Introduction to git and the git command line interface (CLI)</li> <li>Git (II), GitHub as a tool</li> </ul>"},{"location":"VersionControl/git_1/","title":"Git 1","text":""},{"location":"VersionControl/git_1/#about","title":"About","text":"<p>In this tutorial we will, we cover the very basics of Git. Git is a \"free and open source\" (FOSS) \"distributed version control\" system. Let's dissect what \"distributed version control\" means.</p>"},{"location":"VersionControl/git_1/#version-control","title":"Version Control","text":"<p>Version control is a method to keep track of the history of a code base. We can think of this as a list of changes between versions of files from one code version to the next. When working with a version of a code base, we will modify individual files within the code base.  When we are happy with the new versions of the files, we can <code>commit</code> the changes made, along with a helpful message describing the changes.</p> <p> </p> Version control (source xkcd.com)  <p>Informative commit messages help identify changes form the previous commit. This provides useful information when trying to debug issues with the code and identifying significant changes. Knowing the \"history\" of the code, we can <code>revert</code> to a previous version of the code (or a previous version of a specific file). This can help to resolve issues in a code base.</p> <p>Aside on version numbers</p> <p>For example, if version 1.0.0 had a function with an expensive run time (<code>my_func()</code>). <pre><code>def my_func(x):\n    sleep(10)\n    return x[0] + x[1]\n</code></pre></p> <p>We changed the function to reduce the run time by changing how some calculation is done, we would <code>commit</code> a message like \"implemented faster calculation method for my_func\".  <pre><code>def my_func(x):\n    return x[0] + x[1]\n</code></pre> Upon <code>commit</code>-ing we may decide to increment the version number to 1.1.0. Here we changed the second digit. This specifices that there has been a minor change to the code.</p> <p>After some testing of the code, we now know that the new method introduced an error in the calculation, luckily there is a quick fix. <pre><code>def my_func(x):\n    return np.sqrt(x[0]**2 + x[1]**2)\n</code></pre> Implementing this fix we may <code>commit</code> the modifed code with a message \"resolved bug introduced in v1.1.0 for my_func\". With this bug resolved, we now decide to increment the versiion number to 1.1.1.  Here we changed the final digit, this tells the user that going from 1.1.0 to 1.1.1 is a bug fix.</p> <p>After some redesign of our code, we then decide that the <code>my_func</code> function needs a more informative name. So we change the name from <code>my_func</code> to <code>calculate_distance</code>, and <code>commit</code> the changes with the message \"changing my_func name to calculate_distance to improve readabiltiy\". <pre><code>def calculate_distance(x):\n    return np.sqrt(x[0]**2 + x[1]**2)\n</code></pre></p> <p>Changing the function name would be \"API breaking\" meaning that code relying on version 1.1.1 will no longer work on the current version. For this reason we would call the version 2.0.0. Here incrementing the first digit tells the user that it is nolonger compatable with version 1.x.y. <pre><code>...\n\nvalues = [6.28, 3.14]\n# Workds with v1.* but not v2.*\ndistance = my_func(values)\n# Workds with v2.* but not with v1.*\ndistance = calculate_distance(values)\n</code></pre></p> <p>This is known as Semantic Versioning. Under this convention versions are named using 3 digits (version a.b.c): * a: Major version number (when the API changes) * b: Minor version number (when functionality changes in a backwards compatable way) * c: Patch version number (when bug fixes are added in a backwards compatiable way)</p> <p>It's also common to see x.y.z.-alpha, x.y.z-beta or x.y.z-rc. Here the developers have frozen development for the future x.y.z release, with any further major/minor changes to be included in a future version. alpha/beta refer to testing versions with an alpha typically an initial test version open to a small set of interal collaborators, beta refering to a more open or second round of testing which is open to the public.  rc would refer to a \"release candidate\".  A rc version would be used for final tests of the full x.y.z version before making it available. The differences between x.y.z-rc and x.y.z would only reflect bug fixes found during final testing of the x.y.z-rc branch.</p> <p>See this post for a discussion of different versioning schemes.</p>"},{"location":"VersionControl/git_1/#distributed-version-control","title":"Distributed Version Control","text":"<p>Distributed version control allows for the entire codebase to be mirrored on every developer's machine, rather than a single location. This prevents the codebase from being destroyed either due to a disk failure or by local changes. With distributed version control, multiple developers can work on a single code base without conflicting with other developers' work. Git allows for remote developers to change code and <code>commit</code> the code to remote <code>repository</code> (e.g. GitHub), by <code>push</code>-ing their changes. </p> <p>Git allows for <code>branch</code>-es. These are parallel instances of the codebase, which allow for independent development without interfering with other branches. </p> <p> </p> Branching (source nobledesktop.com)  <p>It is common practice to maintain a <code>main</code> or <code>master</code> branch.  This branch reflects the most up to date, yet stable, version of the code. When someone wants to develop a new feature, they will <code>branch</code> out from the <code>main</code> branch to create what might be referred to as a \"feature\" branch. With this branch the developer will make the required changes and run tests. Once they're happy with the changes, they will then <code>merge</code> their code into the <code>main</code> branch.  It is common for integration tests to be passed before allowing anything to be merged into the main branch.</p>"},{"location":"VersionControl/git_1/#using-git","title":"Using Git","text":"<p>Let create a small project to illustrate how to use git.</p> <pre><code>\u00bb tree                    [11:05:58]\n.\n\u251c\u2500\u2500 astro_calculations\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 math_operations.py\n\u2514\u2500\u2500 test.py\n</code></pre> <p>Here we have a simple python package called <code>astro_calculations</code>. The folder <code>astro_calculations</code> includes two files <code>math_operations.py</code>, which defines the functions:</p> math_operations.py<pre><code>def add(a, b):\n    return a + b\n\ndef sub(a,b):\n    return a - b\n\ndef mult(a,b):\n    return a * b\n\ndef div(a,b):\n    return a / b    \n</code></pre> <p>and <code>__init__.py</code> which imports the functions and specifies the <code>__all__</code> list: __init__.py<pre><code>from .math_operations import add, sub, mult, div\n\n__all__ = [\"add\", \"sub\", \"mult\", \"div\"]\n</code></pre></p> <p><code>__all__</code> is a special variable in python which defines a list of what is imported when running <code>from package import *</code>.</p> <p>We also have a <code>test.py</code> file: test.py<pre><code>from astro_calculations import *\n\nx = 1\ny = 2\n\n# Running some test\nassert(add(x,y) == 3)\nassert(sub(x,y) == -1)\nassert(mult(x,y) == 2)\nassert(div(x,y) == 0.5)\n</code></pre></p> <p>This defines a set of tests we can use to ensure the correct behavior of the code.</p>"},{"location":"VersionControl/git_1/#creating-a-new-repository","title":"Creating a new repository","text":"<p>We can create a new repository for the existing <code>astro_calculation</code> codebase by running: <pre><code>git init\n</code></pre></p> <p>From within the project directory. This provides the following output: <pre><code>git init                [11:13:43]\nhint: Using 'master' as the name for the initial branch. This default branch name\nhint: is subject to change. To configure the initial branch name to use in all\nhint: of your new repositories, which will suppress this warning, call:\nhint: \nhint:   git config --global init.defaultBranch &lt;name&gt;\nhint: \nhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\nhint: 'development'. The just-created branch can be renamed via this command:\nhint: \nhint:   git branch -m &lt;name&gt;\nInitialized empty Git repository in /raid/RAID1/Tutorials/git-tutorial/.git/\n</code></pre></p> <p>This also creates a <code>.git</code> folder within the directory. The <code>.git</code> folder is the \"repository\".  This folder contains history of the codebase, by tracking changes made to files.  If one deletes the <code>.git</code> folder, one deletes the history of the codebase (or at least the local copy of it).  With access to the <code>.git</code> folder, a developer can see what has been changed across the entire history of the code, revert to previous versions of the code, check out <code>tagged</code> versions of the code, swap to a different <code>branch</code> of the code and access many other features. For now, we'll ignore the <code>.git</code> folder and its contents.</p>"},{"location":"VersionControl/git_1/#the-staging-area","title":"The staging area","text":"<p>Git uses a \"staging area\" to prepare changes to the codebase to be committed. </p> <p> </p> Staging Area (source git-scm.com)  <p>After files are modified, we can specify which files are set to be committed by adding them to the staging area.  Once we are happy with the files to be committed, we can then commit these changes to the repository (the <code>.git</code> folder).</p> <p>We can see the current status of the repo using: <pre><code>git status\n</code></pre> For the <code>astro_calculations</code> example, this yields: <pre><code>On branch master\n\nNo commits yet\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n    astro_calculations/\n    test.py\n\nnothing added to commit but untracked files present (use \"git add\" to track)\n</code></pre> On line 1 the current branch is specified as <code>master</code>.  On line 3 it says that we have no commits On lines 5-8 we have a list of untracked files.  Untracked files are files which are not being tracked by the fit repo.</p> <p>From the above, we see that we can use <code>git add &lt;file&gt;</code> to add a file or change to a file, to be committed.  We can add all files that have been checked to the commit using <code>git add .</code> .  We can add <code>astro_calculations</code> folder and <code>test.py</code> file to the \"staging area\" with: <pre><code>git add test.py astro_calculations/\n</code></pre></p> <p>Rerunning <code>git status</code> we get: <pre><code>On branch master\n\nNo commits yet\n\nChanges to be committed:\n  (use \"git rm --cached &lt;file&gt;...\" to unstage)\n    new file:   astro_calculations/__init__.py\n    new file:   astro_calculations/math_operations.py\n    new file:   test.py\n</code></pre> On lines 7-9 we now see that we have new files staged to be committed. This is to say that <code>astro_calculations/__init__.py</code>, <code>astro_calculations/math_operations.py</code> and <code>test.py</code> have been added to the staging area. The changes to these files haven't yet been committed to the repo. To commit changes we use: <pre><code>git commit -m \"message\"\n</code></pre> Where \"message\" is the commit message, an informative description of what has been committed.  Since we are adding some initial code to the repo, we might write a commit message like: <pre><code>git commit -m \"Adding initial code to the repo\"\n</code></pre> Which gives the following output: <pre><code>[master (root-commit) 38e2d00] Adding initial code to the repo\n 3 files changed, 24 insertions(+)\n create mode 100644 astro_calculations/__init__.py\n create mode 100644 astro_calculations/math_operations.py\n create mode 100644 test.py\n</code></pre> Here we see that 3 files have been changed, with each file having a <code>create mode</code>.  This tells the repo that new files have been \"created\".</p> <p>Let's make a change to <code>astro_calculations/math_operations.py</code> to include a modulo function: math_operations.py<pre><code>def modulo(a,b):\n    return a % b\n</code></pre></p> <p>Running <code>git status</code> we can see that changes to the file have been detected: <pre><code>On branch master\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n    modified:   astro_calculations/math_operations.py\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n</code></pre> We can see that <code>astro_calculations/math_operations.py</code> have been modified, but not yet added to the staging area. </p>"},{"location":"VersionControl/git_1/#difference-between-commits-git-diff","title":"Difference between commits <code>git diff</code>","text":"<p>Let's use <code>git diff</code> to see what has been changed in <code>astro_calculations/math_operations.py</code>. <pre><code>git diff astro_calculations/math_operations.py\n</code></pre></p> <pre><code>diff --git a/astro_calculations/math_operations.py b/astro_calculations/math_operations.py\nindex fcfdc03..75e7956 100644\n--- a/astro_calculations/math_operations.py\n+++ b/astro_calculations/math_operations.py\n@@ -9,3 +9,6 @@ def mult(a,b):\n\n def div(a,b):\n     return a / b\n+\n+def modulo(a,b):\n+    return a % b\n</code></pre> <p>This shows the <code>diff</code> between the current version of the file and the version last committed to the repo. We can add the file to the staging area:  <pre><code>git add astro_calculations/math_operations.py\n</code></pre></p> <p>And rerun <code>git status</code> to see: <pre><code>On branch master\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n    modified:   astro_calculations/math_operations.py\n</code></pre></p> <p>We can commit the change with a useful message: <pre><code>git commit -m \"Adding modulo function\"\n</code></pre></p>"},{"location":"VersionControl/git_1/#understanding-histories-of-files-git-log","title":"Understanding Histories of files (<code>git log</code>)","text":"<p>Git stores the history of a file by tracking the lines that have changed from one commit to the next, rather than storing multiple copies of the same file. In terms of storage, this provides a light-weight method to store long histories. We can inspect the history of files and repos using the <code>git log</code> command. Running <code>git log</code> will show the previous commits of the repo, for example:</p> <pre><code>commit 4bbbf9e19f3d4f6d067ca04258e2f72e342c8e42 (HEAD -&gt; master)\nAuthor: Ste O Brien &lt;stephan.obrien@mcgill.ca&gt;\nDate:   Fri Jan 26 12:05:25 2024 -0500\n\n    Adding modulo function\n\ncommit 38e2d008831daaaeadcf6466dbe1a375f6cfbc1b\nAuthor: Ste O Brien &lt;stephan.obrien@mcgill.ca&gt;\nDate:   Fri Jan 26 11:50:29 2024 -0500\n\n    Adding initial code to the repo\n</code></pre> <p>On line 1 we can see the id for the most recent commit, on line 2 we see the author of that commit, with the date of line 3.  On line 5 we see the commit message.  In the above example we only have two commits.  Let's add a new file: astro_calculations/printing.py<pre><code>def print_details(a, b):\n    print(f\"Calling with arguments {a} and {b} \")\n</code></pre> We'll also add this to be imported into when we run <code>from astro_calculations import *</code>: ```python title=\"astro_calculations/init.py\" linenums=\"1\"   hl_lines=\"2,3\" from .math_operations import add, sub, mult, div from .printing import print_details all = [\"add\", \"sub\", \"mult\", \"div\", \"print_details\"] <pre><code>Add these to the staging area :\n```bash\ngit add astro_calculations/__init__.py astro_calculations/printing.py \n</code></pre></p> <pre><code>$ git status\nOn branch master\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n    modified:   astro_calculations/__init__.py\n    new file:   astro_calculations/printing.py\n</code></pre> <pre><code>git commit -m \"Adding printing function\"\n[master d38bd2d] Adding printing function\n 2 files changed, 4 insertions(+), 2 deletions(-)\n create mode 100644 astro_calculations/printing.py\n</code></pre> <p>This says that we have modified two files, one of which was newly created.  Note that it records \"insertions\" and \"deletions\" to the files.</p> <p>Running <code>git log</code> we can see our new commit: <pre><code>commit d38bd2de7b4be9b7ccc843889d1894da96e68a00 (HEAD -&gt; master)\nAuthor: Ste O Brien &lt;stephan.obrien@mcgill.ca&gt;\nDate:   Fri Jan 26 13:12:47 2024 -0500\n\n    Adding printing function\n\ncommit 4bbbf9e19f3d4f6d067ca04258e2f72e342c8e42\nAuthor: Ste O Brien &lt;stephan.obrien@mcgill.ca&gt;\nDate:   Fri Jan 26 12:05:25 2024 -0500\n\n    Adding modulo function\n\ncommit 38e2d008831daaaeadcf6466dbe1a375f6cfbc1b\nAuthor: Ste O Brien &lt;stephan.obrien@mcgill.ca&gt;\nDate:   Fri Jan 26 11:50:29 2024 -0500\n\n    Adding initial code to the repo\n</code></pre></p> <p>We can also check the log of an individual file using: <pre><code>git log astro_calculations/math_operations.py\n</code></pre></p> <pre><code>commit 4bbbf9e19f3d4f6d067ca04258e2f72e342c8e42\nAuthor: Ste O Brien &lt;stephan.obrien@mcgill.ca&gt;\nDate:   Fri Jan 26 12:05:25 2024 -0500\n\n    Adding modulo function\n\ncommit 38e2d008831daaaeadcf6466dbe1a375f6cfbc1b\nAuthor: Ste O Brien &lt;stephan.obrien@mcgill.ca&gt;\nDate:   Fri Jan 26 11:50:29 2024 -0500\n\n    Adding initial code to the repo\n(END)\n</code></pre> <p>Recall we've only changed <code>astro_calculations/math_operations.py</code> once since the initial commit. The most recent commit, where we added the printing function, isn't included in this file's history.  Since the file hasn't changed, there is no need to record any difference to the file.</p> <p>If we want to see a more detailed log, we can pass a <code>-p</code> flag to <code>git log</code>: <pre><code>git log -p astro_calculations/math_operations.py\n</code></pre> This gives a more detailed description of the changes: <pre><code>commit 4bbbf9e19f3d4f6d067ca04258e2f72e342c8e42\nAuthor: Ste O Brien &lt;stephan.obrien@mcgill.ca&gt;\nDate:   Fri Jan 26 12:05:25 2024 -0500\n\n    Adding modulo function\n\ndiff --git a/astro_calculations/math_operations.py b/astro_calculations/math_operations.py\nindex fcfdc03..259f48e 100644\n--- a/astro_calculations/math_operations.py\n+++ b/astro_calculations/math_operations.py\n@@ -9,3 +9,6 @@ def mult(a,b):\n\n def div(a,b):\n     return a / b\n+\n+def modulo(a,b):\n+    return a % b\n\ncommit 38e2d008831daaaeadcf6466dbe1a375f6cfbc1b\nAuthor: Ste O Brien &lt;stephan.obrien@mcgill.ca&gt;\nDate:   Fri Jan 26 11:50:29 2024 -0500\n\n    Adding initial code to the repo\n\ndiff --git a/astro_calculations/math_operations.py b/astro_calculations/math_operations.py\nnew file mode 100644\nindex 0000000..fcfdc03\n--- /dev/null\n+++ b/astro_calculations/math_operations.py\n@@ -0,0 +1,11 @@\n+def add(a, b):\n+    return a + b\n+\n+def sub(a,b):\n+    return a - b\n+\n+def mult(a,b):\n+    return a * b\n+\n+def div(a,b):\n+    return a / b\n</code></pre> This highlights the actual changes between commits.</p>"},{"location":"VersionControl/git_1/#viewing-different-version-of-the-code","title":"Viewing different version of the code","text":"<p>We can use the <code>git checkout</code> command to view different version of the code.  This is a versatile command, for the moment let's use it to view a previous version of the codebase. Use <code>git log</code> to view the previous commits: <pre><code>commit d38bd2de7b4be9b7ccc843889d1894da96e68a00 (HEAD -&gt; master)\nAuthor: Ste O'Brien &lt;stephan.obrien@mcgill.ca&gt;\nDate:   Fri Jan 26 13:12:47 2024 -0500\n\n    Adding printing function\n\ncommit 4bbbf9e19f3d4f6d067ca04258e2f72e342c8e42\nAuthor: Ste O'Brien &lt;stephan.obrien@mcgill.ca&gt;\nDate:   Fri Jan 26 12:05:25 2024 -0500\n\n    Adding modulo function\n\ncommit 38e2d008831daaaeadcf6466dbe1a375f6cfbc1b\nAuthor: Ste O'Brien &lt;stephan.obrien@mcgill.ca&gt;\nDate:   Fri Jan 26 11:50:29 2024 -0500\n\n    Adding initial code to the repo\n</code></pre></p> <p>Lets checkout the version of the code prior to adding the printing function <code>4bbbf9e19f3d4f6d067ca04258e2f72e342c8e42</code>: <pre><code>git checkout 4bbbf9e19f3d4f6d067ca04258e2f72e342c8e42\n</code></pre></p> <pre><code>Note: switching to '4bbbf9e19f3d4f6d067ca04258e2f72e342c8e42'.\n\nYou are in 'detached HEAD' state. You can look around, make experimental\nchanges and commit them, and you can discard any commits you make in this\nstate without impacting any branches by switching back to a branch.\n\nIf you want to create a new branch to retain commits you create, you may\ndo so (now or later) by using -c with the switch command. Example:\n\n  git switch -c &lt;new-branch-name&gt;\n\nOr undo this operation with:\n\n  git switch -\n\nTurn off this advice by setting config variable advice.detachedHead to false\n\nHEAD is now at 4bbbf9e Adding modulo function\n</code></pre> <p>We can see that <code>astro_calculations/printing.py</code> is no longer in our directory: <pre><code>tree                                                                                                                       [13:47:40]\n.\n\u251c\u2500\u2500 astro_calculations\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 math_operations.py\n\u2514\u2500\u2500 test.py\n</code></pre></p> <p>We can switch back to the main branch using <code>git switch -</code>: <pre><code>git switch -\n</code></pre></p> <pre><code>Previous HEAD position was 4bbbf9e Adding modulo function\nSwitched to branch 'master'\n</code></pre>"},{"location":"VersionControl/git_1/#reverting-individual-files","title":"Reverting individual files","text":"<p>Let's say we've made some changes to a file and prematurely committed the file.  We might see something like this: <pre><code>On branch master\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n    modified:   astro_calculations/math_operations.py\n</code></pre></p> <p>We can remove the file from the staging area using: <pre><code>git restore --staged astro_calculations/math_operations.py\n</code></pre></p> <p>This file is now marked as modified but not staged: <pre><code>On branch master\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n    modified:   astro_calculations/math_operations.py\n</code></pre></p> <p>We note that the file is still modified. If we wanted to undo these changes and revert to the current version in the <code>master</code> branch we could run: <pre><code>git checkout master -- astro_calculations/math_operations.py\n</code></pre> We will now see the <code>master</code> branch version of <code>astro_calculations/math_operations.py</code>. </p> <p>Let's say we now know that there was a bug introduced in one of the previous commits. We can access the previous version of the file, prior to that commit by first looking at the log of the file: <pre><code>git log -p astro_calculations/math_operations.py\n</code></pre> For me the initial commit is <code>38e2d008831daaaeadcf6466dbe1a375f6cfbc1b</code>. To revert the file back to this commit: <pre><code>git checkout 38e2d008831daaaeadcf6466dbe1a375f6cfbc1b -- astro_calculations/math_operations.py\n</code></pre></p> <p>We will now see the original version of the file, and the file will be listed as <code>modified</code> when running <code>git status</code>. We can commit this as we would any other change to the code <code>git add astro_calculations/math_operations.py</code>.</p>"},{"location":"VersionControl/git_1/#gitignore","title":"Gitignore","text":"<p>You might have noticed that if you've tried to run the python code then a <code>astro_calculations/__pycache__/</code> directory was created. Running <code>git status</code> you might see: <pre><code>Untracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n    astro_calculations/__pycache__/\n</code></pre></p> <p>This directory contains the precompiled python code. When we create a python module, it can be precompiled rather than compiled at run time.  This helps to improve the performance of the code.</p> <p>We don't want this precompiled byte code to be committed or tracked.  We can exclude files using a <code>.gitignore</code> file.  A file which starts with a <code>.</code> will typically be hidden from file browsers and <code>ls</code>.  Git will look for the <code>.gitignore</code> file to define what files should be excluded when monitoring the codebase.</p> <p>Create a file in the root directory of the repository and add the following lines: .gitignore<pre><code># Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n</code></pre></p> <p>This will exclude any directory with the name <code>__pycache__</code> and any files ending in <code>.pyc</code>, <code>.pyo</code> or <code>.pyd</code>.  These are suffices created when compiling python code. If we rerun <code>git status</code> we see: <pre><code>Untracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n    .gitignore\n\nnothing added to commit but untracked files present (use \"git add\" to track)\n</code></pre> Git sees the <code>.gitignore</code> file and knows to exclude the <code>astro_calculations/__pycache__/</code> directory because it has the <code>__pycache__</code> pattern. We should add and commit the <code>.gitignore</code> file.  As the project gets more complicated we might want to add additional patterns to this file. Standard <code>.gitignore</code> files can be found here.</p>"},{"location":"VersionControl/git_1/#branching","title":"Branching","text":"<p>Git uses <code>branch</code>-es to allow the codebase to be modified without affecting the current state of the <code>branch</code>. When branching we take an instance of one <code>branch</code> of the code (for example the <code>master</code> or <code>main</code> branch), and spawn a copy of that <code>branch</code> that where we will develop the code further. This is particularly useful when developing and testing new features of the code base without disrupting the codebase for other users.</p>"},{"location":"VersionControl/git_1/#renaming-branches","title":"Renaming branches","text":"<p>We'll start off by using the <code>branch</code> command to rename the <code>master</code> branch. First make sure we're on the <code>master</code> branch. <pre><code>git checkout master\n</code></pre></p> <p>Now rename the branch using: <pre><code>git branch -M main\n</code></pre> <code>git status</code> should now show that we are on the <code>main</code> branch: <pre><code>On branch main\n...\n</code></pre> It is good practice to adopt inclusive language standards when programming. </p>"},{"location":"VersionControl/git_1/#creating-new-branches","title":"Creating new branches","text":"<p>We can create a new branch from the current branch we are on using: <pre><code>git checkout -b documentation                                                                                                  [9:34:01]\n</code></pre> <pre><code>Switched to a new branch 'documentation'\n</code></pre></p> <p>Here we have used <code>checkout</code> with the <code>-b</code> flag to create a new branch called <code>documentation</code>.  Under the hood the <code>-b</code> flag tells git to create a new branch called <code>documentation</code> and then immediately switch to that branch. We can use <code>git branch</code> to see the current branched and <code>git branch new-branch</code> to create a new branch called <code>new-branch</code>, without switching to that branch.</p> <p>We can change between branches using <code>git checkout &lt;branch_name&gt;</code>. For example <code>git checkout main</code> will put us back on the <code>main</code> branch.</p> <p>From the <code>documentation</code> branch lets add some doc strings to help document our code: astro_calculations/math_operations.py<pre><code>def add(a, b):\n    \"\"\"Adds two values together.\n\n    Args:\n      a: input value float or int\n      b: input value float or int\n\n    Returns:\n      a + b\n\n    Examples:\n        Two values can be added like:\n\n        &gt;&gt;&gt; add(1,2)\n        3\n    \"\"\"\n    return a + b\n\ndef sub(a,b):\n    \"\"\"Subtracts one value from another.\n\n    Args:\n      a: input value float or int\n      b: input value float or int\n\n    Returns:\n      a - b\n\n    Examples:\n        Two values can be subtracted like:\n\n        &gt;&gt;&gt; sub(1,2)\n        -1\n    \"\"\"\n    return a - b\n\ndef mult(a,b):\n    \"\"\"Multiplies two values together.\n\n    Args:\n      a: input value float or int\n      b: input value float or int\n\n    Returns:\n      a * b\n\n    Examples:\n        Two values can be multiplied like:\n\n        &gt;&gt;&gt; mult(1,2)\n        2\n    \"\"\"\n    return a * b\n\ndef div(a,b):\n    \"\"\"Divides one value by another.\n\n    Args:\n      a: input value float or int\n      b: input value float or int\n\n    Returns:\n      a / b\n\n    Examples:\n        Two values can be divided like:\n\n        &gt;&gt;&gt; div(1,2)\n        0.5\n    \"\"\"\n    return a / b\n</code></pre></p> <p>And commit this to the <code>documentation</code> branch.  This can be merged into the <code>main</code> branch using the following: <pre><code># Change into the main branch\ngit checkout main\n</code></pre></p> <p>Use <code>git merge</code> to merge the <code>documentation</code> branch into the <code>main</code> branch.  <pre><code>git merge documentation\n</code></pre></p> <pre><code>Updating e4ea138..b1ce3e9\nFast-forward\n astro_calculations/math_operations.py | 60 ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n astro_calculations/printing.py        | 10 ++++++++++\n 2 files changed, 70 insertions(+)\n</code></pre> <p>The documentation branch can then be deleted using: <pre><code>git branch -d documentation\n</code></pre> <pre><code>Deleted branch documentation (was b1ce3e9).\n</code></pre></p>"},{"location":"VersionControl/git_1/#resolving-a-confliction","title":"Resolving a Confliction","text":"<p>Let's create two new branches from the current <code>main</code> branch.  <pre><code>git checkout -b improve_tests\n</code></pre> In the <code>improve_tests</code> branch we will improve the testing program <code>test.py</code>. test.py<pre><code>import numpy as np\nfrom astro_calculations import *\n\nx = 1\ny = 2\n\n# Testing all values between 0 and 99\nfor i in range(100):\n    assert(add(i,x) == i + 1)\n    assert(sub(i,x) == i - 1)\n    assert(mult(i,y) == i * y)\n    assert(div(i,y) == i / y)\n\nx_arr = np.array([1,2,3,4])\ny_arr = np.array([1,2,3,4])\n\n# Testing numpy arrays\nassert(np.all(add(x_arr, y_arr) == (x_arr + y_arr)))\nassert(np.all(sub(x_arr, y_arr) == x_arr - y_arr))\nassert(np.all(mult(x_arr, y_arr) == x_arr * y_arr))\nassert(np.all(div(x_arr, y_arr) == x_arr / y_arr))\n</code></pre> Add and commit these changes to the <code>improve_tests</code> branch.</p> <p>Create another new branch from the <code>main</code> branch. We can specify a branch to branch from like so: <pre><code>git checkout -b advanced_operations main\n</code></pre></p> <p>In the <code>advanced_operations</code> branch lets add a <code>sqrt</code> function to the <code>math_operations.py</code> file: astro_calculations/math_operations.py<pre><code>...\n\ndef sqrt(a):\n    \"\"\"Returns the square root if real, or 0 if complex.\n\n    Args:\n      a: input value float or int\n\n    Returns:\n      np.sqrt(a)\n\n    Examples:\n        Two values can be divided like:\n\n        &gt;&gt;&gt; sqrt(4)\n        2\n    \"\"\"\n    if a &gt;= 0:\n        return np.sqrt(a)\n    else:\n        return 0\n</code></pre></p> <p>Add this function to the <code>__init__.py</code> file: astro_calculations/math_operations.py<pre><code>from .math_operations import add, sub, mult, div, sqrt\nfrom .printing import print_details\n__all__ = [\"add\", \"sub\", \"mult\", \"div\", \"sqrt\", \"print_details\"]\n</code></pre></p> <p>Let's also add a test case to the <code>test.py</code> file: test.py<pre><code>from astro_calculations import *\n\nx = 1\ny = 2\n\n# Running some test\nassert(add(x,y) == 3)\nassert(sub(x,y) == -1)\nassert(mult(x,y) == 2)\nassert(div(x,y) == 0.5)\nassert(sqrt(4) == 2)\n</code></pre></p> <p>So we currently have 3 branches. * <code>main</code> a branch one commit behind both of the feature branches * <code>improve_tests</code> branch which has modified <code>test.py</code> * <code>advanced_operations</code> branch which has modified <code>test.py</code>, <code>astro_calculations/math_operations.py</code> and <code>astro_calculations/__init__.py</code></p> <p>We can start merging branches together using <code>git merge</code>. First lets merge <code>improve_tests</code> into main. <pre><code>$ git checkout main\n$ git merge improve_tests                                                                                                         [10:57:48]\nUpdating b1ce3e9..c28271c\nFast-forward\n test.py | 21 ++++++++++++++++-----\n 1 file changed, 16 insertions(+), 5 deletions(-)\n</code></pre></p> <p>This merged without any issues. Now lets merge in <code>advanced_operations</code>: <pre><code>$ git merge advanced_operations                                                                                                   [10:57:55]\nAuto-merging test.py\nCONFLICT (content): Merge conflict in test.py\nAutomatic merge failed; fix conflicts and then commit the result.\n</code></pre></p> <p>This detects a confliction in the <code>test.py</code>. If we look contents of <code>test.py</code> we will see something like the following: test.py<pre><code>import numpy as np\nfrom astro_calculations import *\n\nx = 1\ny = 2\n\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n# Testing all values between 0 and 99\nfor i in range(100):\n    assert(add(i,x) == i + 1)\n    assert(sub(i,x) == i - 1)\n    assert(mult(i,y) == i * y)\n    assert(div(i,y) == i / y)\n\nx_arr = np.array([1,2,3,4])\ny_arr = np.array([1,2,3,4])\n\n# Testing numpy arrays\nassert(np.all(add(x_arr, y_arr) == (x_arr + y_arr)))\nassert(np.all(sub(x_arr, y_arr) == x_arr - y_arr))\nassert(np.all(mult(x_arr, y_arr) == x_arr * y_arr))\nassert(np.all(div(x_arr, y_arr) == x_arr / y_arr))\n=======\n# Running some test\nassert(add(x,y) == 3)\nassert(sub(x,y) == -1)\nassert(mult(x,y) == 2)\nassert(div(x,y) == 0.5)\nassert(sqrt(4) == 2)\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; advanced_operations\n</code></pre> Here we can see that the file has been modified to include all changes.  The file also lets us know what sections of the code are conflicting and which branch they have been modified from. Everything between <code>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</code> and <code>=======</code> (lines 7 and 23) exist in our current branch while everything between <code>=======</code> and <code>&gt;&gt;&gt;&gt;&gt;&gt;&gt; advanced_operations</code> (lines 23 and 30) is the requested change from the <code>advanced_operations</code> branch. We can see that the new changes would undo the changes from the <code>improve_tests</code> branch.  We can directly modify this file to resolve the confliction: test.py<pre><code>import numpy as np\nfrom astro_calculations import *\n\nx = 1\ny = 2\n\n# Testing all values between 0 and 99\nfor i in range(100):\n    assert(add(i,x) == i + 1)\n    assert(sub(i,x) == i - 1)\n    assert(mult(i,y) == i * y)\n    assert(div(i,y) == i / y)\n    assert(sqrt(i) == np.sqrt(i))\n\nx_arr = np.array([1,2,3,4])\ny_arr = np.array([1,2,3,4])\n\n# Testing numpy arrays\nassert(np.all(add(x_arr, y_arr) == (x_arr + y_arr)))\nassert(np.all(sub(x_arr, y_arr) == x_arr - y_arr))\nassert(np.all(mult(x_arr, y_arr) == x_arr * y_arr))\nassert(np.all(div(x_arr, y_arr) == x_arr / y_arr))\nassert(np.all(sqrt(x_arr) == [np.sqrt(x) if x &gt;= 0 else 0 for x in x_arr]))\n</code></pre></p> <p>Here we note that we now have a <code>sqrt</code> function to test. This is done on lines 13 and 23. If you try this test, it will fail because <code>sqrt</code> isn't equipped to handle <code>numpy.array</code>.  We change this using the <code>np.vectorize</code> function. astro_calculations.py<pre><code>...\ndef _sqrt(a):\n    \"\"\"Returns the square root if real, or 0 if complex.\n\n    Args:\n      a: input value float or int\n\n    Returns:\n      np.sqrt(a)\n\n    Examples:\n        Two values can be divided like:\n\n        &gt;&gt;&gt; sqrt(4)\n        2\n    \"\"\"\n    if a &gt;= 0:\n        return np.sqrt(a)\n    else:\n        return 0\n\nsqrt = np.vectorize(_sqrt)\n</code></pre> Here we've renamed the <code>sqrt</code> function to be <code>_sqrt</code> and created a new function <code>sqrt = np.vectorize(_sqrt)</code>.  The <code>sqrt</code> function will now call the vectorized <code>_sqrt</code> function, allowing us to pass a <code>numpy.array</code>.</p>"},{"location":"VersionControl/git_1/#tagging","title":"Tagging","text":"<p>We can <code>tag</code> a specific version of the code.  This can be used to signal to users specific versions of the code for use. For example, a user may want to use a stable version of the code, rather than using the <code>main</code> branch which may have feature breaking changes in the future. We can tag a version using: <pre><code>git tag -a v0.1.0 -m \"First Release\"\n</code></pre> Here we are adopting the Semantic Versioning convention. We can see existing tags with: <pre><code>git tag\n</code></pre></p> <p>We can always delete a tag using: <pre><code>git tag -d &lt;tag-name&gt;\n</code></pre></p>"},{"location":"VersionControl/git_1/#pushing-to-a-remote-repository","title":"Pushing to a remote repository","text":"<p>Git allows for remote repositories.  This allows to maintain a hosted copy of the repository, which better protects against loss due to a computer crash. There are many free and for profit hosting services such as GitHub, GitLab and BitBucket.  You can also host your own remote repository. Here we'll use GitHub, since GitHub offers free premium accounts for educations institutes (instructions for students, instructions for \"teachers\").</p> <p> </p>  Creating a new repository  <p> </p>  Setup new repo  <p>From the newly created repo we can get the url by clicking on the green <code>code</code> button to the top right of the page. </p> <p> </p>  Get the remote origin url  <p>With the new repo created we can add the remote origin for our repo: <pre><code>git remote add origin git@github.com:steob92/astro_calculations.git \n</code></pre> If we have initialized the repo with a <code>README.md</code> or  <code>.gitignore</code> file we might want to <code>pull</code> the remote changes into our local repo. First let's set the remote origin: <pre><code>git branch --set-upstream-to=origin/main main \n</code></pre> Next we can try to <code>git pull</code>: <pre><code> git pull                                                                                                                        [12:01:38]\nhint: You have divergent branches and need to specify how to reconcile them.\nhint: You can do so by running one of the following commands sometime before\nhint: your next pull:\nhint: \nhint:   git config pull.rebase false  # merge (the default strategy)\nhint:   git config pull.rebase true   # rebase\nhint:   git config pull.ff only       # fast-forward only\nhint: \nhint: You can replace \"git config\" with \"git config --global\" to set a default\nhint: preference for all repositories. You can also pass --rebase, --no-rebase,\nhint: or --ff-only on the command line to override the configured default per\nhint: invocation.\nfatal: Need to specify how to reconcile divergent branches.\n</code></pre></p> <p>We have a divergent branch since the remote main has a <code>README.md</code> file.  We can follow the default strategy: <pre><code>git config pull.rebase false\n</code></pre></p> <p>And then <code>pull</code> <pre><code>git pull --allow-unrelated-histories    \n</code></pre></p> <p>The <code>--allow-unrealted-histories</code> resolves an issue with <code>README.md</code> not having a history in our local repo: <pre><code>Merge made by the 'ort' strategy.\n README.md | 1 +\n 1 file changed, 1 insertion(+)\n create mode 100644 README.md\n</code></pre></p> <p>We can then <code>push</code> our local changes to the remote repo: <pre><code>git push\n</code></pre> <pre><code>Enumerating objects: 43, done.\nCounting objects: 100% (43/43), done.\nDelta compression using up to 16 threads\nCompressing objects: 100% (42/42), done.\nWriting objects: 100% (42/42), 4.89 KiB | 2.45 MiB/s, done.\nTotal 42 (delta 9), reused 0 (delta 0), pack-reused 0\nremote: Resolving deltas: 100% (9/9), done.\nTo github.com:steob92/astro_calculations.git\n   78f86d6..4a5bc79  main -&gt; main\n</code></pre></p> <p> </p>  Uploaded changes to GitHub  <p>We can see the GitHub repo has been changed.</p> <p>We can also push our tag to GitHub with: <pre><code>$ git push origin v0.1.0\n\nEnumerating objects: 1, done.\nCounting objects: 100% (1/1), done.\nWriting objects: 100% (1/1), 172 bytes | 172.00 KiB/s, done.\nTotal 1 (delta 0), reused 0 (delta 0), pack-reused 0\nTo github.com:steob92/astro_calculations.git\n * [new tag]         v0.1.0 -&gt; v0.1.0\n</code></pre> This tag will appear on the GitHub page under \"Releases\".</p> <p>We can use the editor on github to directly edit the <code>README.md</code> file.</p> <p>Once edited we can <code>pull</code> the changes to our local copy using: <pre><code>git pull\n</code></pre> <pre><code>remote: Enumerating objects: 5, done.\nremote: Counting objects: 100% (5/5), done.\nremote: Compressing objects: 100% (3/3), done.\nremote: Total 3 (delta 1), reused 0 (delta 0), pack-reused 0\nUnpacking objects: 100% (3/3), 974 bytes | 974.00 KiB/s, done.\nFrom github.com:steob92/astro_calculations\n   4a5bc79..82fc7e0  main       -&gt; origin/main\nUpdating 4a5bc79..82fc7e0\nFast-forward\n README.md | 4 +++-\n 1 file changed, 3 insertions(+), 1 deletion(-)\n</code></pre></p>"},{"location":"VersionControl/git_1/#remote-branches","title":"Remote Branches","text":"<p>Let's create a new branch to improve the documentation: <pre><code>git checkout -b update_readme\n</code></pre></p> <p>Make a small change to <code>README.md</code>: README.md<pre><code># astro_calculations\n\nA python package to do some astronomical calculations\n\nExample `add(4,5)`\n</code></pre></p> <p>Add the file and commit the change. We can add a remote origin for this branch using: <pre><code>git push --set-upstream origin update_readme\n</code></pre></p> <p>Once we've added the remote upstream we can make further changes and push them with just <code>git push</code>.</p> <p>We can force a deletion of the local <code>update_readme</code> branch using <code>-D</code>: <pre><code>git branch -D update_readme                                                                                                     [12:16:22]\n</code></pre></p> <p>With the branch deleted we can checkout the remote version of the <code>update_readme</code> branch. First let's <code>fetch</code> any changes to the remote repo: <pre><code>git fetch\n</code></pre></p> <p>Next let's look at the local and remote branches available. <code>git branch</code> will list the local branches while <code>git branch -r</code> will list the remote branches: <pre><code>  origin/main\n  origin/update_readme\n</code></pre></p> <p>We can checkout the remote branch <code>update_readme</code> using: <pre><code>git checkout -b update_readme origin/update_readme \n</code></pre> This creates a new branch locally named <code>update_readme</code>  with the upstream <code>origin/update_readme</code>. The two names can be different, but it is good practice to keep a consistent naming between remote and local branches.</p>"},{"location":"VersionControl/git_1/#closing","title":"Closing","text":"<p>In this tutorial we've gone over the basics of the git command line interface (CLI).  We've learned the commands used to create a new repository, commit local changes to the repository, create branches to work on new features within our code base, merge these branches back into the main, revert to previous versions and tag specific instances of the code base. We finally showed how to push a repository to a remote respository hosting service such as GitHub.  In the next tutorial we'll focus on how to use GitHub as a tool for collaborative development.</p>"},{"location":"VersionControl/git_2/","title":"Git 2","text":""},{"location":"VersionControl/git_2/#about","title":"About","text":"<p>In this tutorial, we will build on the basics of git tutorial by moving our codebase to GitHub.  We'll assume the codebase developed in the basics of git tutorial, which, at the end of the tutorial, we pushed to the repository on GitHub.  To get the most out of this tutorial, please ensure: - You have a GitHub account. I strongly recommend using your McGill account to avail yourself of a free GitHub education account. See (instructions for students and instructions for educators). - You have a codebase on GitHub that you can play around with (for example, the basics of git tutorial codebase).</p> <p>Here, we'll be focusing on using GitHub as a tool for development, rather than simply a place to store a codebase. By the end of the tutorial, you'll be able to:</p> <ul> <li>Navigate GitHub's interface for your repository</li> <li>Modify files from within GitHub</li> <li>Open and close issues, and reference specific commits when discussing them</li> <li>Use GitHub to merge branches using a pull request</li> <li>Use GitHub to streamline tasks to allow you to focus on more important things!</li> </ul>"},{"location":"VersionControl/git_2/#welcome-to-github","title":"Welcome to GitHub","text":"<p>In the previous tutorial, we introduced the concept of a remote repository. While today, we'll be using GitHub, there are other options such as GitLab and BitBucket.</p> <p>Navigating to a GitHub repo, we will be presented with something like this:</p> <p> </p>  Annotated GitHub repo homepage  <ol> <li>In the top left corner, we can see the owner and name of the repo. In this case, the owner is steob92 (me), and the name of the repo is \"astro_calculations\". You'll also notice a little lock icon; this signifies that the repo is a \"private repo\". It means that only accounts with the correct permissions can view this repo (more on this later). With the owner's account name and the name of the repo, we can quickly find the URL of the GitHub page, which takes the format <code>https://github.com/&lt;owner_name&gt;/&lt;repo_name&gt;/</code>.</li> <li>Here we can see the name of the repo along with the owner's icon.</li> <li>Here we see the latest update to the <code>main</code> branch. We can see that the last update was 2 weeks ago, with 12 commits pushed, made by a user with the profile name steob92, and the commit message was <code>Update README.md</code>.</li> <li>Here we can see a snapshot of the repo. We can see that we are currently looking at the <code>main</code> branch, with two branches and 1 tagged version of the code. Clicking on the <code>main</code> icon presents a dropdown menu with the active branches, allowing us to view a different branch of the codebase.</li> <li>Tagged versions of the code appear under the \"Releases\" section on the page. If we want to view different releases and download the source code for a specific release, we could click on the \"1 tags\" link. From here, we can also create a new tag or release of the code.</li> <li>Here we can see a snapshot of the programming languages used within this repo. At the moment, we have a pure Python package, so this appears as 100% Python.</li> <li>Here we can see the contents of the <code>README.md</code> file. GitHub will automatically render Markdown files when viewing them. Additionally, the <code>README.md</code> file is a special file name. If present in a folder, GitHub will render this page below the list of files present in the folder. This provides a useful way of providing documentation for users.</li> <li>Here we can see a panel of options for the repo. We'll go through some of these options in more detail later. The important thing to remember here is that these options are specific to this repository.</li> <li>In the top right corner, we can see notifications and the user account settings. When collaborating, we can receive notifications relating to the codebase. For example, if we're assigned to resolve an issue, we would receive a notification here. The user account settings can be accessed by clicking on the user's portrait.</li> </ol>"},{"location":"VersionControl/git_2/#viewing-and-modifying-files-on-github","title":"Viewing and modifying files on GitHub","text":"<p>We can view raw files on GitHub by clicking on them. Let's start by navigating inside the <code>astro_calculations</code> folder by clicking on it and then take a look at the <code>math_operations.py</code> file. We can now see raw Python code.</p> <p>We can edit this code by clicking on the pencil icon in the top right of the panel. This changes our view to an edit mode. Let's modify the file by adding a function to convert from \\(km/hr\\) to \\(m/s\\):</p> <p><pre><code>def convert(x):\n    \"\"\"Converts from km/hr to m/s\n\n    Args:\n      x: input value in km/hr to be converted Quantity\n\n    Returns:\n      x in m/s\n    \"\"\"\n    return (x * u.km / u.hr).to(\"m/s\")\n</code></pre> We can commit this change by either hitting Ctrl+S or by clicking the <code>Commit changes...</code> button.</p> <p> </p>  Commiting changes via GitHub  <p>Doing so prompts us to write a commit message:</p> <p> </p>  Writing a commit message via Github  <p>We see that we have the option to write a more detailed commit message.  We can also choose to commit directly to the <code>main</code> branch or create a new branch and start a \"pull request\".  It is bad practice to modify the <code>main</code> branch this way; we should aim to keep the <code>main</code> branch as stable as possible, so we should create a new branch and start a pull request.  For now, let's commit directly to the <code>main</code> branch.</p> <p>With the commit now committed to the <code>main</code> branch, we can now see the following message above the code:</p> <p> </p>  Details on the latest commit is shown above the file.  <p>This tells us the latest commit to this file.  We can also see when the commit was made (1 minute ago), the commit message (<code>Update math_operations.py</code>), and the commit ID (<code>29d1c33</code>).  If we click on the three dots beside the commit message, we see the more detailed commit message <code>Adding function to convert from km/hr to m/s</code>.  If we click on the commit ID (<code>29d1c33</code>), we are brought to a page which shows the previous and current versions of the code side by side, with additions and subtractions highlighted with green <code>+</code> and red <code>-</code> respectively.</p> <p> </p>  Viewing the `diff` for a commit.  <p>Below the code block, we can also have the option to comment on the commit. This can be useful for discussing changes to the code with other collaborators. We can write in Markdown in this comment box, with the Markdown being rendered in the preview tab. The submitted comment will also be rendered.</p>"},{"location":"VersionControl/git_2/#blaming-opening-issues-and-pull-requests","title":"Blaming, Opening Issues and Pull Requests","text":""},{"location":"VersionControl/git_2/#blaming","title":"Blaming","text":"<p>You may have noticed that we have an error in our code.  Specifically, the <code>convert</code> function has the line: <pre><code>def convert(x):\n...\n\n    return (x * u.km / u.hr).to(\"m/s\")\n</code></pre></p> <p>However, <code>u</code> is never defined. Here, we've omitted importing the <code>Units</code> class from the <code>astropy</code> package. If we attempted to run this code, there would be an error.</p> <p>For more complicated codebases, it might be difficult to debug this, so we might want to talk to the person who implemented this section of the code.  We can open the file in GitHub and click on the line number that is causing the problem.  This will highlight the line and present a dropdown menu accessible by clicking the three dots.</p> <p> </p>  Highlighting a line in GitHub  <p>There are many useful options here, but let's start by looking at the <code>View git blame</code> option:</p> <p> </p>  Viewing `git blame` through GitHub.  <p>Here, we can see a line-by-line breakdown of the code, with each section showing when the last modification was made, along with the developer who implemented that line.</p>"},{"location":"VersionControl/git_2/#opening-an-issue","title":"Opening an Issue","text":"<p>Going back to the original code, we can click on the three dots again and select the \"Reference in new issue\" option. This will bring us to a new issue, with a link to the line automatically added to the issue. (Note: If we had used the line from the git blame page, the link would point to the git blame line.)</p> <p>We can write some useful debugging information for the developers. Since I know who should look at this problem, I am going to assign the user who last edited that line to fix the problem. I'm also going to label this issue as a <code>bug</code>.</p> <p> </p>  Opening an issue on GitHub.  <p>Submitting this issue will take us to the issue page for this problem.  From there, we can add additional information, comment on the issue, and close it.</p> <p>By clicking on the <code>Issues</code> tab in the repository options panel, we can view all the current issues.</p> <p> </p>  Viewing current issues on GitHub.  <p>Note that the issue created has the number <code>#1</code>. Each issue will be assigned a unique ID.</p> <p>Issues are a great way to communicate bugs to developers and request new features. For example, let's say we want to request a new feature. We can create a new issue, write a brief description of the feature, and then label the issue as an <code>enhancement</code>. There are various options for labels, including <code>help wanted</code>, <code>good first issue</code>, and <code>question</code>.</p> <p> </p>  Labeling issues on GitHub.  <p>Specifying labels can help encourage collaboration by specifying how new developers can contribute.</p> <p>We can also add <code>project</code> and <code>milestone</code> tags to further categorize issues.  For example, the development team might have identified specific goals for a particular version (e.g., V2.0.0, the next major release) of the code.  Issues can be used to track the progress being made on these milestones.</p>"},{"location":"VersionControl/git_2/#pull-requests","title":"Pull Requests","text":"<p>In GitHub terminology, when we want to merge one branch into another branch, we open a \"Pull Request\". Here, we are \"requesting\" that one branch pull changes from another branch. On other platforms, this may be referred to as a \"Merge Request\". As far as we need to be concerned, these are the same processes.  We are requesting one branch to be merged into another branch.</p> <p>Let's edit the code to resolve this issue.  Instead of editing on GitHub, we'll use our local machine.  Start by pulling the version of the code on GitHub to our local machine: <pre><code>git-tutorial(main)\u00bb git pull                                                                 [13:13:08]\nremote: Enumerating objects: 7, done.\nremote: Counting objects: 100% (7/7), done.\nremote: Compressing objects: 100% (4/4), done.\nremote: Total 4 (delta 2), reused 0 (delta 0), pack-reused 0\nUnpacking objects: 100% (4/4), 1.21 KiB | 621.00 KiB/s, done.\nFrom github.com:steob92/astro_calculations\n   82fc7e0..29d1c33  main       -&gt; origin/main\nUpdating 82fc7e0..29d1c33\nFast-forward\n astro_calculations/math_operations.py | 13 ++++++++++++-\n 1 file changed, 12 insertions(+), 1 deletion(-)\n</code></pre></p> <p>We can see that the changes have been picked up. Let's create a new development branch: <pre><code>git checkout -b astropy_issue                                            [13:22:48]\nSwitched to a new branch 'astropy_issue'\n</code></pre></p> <p>Let's make a few changes to the code.  Firstly, let's add the <code>convert</code> function to our <code>astro_calculations/__init__.py</code> file so we can import it: astro_calculations/__init__.py<pre><code>from .math_operations import add, sub, mult, div, sqrt, convert\nfrom .printing import print_details\n__all__ = [\"add\", \"sub\", \"mult\", \"div\", \"sqrt\", \"print_details\", \"convert\"]\n</code></pre></p> <p>Secondly, let's write a quick test to make sure the code will still work (<code>test.py</code>): test.py<pre><code>import numpy as np\nfrom astro_calculations import *\n\ndef test_ranges():\n\n    x = 1\n    y = 2\n    # Testing all values between 0 and 99\n    for i in range(100):\n        assert(add(i,x) == i + 1)\n        assert(sub(i,x) == i - 1)\n        assert(mult(i,y) == i * y)\n        assert(div(i,y) == i / y)\n        assert(sqrt(i) == np.sqrt(i))\n        assert(convert(i).value - i * 1000 / 60 / 60 &lt; 1e-7)\n\n\ndef test_arrays():\n    x_arr = np.array([1,2,3,4])\n    y_arr = np.array([1,2,3,4])\n\n    # Testing numpy arrays\n    assert(np.all(add(x_arr, y_arr) == (x_arr + y_arr)))\n    assert(np.all(sub(x_arr, y_arr) == x_arr - y_arr))\n    assert(np.all(mult(x_arr, y_arr) == x_arr * y_arr))\n    assert(np.all(div(x_arr, y_arr) == x_arr / y_arr))\n    assert(np.all(sqrt(x_arr) == [np.sqrt(x) if x &gt;= 0 else 0 for x in x_arr]))\n    assert( (convert(x_arr).value - x_arr * 1000 / 60 / 60 &lt; 1e-7).all())\n</code></pre></p> <p>Here we're converting the <code>astropy.Quantity</code> to a <code>float</code> using <code>.value</code>.  We can run the test using <code>pytest</code>. <pre><code>========================================================= test session starts =========================================================\nplatform linux -- Python 3.10.10, pytest-7.2.2, pluggy-1.0.0\nrootdir: /raid/RAID1/Tutorials/git-tutorial\ncollected 0 items / 1 error                                                                                                           \n\n=============================================================== ERRORS ================================================================\n______________________________________________________ ERROR collecting test.py _______________________________________________________\ntest.py:14: in &lt;module&gt;\n    assert(convert(i) == i * 1000 / 60 / 60)\nastro_calculations/math_operations.py:12: in convert\n    return (x * u.km / u.hr).to(\"m/s\")\nE   NameError: name 'u' is not defined\n======================================================= short test summary info =======================================================\nERROR test.py - NameError: name 'u' is not defined\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n========================================================== 1 error in 0.87s ===========================================================\n</code></pre></p> <p>Let's modify astro_calculations/math_operations.py to fix the import bug: astro_calculations/math_operations.py<pre><code>import numpy as np\nfrom astropy import units as u\n\ndef convert(x):\n    ...\n</code></pre></p> <p><code>pytest</code> should no longer report any errors. With these changes made we can commit the changes.</p> <pre><code>git add .\n</code></pre> <p>And commit with a useful message, adding the issue number to the commit message: <pre><code>git commit -m \"Adding astropy import to resolve issue #1\"  [13:41:24]\n[astropy_issue 50a9db3] Adding astropy import to resolve issue #1\n 3 files changed, 6 insertions(+), 2 deletions(-)\n</code></pre></p> <p>Typing <code>git push</code> will inform us that there is no upstream branch: <pre><code>fatal: The current branch astropy_issue has no upstream branch.\nTo push the current branch and set the remote as upstream, use\n\n    git push --set-upstream origin astropy_issue\n</code></pre></p> <p>Using the helpful message (in case we ever forget it!) we can add the upstream when pushing: <pre><code>git push --set-upstream origin astropy_issue                 [13:42:19]\n\nEnumerating objects: 11, done.\nCounting objects: 100% (11/11), done.\nDelta compression using up to 16 threads\nCompressing objects: 100% (6/6), done.\nWriting objects: 100% (6/6), 751 bytes | 751.00 KiB/s, done.\nTotal 6 (delta 3), reused 0 (delta 0), pack-reused 0\nremote: Resolving deltas: 100% (3/3), completed with 3 local objects.\nremote: \nremote: Create a pull request for 'astropy_issue' on GitHub by visiting:\nremote:      https://github.com/steob92/astro_calculations/pull/new/astropy_issue\nremote: \nTo github.com:steob92/astro_calculations.git\n * [new branch]      astropy_issue -&gt; astropy_issue\nBranch 'astropy_issue' set up to track remote branch 'astropy_issue' from 'origin'.\n</code></pre></p> <p>Switching back to GitHub, we can now see that the issue has been updated to indicate that the commit referenced this issue.</p> <p> </p>  Referencing issues in commit messages on GitHub.  <p>After some testing and discussion, we might decide that this change resolves all the issues, so we want to merge the changes into the <code>main</code> branch.  To do this, we can navigate to the <code>Pull Request</code> tab in the top panel:</p> <p> </p>  Opening a pull request.  <p>We can see that there is a highlighted option at the top allowing us to merge in the fastest request. This won't always be the branch we want to merge, so let's do it the manual way by clicking <code>New pull request</code>. Using the drop-down menu, we can select which branches we want to merge. The arrow indicates which branch is being merged into which. In the below example, the <code>astropy_issue</code> branch is to be merged into the <code>main</code>.</p> <p> </p>  Labeling issues on GitHub.  <p>Clicking <code>Create pull request</code> brings us to a page to add a description.  We can add a helpful description and create the request.</p> <p>We can always see open pull requests by clicking on the <code>Pull requests</code> tab on the repository options panel:</p> <p> </p>  Open pull requests.  <p>When clicking on an open pull request, we are provided with a wealth of additional information:</p> <p> </p>  Page for an individual pull request.  <p>GitHub will check if the merge can happen as is, or if there are conflicts that need to be resolved. Like issues, we can also assign developers, <code>labels</code>, <code>projects</code>, etc. It is common to require a code review before accepting a pull/merge request. Once we're happy, we can approve the merge request by clicking <code>Merge pull request</code>, add a comment if needed, and click <code>Confirm merge</code>. Once merged, we can delete the branch that was just merged.</p>"},{"location":"VersionControl/git_2/#tagging-versions","title":"Tagging Versions","text":"<p>With a new feature added to the code, it might be time to release a new version of the code.  To do this, we'll use the <code>Create a new release</code> button on the repository home page:</p> <p> </p>  GitHub releases.  <p>From here, we can set a <code>tag</code> or version number, set the target (for example, the <code>main</code> branch), and set the previous <code>tag</code>:</p> <p> </p>  GitHub release page.  <p>We can use the <code>Generate release notes</code> option to automatically create a set of notes for the new release.  This tells the user what has changed between the new release and the previous tag.  GitHub will generate these notes from the commit messages.  When we're happy with the description of the new release, we can click <code>Publish release</code>.</p> <p>This brings us to a release page for the tag:</p> <p> </p>  Generated change log and release page.  <p>This contains lots of useful information like what has changed since the previous release, new contributors, and who contributed to this release.  Additionally, a copy of the source code is available for download.</p> <p>From our local machine, we can retrieve all the releases using the following command: <pre><code>git fetch --tags\n</code></pre> We can list available tags with: <pre><code>git tag\n</code></pre> and checkout the v1.0.0 tag with: <pre><code>git checkout tags/v1.0.0\n</code></pre></p> <p>Using tagged versions allows us to control which version of the code is being used. Tags serve as markers or labels for specific points in the repository's history, typically representing stable releases, major milestones, or significant changes. They provide a convenient way to reference and track specific versions of the codebase, facilitating reproducibility, collaboration, and deployment workflows.</p>"},{"location":"VersionControl/git_2/#using-github-to-streamline-workflow-with-github-actions","title":"Using GitHub to Streamline Workflow with GitHub Actions","text":"<p>When maintaining a project with multiple developers and users, it's crucial to ensure that one developer doesn't introduce code-breaking features. We've already explored how to use <code>pytest</code> to run a set of tests locally. However, we can leverage \"GitHub Actions\" to automate workflows triggered by specific events.</p> <p>For instance, let's consider ensuring that our <code>main</code> branch only contains code that works without any errors. We could define a series of tests that must be passed before pushing changes to the <code>main</code> branch. While we could enforce a policy requiring all developers to run these tests before pushing changes, human error or forgetfulness can occur. Instead, we can set up a \"GitHub Action\" to trigger automatically whenever a <code>push</code> attempt is made to the <code>main</code> branch.</p> <p>Firstly, let's clarify the dependencies required to compile and run the code. For new developers attempting to run the code in a new environment, they'll need a list of required packages to install. We can define these dependencies in a requirements file:</p> <p>requirements.txt<pre><code>numpy\nastropy\n</code></pre> We only have two external packages, <code>numpy</code> and <code>astropy</code>. To install them, one would run the following command: <pre><code>pip install -r requirements.txt\n</code></pre></p> <p>Commit and push this file to <code>main</code>.</p> <p>Next, on GitHub, click on the <code>Actions</code> tab on the repository options panel. If this is our first action, we'll see a page like this:</p> <p> </p>  Setting up our first GitHub Action.  <p>There are lots of pre-made actions available to choose from. We'll take a pre-made action for now and revisit setting up our own workflow in a future tutorial. Let's select the <code>Python package</code> action by clicking <code>Configure</code> on that action's card. This will bring us to a new page with a pre-generated YAML file:</p> <p>python-package.yml<pre><code># This workflow will install Python dependencies, run tests and lint with a variety of Python versions\n# For more information see: https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python\n\nname: Python package\n\non:\n  push:\n    branches: [ \"main\" ]\n  pull_request:\n    branches: [ \"main\" ]\n\njobs:\n  build:\n\n    runs-on: ubuntu-latest\n    strategy:\n      fail-fast: false\n      matrix:\n        python-version: [\"3.9\", \"3.10\", \"3.11\"]\n\n    steps:\n    - uses: actions/checkout@v3\n    - name: Set up Python ${{ matrix.python-version }}\n      uses: actions/setup-python@v3\n      with:\n        python-version: ${{ matrix.python-version }}\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        python -m pip install flake8 pytest\n        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n    - name: Lint with flake8\n      run: |\n        # stop the build if there are Python syntax errors or undefined names\n        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n        # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\n        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n    - name: Test with pytest\n      run: |\n        pytest test.py\n</code></pre> Let's break this down: - On line 4, we define the name of the test as <code>Python package</code>. - On lines 6-10, we define the actions to trigger (<code>push</code> and <code>pull_request</code>) and the branches to trigger on (<code>main</code>). This specific action will trigger on any <code>push</code> or <code>pull_request</code> to the <code>main</code> branch. - On line 15, we specify the container to run our tests on. Here, we use <code>ubuntu-latest</code>, representing the latest version of the Ubuntu Linux OS. - On line 19, we define a list of Python versions to test the code against. - From line 21 onwards, we define the steps of the action. - On lines 27-31, we define the <code>Install dependencies</code> step, which installs and upgrades <code>pip</code> on line 29, and installs <code>flake8</code> and <code>pytest</code> using the <code>requirements.txt</code> file on line 31. - On lines 32-37, we perform linting using the <code>flake8</code> command, which tests the code for bugs and formatting issues. - On lines 38-40, we run <code>pytest</code>. - I've modified line 40 to include the name of the test file (<code>test.py</code>).</p> <p>This file is saved to <code>.github/workflows/python-package.yml</code>. Now, let's try creating a new branch with some new code.</p> <pre><code>git checkout -b new_feature \n</code></pre> <p>Modify some file to include a new dependency: astro_calculations/math_operations.py<pre><code>import numpy as np\nfrom astropy import units as u\nfrom scipy import optimize\n\ndef convert(x):\n    ...\n</code></pre></p> <p>Commit and push this new branch: <pre><code>git commit -m \"adding new dependency\"\ngit push --set-upstream origin new_feature\n</code></pre></p> <p>From GitHub, we can now create a pull request from this new branch.  After some time, we'll notice that the pull request to <code>main</code> triggered our action, and this action has failed the checks.</p> <p> </p>  (Failed) GitHub actions triggered by a pull request show up on the pull request page.  <p>By clicking on the details link, we can see what caused the pull request to fail:</p> <p> </p>  Details of the failed workflow.  <p>The action failed because we forgot to add <code>scipy</code> to the <code>requirements.txt</code> file.  If we make these changes and commit them to the same branch, these changes will automatically be tested.</p> <p> </p>  Successful workflow showing up under the pull request.  <p>We can see that the latest commit ('adding scipy') passes all the tests, allowing us to merge the pull request knowing that we haven't introduced any new bugs to our code.</p> <p>By using GitHub Actions to automatically test our code, we have adopted a DevOps development practice known as \"Continuous Integration\" (CI). Continuous Integration involves continuously integrating code changes into a shared repository, where automated tests are run to detect integration errors and ensure code quality. This approach helps teams catch and fix bugs early in the development process, leading to faster delivery of high-quality software.</p> <p>In the next tutorial, we'll delve deeper into Continuous Integration methods and practices and explore how we can leverage GitHub Actions and other tools to further improve our code quality and development workflow.</p>"},{"location":"VersionControl/git_3/","title":"Git 3","text":""},{"location":"VersionControl/git_3/#about","title":"About","text":"<p>In this tutorial we'll focus on the concept of \"Continuous Integration\" (CI). CI is a process where changes made to a codebase by developers are automatically integrated and tested. Automatic integration tools and tests are used to reduce future issues to the codebase by flagging potential errors and bugs before they are pushed to a <code>main</code> branch or, worse, a production version of the codebase.</p> <p>An example of this workflow could be something like this:</p> <ol> <li>A developer writes a section of code.</li> <li>Upon commiting their changes, a series of tests are ran on the code.</li> <li>If the tests are successful then the code can be pushed to the codebase.</li> <li>If unsuccessful, the tests report helpful debugging information to help the developer resolve a potetential issue before it is commited to the codebase.</li> <li>With the code passing tests before being merged into the <code>main</code> branch, users of the codebase can expect a greater deal of stability as mainy bugs will be caught before reaching this level.</li> </ol> <p>There are many different processes that can be run as part of CI, such as:</p> <ul> <li> <p>Testing: This is when the code is ran with a set of parameters that has a known behaviour. For example if a function f(x) has the numerical solution f(0) = 42, then the coded function function_f(0) should return 42. Anything other than 42 suggests an error in the calculation.</p> </li> <li> <p>Formatting: Espeically in larger codebases, it is good to stick to a consistent formatting (tabs vs spaces), variable naming conventions (camel vs snake vs pascal) and function naming convention (Importance of naming in programming). By having a style guide for developers (for example PEP 8, Google Python Style Guide), will help with the overall readabiltiy of the code.</p> </li> <li> <p>Linting: Linting is the process of analyzing static code to flag potential issues such programming errors, bugs, stylistic issues. Analyzing the static code, linting can catch common issues and bugs before they are even ran. The concept of \"linting\" is to act as a \"lint filter\" to stop small bugs being propagated and causing larger issues, either performance related or more serious bugs.</p> </li> <li> <p>Automatic release: If there is a significant change to the code, we might want to automatically tag a new version of the code and generate a change log. This allows users to know what is the most up to date and stable version of the code. Additinoally, helps when flagging issues, as users reporting bugs can point to specific release versions of the code. </p> </li> <li> <p>Automatic Building: If our code passes the required tests, we might want to automatically trigger a new build of the code. For example, if we use a container to run the code, we might want a new image (e.g. a docker image) to be build and push to a container registary (e.g. dockerhub), allowing remote users to grab the most up to date container when they want to run the code. For Python project, we could also trigger a new version of the code to be published to PyPi, allowing users to install the latest version with <code>pip</code>.</p> </li> </ul> <p>In this tutorial we'll focus on three main concepts to help adapt CI principles to our codebase:</p> <ul> <li> <p>Commitizen a tool for writing Conventional Commits, automatic handling of versionings (SemVer) and automatic generation of change logs. This encourages conforming to consistent commit messages syntax, allowing for informative change logs to be automatically generated, detailing the changes to the code.</p> </li> <li> <p>Pre-commit to tool to run git hook scripts upon triggering a git action such  as committing or pushing to a remote repository. This allows for common bugs and issues to be caught before the code is even committed.</p> </li> <li> <p>GitHub Actions predefined \"workflows\" that can be triggered by a GitHub event (e.g. <code>commit</code>, <code>push</code>, <code>issue</code>, etc). GitHub Actions will run on a virtual machine hosted by GitHub, with a large number of pre-made actions available. These actions can range from testing to full deployment.</p> </li> </ul>"},{"location":"VersionControl/git_3/#commitizen","title":"Commitizen","text":"<p>Commitizen is a tool to help with writing convention commits.  Coventional commits are human and machine readable commit messages. These allow specific changes to be highlighted and flagged by what the specific change does. For example, if a file (<code>my_file.py</code>) is changed to fix a bug, one might flag the commit as a <code>fix(my_file.py)</code>.  Here we would refer to <code>my_file.py</code> as the \"scope\" of the change. If a new feature is added to the code we might tag the commit with <code>feat(my_file.py)</code>.  If the feature or fix that was implemented breaks backwards compatability, it would be good to tag this as feature breaking, this is done by adding a <code>!</code> to the end of the tag (for example <code>feat!(my_file.py)</code>).</p> <p>We would then add a short comment to describe the what we have changed, for example <code>feat(my_file.py) adding function to do magic things</code>. This provides a high level description of the change. </p> <p>Finally, we might want a more descriptive commit for our records so a developer who needs a more detailed understanding can understand what changed: <pre><code>feat(my_file.py) adding function to do magic things\n\nThe magic function is added to my_file, implementing the Houdini et al method of slight-of-handary\n</code></pre></p> <p>When viewing this commit on GitHub, a quicklook at the commit would see <code>feat(my_file.py) adding function to do magic things</code>, whereas clicking on the commit one would see the full message.  When generating the changelog, the entire message would be copied, with different commits grouped by their catagories (e.g. <code>feat</code>, <code>fix</code>, <code>docs</code> etc.).</p> <p>Commitizen can be installed using <code>pip</code>: <pre><code>pip install -U commitizen\n</code></pre></p> <p>We can set this up commitizen in a pre-exisiting repository using:</p>"}]}